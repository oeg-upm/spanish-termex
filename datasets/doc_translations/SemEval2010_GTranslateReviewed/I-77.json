{
    "id": "I-77",
    "original_text": "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC). We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness). The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions. The balance is the difference between these matrices. A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future. The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy. Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1. INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters. The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14]. We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical. Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results. In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them. These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels). These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows. We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process. Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level. It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension. In this sense, humans aim at a general sense of fairness in an interaction. In Section 2 we outline the aspects of human negotiation modelling that we cover in this work. Then, in Section 3 we introduce the negotiation language. Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation. Section 5 contains a description of the different metrics used in the agent model including intimacy. Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2. HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy. What information is relevant to the negotiation process? What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options. What are the possible agreements we can accept? • Goals. What are the underlying things we need or care about? What are our goals? • Independence. What will we do if the negotiation fails? What alternatives have we got? • Commitment. What outstanding commitments do we have? Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . . A key part of any negotiation process is to build a model of our opponent(s) along these dimensions. All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments. Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them. For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain. These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships. The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable). However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need. Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby). For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension. The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level). In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need. We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved. According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used. This might be part of our social evolution. There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce. In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity). In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods. See Table 1 for some examples of desired balances along the LOGIC dimensions. The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators. For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close. Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3. COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1. C is a finite set of concept symbols (including basic data types); 2. R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found. This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure. Usually, food is accumulated at the shelter for future use. Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required. We will omit this here. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy. To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on). R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues. The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation. Semantic distance plays a fundamental role in strategies for information-based agency. How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships. A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts). Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions. First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated. Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts. A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively. Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed. Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour. Norms, contracts, and information have an obvious temporal dimension. Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time. The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place. In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions. C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a. Representing an ontology as a set predicates in Prolog is simple. The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y. Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4. AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2]. The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment. In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation. The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents. Agents have a probabilistic first-order internal language L used to represent a world model, Mt . A generic information-based architecture is described in detail in [15]. The LOGIC agent architecture is shown in Figure 1. Agent α acts in response to a need that is expressed in terms of the ontology. A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires. Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 . The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here. For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term. Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt . Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer. We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2. Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2. The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6. We now describe two of the distributions in Mt that support offer exchange. Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability. The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15]. Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space. The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space. We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space. Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6. The parameters g and h are independent. We can imagine a relationship that begins with g = 1 and h = 0. Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1. The basis for the acceptance criterion has thus developed from equity to equality, and then to need. Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses. For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}. In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible. This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed. All observations about the world are received as utterances from an all-truthful institution agent ξ. For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger. So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received. We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility. For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions. In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data. In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi). For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution. Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi). Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution. We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations. We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows. Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj . This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11]. Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution. The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation. The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease. We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5. SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context. A relationship, Ψ∗t , is a sequence of dialogues. We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation). Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures. Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories. In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14]. Ideal observations. Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe. This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e). Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ). That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have. This equation measures confidence for a single statement ϕ. It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ. Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations. The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ. Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ. Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation. Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement. If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before. Computational Note. The various measures given above involve extensive calculations. For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ . We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ). The extent of this calculation is controlled by the parameter η. An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β. This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}. Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own. In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated. The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define. This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach. For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain. Attaching a utilitarian measure to this utterance may not be so simple. We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements. Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ). The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ . If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction. Dx is the reputation of agent x. The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β . In particular, the intimacy determines values for the parameters g and h in Equation 1. As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases. The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues. In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6. STRATEGIES AND TACTICS Each negotiation has to achieve two goals. First it may be intended to achieve some contractual outcome. Second it will aim to contribute to the growth, or decline, of the relationship intimacy. We now describe in greater detail the contents of the Negotiation box in Figure 1. The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships. The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy. The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases. The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions. The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy. The negotiation strategy is concerned with maintaining a working set of Options. If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position. In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options. This is achieved by increasing the value of s in Equation 1. The following strategy uses the machinery described in Section 4. Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances. Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation. Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ . Suppose that the relationship target is (T∗t α , T∗t β ). Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g. Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.). It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6. Nα(Ψt ) is what α hopes to receive from β during Ψt . This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions. A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance. A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue. If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7. DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies. It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic. Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment. Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures. The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next. We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).) Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8. REFERENCES [1] Adams, J. S. Inequity in social exchange. In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2. New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J. A., and Sierra, C. Environment engineering for multiagent systems. Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B. Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives. Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation. Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering. American Institute of Physics, Melville, NY, USA, 2004, ch. On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J. Bargaining with information. In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B. Getting to Yes: Negotiating agreements without giving in. Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory. In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science. Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation. McGraw Hill, 2001. [10] Li, Y., Bandar, Z. A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources. IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms. Cambridge University Press, 2003. [12] Paris, J. Common sense and maximum entropy. Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J. An information-based model for trust. In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J. Trust and honour in information-based agency. In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency. In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship. Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A. Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations. In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5. JAI Press, 1995, pp. 65-94. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037",
    "original_translation": "El modelo de negociación lógica Carles Sierra Institut Dinvestigacio en Intel. Ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Cataluña, España Sierra@iiia.csic.es John Debenham Facultad de Tecnología de la Información Universidad de Tecnología, Sydney NSW, Australia Debenham@IT.UTS.EDU.AU Resumen Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: legitimidad, opciones, objetivos, independencia y compromiso, (lógica). Introducimos un modelo de negociación basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad). La intimidad es un par de matrices que evalúan tanto una contribución de los agentes a la relación como a sus oponentes que contribuyen a cada uno desde una vista de información y desde una visión utilitaria en las cinco dimensiones lógicas. El balance es la diferencia entre estas matrices. Una estrategia de relación mantiene una intimidad objetivo para cada relación que un agente le gustaría que la relación avance en el futuro. La estrategia de negociación mantiene un conjunto de opciones que están en línea con el nivel de intimidad actual, y luego las tácticas envuelven las opciones en la argumentación con el objetivo de lograr un acuerdo exitoso y manipular los sucesivos saldos de negociación hacia la intimidad objetivo. Categorías y descriptores de sujetos I.2.11 [Inteligencia artificial]: Sistemas de inteligencia artificial distribuida Sistemas de Términos Generales Teoría 1. Introducción En este documento proponemos un nuevo modelo de negociación para lidiar con relaciones a largo plazo que se basan en sucesivos encuentros de negociación. El modelo se basa en los resultados de los estudios comerciales y psicológicos [1, 16, 9], y reconoce que la negociación es un proceso de intercambio de información, así como un proceso de intercambio de servicios públicos [15, 14]. Creemos que si los agentes tienen éxito en dominios de aplicación reales, tienen que conciliar ambas opiniones: informativa y gametheórica. Nuestro objetivo es modelar escenarios comerciales en los que los agentes representan a sus directores humanos y, por lo tanto, queremos que su comportamiento sea comprensible por los humanos y respetar los procedimientos habituales de negociación humana, al tiempo que sean consistentes y de alguna manera extendiendo los resultados teóricos y teóricos teóricos de la información del juego. En este sentido, los agentes no son solo los maximizadores de servicios públicos, sino que tienen como objetivo construir relaciones duraderas con niveles de intimidad progresivos que determinen qué equilibrio en la información y el intercambio de recursos es aceptable para ellos. Estos dos conceptos, la intimidad y el equilibrio son clave en el modelo y nos permiten comprender la teoría de juegos competitivos y cooperativos como dos teorías particulares de las relaciones de agentes (es decir, en diferentes niveles de intimidad). Estas dos teorías son demasiado específicas y distintas para describir cómo podría crecer una relación (comercial) porque las interacciones tienen algunos aspectos de estos dos extremos en un continuo en el que, por ejemplo, los agentes revelan cantidades cada vez mayores de información privada a medida que su intimidad crece. No seguimos la cooperación de APROACH [4] donde la cooperación y la competencia dependen del tema bajo negociación, sino que creemos que la voluntad de cooperar/competir afecta todos los aspectos en el proceso de negociación. Las estrategias de negociación pueden verse naturalmente como procedimientos que seleccionan tácticas utilizadas para lograr un acuerdo exitoso y alcanzar un nivel de intimidad objetivo. Es común en entornos humanos usar tácticas que compensan los desequilibrios en una dimensión de una negociación con desequilibrio en otra dimensión. En este sentido, los humanos apuntan a un sentido general de justicia en una interacción. En la Sección 2 describimos los aspectos del modelado de negociación humana que cubrimos en este trabajo. Luego, en la Sección 3 presentamos el lenguaje de negociación. La Sección 4 explica en la arquitectura y los conceptos de intimidad y equilibrio, y cómo influyen en la negociación. La Sección 5 contiene una descripción de las diferentes métricas utilizadas en el modelo de agente, incluida la intimidad. Finalmente, la Sección 6 describe cómo las estrategias y las tácticas utilizan el marco lógico, la intimidad y el equilibrio.2. La negociación humana antes de que comience una negociación, los negociadores humanos preparan los intercambios dialógicos que se pueden hacer a lo largo de las cinco dimensiones lógicas [7]: • Legitimidad. ¿Qué información es relevante para el proceso de negociación? ¿Cuáles son los argumentos persuasivos sobre la equidad de las opciones?1030 978-81-904262-7-5 (RPS) C 2007 Ifaamas • Opciones. ¿Cuáles son los posibles acuerdos que podemos aceptar?• Objetivos. ¿Cuáles son las cosas subyacentes que necesitamos o nos importan? ¿Cuáles son nuestros objetivos?• Independencia. ¿Qué haremos si la negociación falla? ¿Qué alternativas tenemos?• Compromiso. ¿Qué compromisos pendientes tenemos? Los diálogos de negociación, en este contexto, intercambian movimientos dialógicos, es decir, mensajes, con la intención de obtener información sobre el oponente o dar información sobre nosotros a lo largo de estas cinco dimensiones: solicitud de información, proponer opciones, informar sobre intereses, emitir promesas, apelar aestándares... Una parte clave de cualquier proceso de negociación es construir un modelo de nuestros oponentes a lo largo de estas dimensiones. Todos los agentes de expresiones hacen durante una negociación dan información sobre su modelo lógico actual, es decir, sobre su legitimidad, opciones, objetivos, independencia y compromisos. Además, varias expresiones pueden tener una interpretación utilitaria en el sentido de que un agente puede asociarles una ganancia preferencial. Por ejemplo, una oferta puede informar a nuestro oponente de negociación sobre nuestra voluntad de firmar un contrato en los términos expresados en la oferta, y al mismo tiempo el oponente puede calcular cuál es su ganancia utilitaria esperada asociada. Estas dos vistas: basadas en información y basadas en servicios públicos son centrales en el modelo propuesto en este documento.2.1 Intimidad y equilibrio En las relaciones hay evidencia de estudios psicológicos que los humanos buscan un equilibrio en sus relaciones de negociación. La visión clásica [1] es que las personas perciben las asignaciones de recursos como distributivamente justas (es decir, bien equilibradas) si son proporcionales a las entradas o contribuciones (es decir, equitativas). Sin embargo, estudios más recientes [16, 17] muestran que los humanos siguen un conjunto más rico de normas de justicia distributiva dependiendo de su nivel de intimidad: equidad, igualdad y necesidad. El capital es la asignación proporcional al esfuerzo (por ejemplo, la ganancia de una empresa va a los acciones proporcionales a su inversión), es la igualdad la asignación en cantidades iguales (por ejemplo, dos amigos comen la misma cantidad de un pastel cocinado por uno de ellos)y necesita ser la asignación proporcional a la necesidad del recurso (por ejemplo, en caso de escasez de alimentos, una madre le da toda la comida a su bebé). Por ejemplo, si estamos en un entorno puramente económico (baja intimidad) podríamos solicitar equidad para la dimensión de opciones pero podríamos aceptar la igualdad en la dimensión de los objetivos. La percepción de una relación en equilibrio (es decir, justa) depende en gran medida de la naturaleza de las relaciones sociales entre los individuos (es decir, el nivel de intimidad). En relaciones puramente económicas (por ejemplo, negocios), la equidad se percibe como más justa;En las relaciones donde la acción conjunta o el fomento de las relaciones sociales son el objetivo (por ejemplo, amigos), la igualdad se percibe como más justa;Y en situaciones donde el desarrollo personal o el bienestar personal son el objetivo (por ejemplo, la familia), las asignaciones generalmente se basan en la necesidad. Creemos que la percepción del equilibrio en los diálogos (en la negociación o no) se basa en las relaciones sociales, y que cada dimensión de una interacción entre los humanos puede correlacionarse con la cercanía social o la intimidad entre las partes involucradas. Según los estudios anteriores, cuanto más intimidad en las cinco dimensiones lógicas, más se usa la norma de necesidad, y menos intimidad, más se usa la norma de equidad. Esto podría ser parte de nuestra evolución social. Existe una amplia evidencia de que cuando las sociedades humanas evolucionaron de una estructura de cazadores-recolectores1 a un refugio One2, la probabilidad de supervivencia aumentó cuando la comida era escasa. En este contexto, podemos ver claramente que, por ejemplo, las familias intercambian no solo los bienes sino también la información y el conocimiento basados en la necesidad, y que pocas familias considerarían que sus relaciones están desequilibradas y, por lo tanto, injustas, cuando hay una fuerte asimetría enLos intercambios (una madre que explica todo a sus hijos, o comprando juguetes, no esperan reciprocidad). En el caso de los socios, existe alguna evidencia [3] de que las asignaciones de bienes y cargas (es decir, servicios públicos positivos y negativos) se perciben como justos o en equilibrio, basadas en la equidad para las cargas y la igualdad para los bienes. Consulte la Tabla 1 para obtener algunos ejemplos de saldos deseados a lo largo de las dimensiones lógicas. El equilibrio percibido en un diálogo de negociación permite a los negociadores inferir información sobre su oponente, sobre su postura lógica y comparar sus relaciones con todos los negociadores. Por ejemplo, si percibimos que cada vez que solicitamos información se proporciona, y que no se devuelven preguntas significativas, o no se presentan que no reciban información, eso probablemente significa que nuestro oponente percibe que nuestra relación social es muy cercana. Alternativamente, podemos detectar qué problemas están causando una carga a nuestro oponente al observar un desequilibrio en la información o los sentidos utilitarios sobre ese tema.3. Modelo de comunicación 3.1 Ontología Para definir un lenguaje para estructurar los diálogos de agentes, necesitamos una ontología que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizado en una jerarquía IS-A (por ejemplo, Platypuses un mamífero, el dólar australiano es una moneda) y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio (cerveza, aud)). 3 modelamos ontologías siguiendo un enfoque algebraico [8] como: una ontología es una tupla O = =(C, R, ≤, σ) donde: 1. C es un conjunto finito de símbolos conceptuales (incluidos los tipos de datos básicos);2. R es un conjunto finito de símbolos de relación;3. ≤ es una relación reflexiva, transitiva y antisimétrica en C (un orden parcial) 4. σ: R → C+ es la función que asigna a cada símbolo de relación su aridad 1 en su forma más pura, los individuos en estas sociedades recolectan alimentos yConsumirlo cuando y donde se encuentre. Este es un intercambio de capital puro de los recursos, la ganancia es proporcional al esfuerzo.2 En estas sociedades hay unidades familiares, alrededor de un refugio, que representan la estructura básica de intercambio de alimentos. Por lo general, la comida se acumula en el refugio para uso futuro. Entonces la ingesta de alimentos depende más de la necesidad de los miembros.3 Por lo general, también se requiere un conjunto de axiomas definidos sobre los conceptos y relaciones. Omitiremos esto aquí. El sexto intl. Conf.Sobre agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1031 Elemento Un nuevo socio comercial My Butcher mi jefe, mi compañero, mis hijos, legitimidad, equidad de equidad de igualdad de igualdad necesidad de Opciones de equidad Equidad de capital mixta Mezcla Equidad Equidad NECESIDAD NECESIDAD INFERENTE Equidad Equidad Equidad NecesidadCompromiso Equidad Equidad Equidad Mezcla Necesita una equidad en la carga, igualdad en la buena Tabla 1: Algunos equilibrios deseados (sentido de justicia) ejemplos dependiendo de la relación.donde ≤ es la jerarquía tradicional IS-A. Para simplificar los cálculos en la computación de las distribuciones de probabilidad, suponemos que hay una serie de árboles disjuntos de los árboles que cubren diferentes espacios ontológicos (por ejemplo, un árbol para tipos de tela, un árbol para formas de ropa, etc.). R contiene relaciones entre los conceptos en la jerarquía, esto es necesario para definir objetos (por ejemplo, ofertas) que se definen como una tupla de problemas. La distancia semántica entre conceptos dentro de una ontología depende de cuán lejos estén en la estructura definida por la relación ≤. La distancia semántica juega un papel fundamental en las estrategias para la agencia basada en la información. Cómo los contratos firmados, Commit (·), sobre los objetos en una región semántica particular, y su ejecución, realizada (·), afectan nuestro proceso de toma de decisiones sobre la firma de contratos futuros en regiones semánticas cercanas es crucial para modelar el sentido común que aplican los seres humanos.en la gestión de las relaciones comerciales. Una medida [10] basa la similitud semántica entre dos conceptos sobre la longitud de la ruta inducida por ≤ (más distancia en el gráfico ≤ significa menos similitud semántica) y la profundidad del concepto de subsumidor (ancestro común) en el camino más corto entre los dosconceptos (cuanto más profundo en la jerarquía, más cerca es el significado de los conceptos). La similitud semántica se define como: sim (c, c) = e - κ1l · eκ2h - e - κ2h eκ2h + e - κ2h donde l es la longitud (es decir, el número de lúpulo) del camino más corto entre los conceptos, h es elLa profundidad del concepto más profundo que subsume ambos conceptos, y κ1 y κ2 son parámetros que escalan las contribuciones de la longitud de la ruta más corta y la profundidad respectivamente.3.2 Lenguaje La forma del lenguaje que α usa para representar la información recibida y el contenido de sus diálogos depende de dos nociones fundamentales. Primero, cuando los agentes interactúan dentro de una institución general, aceptan explícita o implícitamente las normas que limitarán su comportamiento y aceptan las sanciones y sanciones establecidas cada vez que se violan las normas. En segundo lugar, los diálogos en los que α se involucra se construyen alrededor de dos acciones fundamentales: (i) información de aprobación y (ii) intercambiando propuestas y contratos. Un contrato δ = (a, b) entre los agentes α y β es un par donde A y B representan las acciones de las que los agentes α y β son responsables respectivamente. Los contratos firmados por los agentes y la información aprobados por los agentes son similares a las normas en el sentido de que obligan a los agentes a comportarse de una manera particular, a fin de satisfacer las condiciones del contrato o hacer que el mundo sea consistente con la información aprobada. Los contratos e información pueden considerarse como declaraciones normativas que restringen el comportamiento de los agentes. Las normas, los contratos e información tienen una dimensión temporal obvia. Por lo tanto, un agente tiene que cumplir con una norma mientras está dentro de una institución, un contrato tiene un período de validez, y una información es verdadera solo durante un intervalo en el tiempo. El conjunto de normas que afectan el comportamiento de un agente define el contexto que el agente debe tener en cuenta.El lenguaje de comunicación αS tiene dos primitivas fundamentales: cometer (α, β, ϕ) para representar, en ϕ, el mundo que α tiene como objetivo provocar y que β tiene derecho a verificar, quejarse o reclamar compensación para cualquier desviación de yHecho (μ) para representar el evento de que se ha producido una cierta acción μ4. De esta manera, las normas, los contratos y los fragmentos de información se representarán como casos de compromiso (·) donde α y β pueden ser agentes o instituciones individuales. C es: μ :: = Illoc (α, β, ϕ, t) |μ;μ |Deje que el contexto en μ finalice ϕ :: = término |Hecho (μ) |Cometer (α, β, ϕ) |ϕ ∧ ϕ |ϕ ∨ ϕ |¬ϕ |∀V.ϕv |∃V.ϕv contexto :: = ϕ |id = ϕ |cláusula de prólogo |contexto;contexto donde ϕV es una fórmula con variable libre V, ILLOC es cualquier conjunto apropiado de partículas ilocucionarias;significa secuenciación, y el contexto representa acuerdos anteriores, ilocuciones anteriores, el contexto de trabajo ontológico, que es una proyección de los árboles ontológicos que representan el enfoque de la conversación o el código que alinea las diferencias ontológicas entre los oradores necesarios para interpretar una acción A. Representar una ontología como predicados establecidos en Prolog es simple. El término establecido contiene instancias de los conceptos y relaciones de ontología.5 Por ejemplo, podemos representar la siguiente oferta: si gasta un total de más de E100 en mi tienda durante octubre, le daré un 10% de descuento en todos los bienes enNoviembre, AS: Oferta (α, β, gastado (β, α, octubre, x) ∧ x ≥ E100 → ∀ y. Hecho (informar (ξ, α, pay (β, α, y), noviembre)) → cometer (α, β, descuento (y, 10%))) ξ es un agente institucional que informa el pago.4 Sin pérdida de generalidad, asumiremos que todas las acciones son dialógicas.5 Suponemos la convención que c (c) significa que C es una instancia de concepto C y R (C1, ..., Cn) determina implícitamente que CI es una instancia del concepto en la posición I-Th de la relación R.1032 El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Figura 1: La arquitectura del agente lógico 4. Arquitectura del agente Un sistema multiagente {α, β1 ,..., βn, ξ, θ1 ,..., θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, información que proporciona agentes, θj y un agente institucional, ξ, que representa a la institución donde asumimos que las interacciones ocurren [2]. El agente institucional informa de manera rápida y honesta sobre lo que realmente ocurre después de que un agente firma un contrato, o hace otra forma de compromiso. En la Sección 4.1, esto nos permite medir la diferencia entre un enunciado y una observación posterior. El lenguaje de comunicación C introducido en la Sección 3.2 nos permite estructurar los diálogos y estructurar el procesamiento de la información recopilada por los agentes. Los agentes tienen un lenguaje interno probabilístico de primer orden L utilizado para representar un modelo mundial, MT. Una arquitectura genérica basada en información se describe en detalle en [15]. La arquitectura del agente lógico se muestra en la Figura 1. El agente α actúa en respuesta a una necesidad que se expresa en términos de la ontología. Una necesidad puede ser exógena, como la necesidad de comerciar de manera rentable y puede ser desencadenada por otro agente que ofrece comercio, o endógeno, como α que decide que posee más vino de lo que requiere. Las necesidades desencadenan el razonamiento proactivo de objetivos/planes αS, mientras que otros mensajes se tratan mediante el razonamiento reactivo αS.6 Cada plan se prepara para la negociación al ensamblar el contenido de un maletín lógico que el agente lleva a la negociación7. La estrategia de relación determina con qué agente negociar para una necesidad dada;Utiliza el análisis de gestión de riesgos para preservar un conjunto estratégico de relaciones comerciales para cada necesidad crítica de la misión; esto no se detalla aquí. Para cada relación comercial, esta estrategia genera un objetivo de relación que se expresa en el marco lógico como un nivel deseado de intimidad para lograr a largo plazo. Cada negociación consiste en un diálogo, ψt, entre dos agentes con agente α que contribuye a la expresión μ y a la parte 6 cada uno de los planes y reacciones αS contiene constructores para un modelo mundial inicial MT. Luego, MT se mantiene de las percepciones recibidas utilizando funciones de actualización que transforman las percepciones en restricciones en MT, para más detalles, ver [14, 15].7 La evidencia empírica muestra que en la negociación humana, se logran mejores resultados al sesgar las opciones de apertura a favor del proponente. No sabemos ninguna investigación empírica de esta hipótesis para los agentes autónomos en escenarios comerciales reales.ner β contribuyendo μ usando el lenguaje descrito en la Sección 3.2. Cada diálogo, ψt, se evalúa utilizando el marco lógico en términos del valor de ψt tanto a α como a β - ver Sección 5.2. Luego, la estrategia de negociación determina el conjunto actual de opciones {Δi}, y luego las tácticas, guiadas por el objetivo de negociación, deciden cuál, si alguna, de estas opciones, las presenta y las envuelve en el diálogo de argumentación: ver sección 6. Ahora describimos dos de las distribuciones en MT que admiten el intercambio de ofertas. Pt (ACC (α, β, χ, Δ)) estima que la probabilidad de que α debería aceptar la propuesta δ en satisfacción de su necesidad χ, donde δ = (A, B) es un par de compromisos, A para α y B para β.α aceptará δ IF: Pt (ACC (α, β, χ, δ))> C, para el nivel de certeza c.Esta estimación se agrava desde las opiniones subjetivas y objetivas de la aceptabilidad. La estimación subjetiva tiene en cuenta: la medida en que la promulgación de δ satisfará αs necesitará χ, cuánto Δ vale α y la medida en que α cree que estará en condiciones de ejecutar su compromiso A [14 14, 15]. Sα (β, a) es una variable aleatoria que denota la estimación de αS de la valoración subjetiva βs de A sobre algún espacio de evaluación numérica finita. La estimación objetiva captura si δ es aceptable en el mercado abierto, y la variable Uα (b) denota la valoración del mercado abierto de αs de la promulgación del compromiso B, nuevamente asumido de algún espacio de valoración numérica finita. También consideramos las necesidades, la variable Tα (β, A) denota la estimación de αS de la fuerza de la necesidad motivadora de βs para la promulgación del compromiso A sobre un espacio de valoración. Luego para Δ = (a, b): pt (ACC (α, β, χ, δ)) = Pt „Tα (β, A) Tα (α, B)« H × „Sα (α, B) Sα ((β, a) «g × uα (b) uα (a) ≥ s!(1) donde g ∈ [0, 1] es αS codicia, h ∈ [0, 1] es un grado αS de altruismo, y S ≈ 1 se deriva de la posición 8 descrita en la Sección 6. Los parámetros G y H son independientes. Podemos imaginar una relación que comienza con G = 1 y H = 0. Luego, a medida que los agentes comparten cantidades crecientes de su información sobre sus valoraciones de mercado abierto, G se reduce gradualmente a 0, y luego, a medida que comparten cantidades de información sobre sus necesidades, aumenta a 1. La base para el criterio de aceptación se ha desarrollado de equidad a igualdad, y luego a la necesidad. PT (ACC (β, α, δ)) estima la probabilidad de que β acepte δ, observando las respuestas de βS. Por ejemplo, si β envía la oferta de mensaje (Δ1), entonces α deriva la restricción: {PT (ACC (β, α, Δ1)) = 1} en la distribución PT (β, α, δ), y si esto es unOferta de contador a una oferta anterior de αS, Δ0, entonces: {Pt (ACC (β, α, Δ0)) = 0}. En el caso especial no atípico de negociación de múltiples problemas, donde las preferencias de los agentes sobre los problemas individuales solo se conocen y son complementarios para el otro, se puede aplicar un razonamiento de entropía máxima para estimar la probabilidad de que cualquier δ múltiple sea aceptable aβ al enumerar los posibles mundos que representan el límite de aceptabilidad βS [6].4.1 Actualización del Modelo Mundial MT αS El modelo mundial consiste en distribuciones de probabilidad que representan su incertidumbre en el estado mundial.α está interesado 8 Si α elige inflar sus opciones de apertura, entonces esto se logra en la Sección 6 al aumentar el valor de s.Si S 1, entonces un acuerdo puede no ser posible. Esto ilustra la conocida ineficiencia de la negociación bilateral establecida analíticamente por Myerson y Satterthwaite en 1983. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1033 en el grado en que un enunciado describe con precisión lo que se observará posteriormente. Todas las observaciones sobre el mundo se reciben como enunciados de un agente de la institución complementaria ξ. Por ejemplo, si β comunica el objetivo, tengo hambre y la negociación posterior termina con β compra de un libro de α (al aconsejar α que se ha acreditado una cierta cantidad de dinero a la cuenta de αS), entonces α puede concluir que el objetivo de βEligió satisfacer algo más que hambre. Entonces, el modelo World αS contiene distribuciones de probabilidad que representan sus expectativas inciertas de lo que se observará sobre la base de las expresiones recibidas. Representamos la relación entre el enunciado, ϕ, y la posterior observación, ϕ, por Pt (ϕ | ϕ) ∈ Mt, donde ϕ y ϕ pueden ser categorías ontológicas en interés de la viabilidad computacional. Por ejemplo, si ϕ es, le entregaré un cubo de peces mañana, entonces la distribución P (ϕ | ϕ) no necesita ser sobre todas las cosas posibles que β podría hacer, sino que podría ser sobre categorías ontológicas que resumen las posibles acciones βs. En ausencia de expresiones entrantes, las probabilidades condicionales, Pt (ϕ | ϕ), deben tender a la ignorancia como se representa por una distribución del límite de descomposición d (ϕ | ϕ).α puede tener conocimiento de fondo con respecto a D (ϕ | ϕ) como t → ∞, de lo contrario α puede suponer que tiene la máxima entropía mientras es consistente con los datos. En general, dada una distribución, Pt (xi) y una distribución de límite de descomposición d (xi), pt (xi) decae por: pt+1 (xi) = Δi (d (xi), pt (xi)) (2) donde Δi es la función de descomposición para el xi que satisface la propiedad que limt → ∞ pt (xi) = d (xi). Por ejemplo, Δi podría ser lineal: Pt + 1 (xi) = (1 - νi) × D (xi) + νi × pt (xi), donde νi <1 es la tasa de descomposición para la distribución con ésimo. La función de descomposición o la distribución del límite de desintegración también podrían ser una función del tiempo: ΔT I y DT (xi). Suponga que α recibe un enunciado μ = Illoc (α, β, ϕ, t) del agente β en el tiempo t.Suponga que α une una creencia epistémica RT (α, β, μ) a μ: esta probabilidad tiene en cuenta el nivel de precaución personal de αS. Modelamos la actualización de Pt (ϕ | ϕ) en dos casos, uno para observaciones administradas ϕ, segundo para observaciones dadas φ en el vecindario semántico de ϕ.4.2 Actualización de PT (ϕ | ϕ) Dada ϕ primero, si se observa ϕk, entonces α puede establecer Pt+1 (ϕk | ϕ) a algún valor d donde {ϕ1, ϕ2 ,..., ϕm} es el conjunto de todas las observaciones posibles. Estimamos la distribución posterior completa PT+1 (ϕ | ϕ) aplicando el principio de entropía relativa mínima9 de la siguiente manera. Sea P (μ) la distribución: 9 Dada una distribución de probabilidad q, la distribución mínima de entropía relativa p = (p1, .., pi) sujeto a un conjunto de restricciones lineales j g = {gj (p) = aj ·p - cj = 0}, j = 1 ,..., J (que debe incluir la restricción p i pi - 1 = 0) es: p = arg minr p j rj log rj qj. Esto puede calcularse introduciendo multiplicadores de LaGrange λ: l (p, λ) = p j pj log pj qj + λ · g.Minimizando l, {∂l ∂λj = gj (p) = 0}, j = 1 ,..., J es el conjunto de restricciones dadas g, y una solución a ∂l ∂pi = 0, i = 1 ,..., Conduce eventualmente a la p.La inferencia basada en entropía es una forma de inferencia bayesiana que es conveniente cuando los datos son escasos [5] y encapsula el razonamiento de sentido común [12].arg minx p j xj log xj pt (ϕ | ϕ) j que satisface la restricción p (μ) k = d.Entonces deje que Q (μ) sea la distribución: Q (μ) = RT (α, β, μ) × P (μ) + (1 - RT (α, β, μ)) × Pt (ϕ | ϕ) y luegoSea: R (μ) = (Q (μ) si Q (μ) es más interesante que Pt (ϕ | ϕ) Pt (ϕ | ϕ) de lo contrario una medida general de si Q (μ) es más interesante que PT (ϕ (ϕ| ϕ) es: k (q (μ) d (ϕ | ϕ))> k (pt (ϕ | ϕ) d (ϕ | ϕ)), donde k (x y) = p j xj ln xj yj es el kullback-La distancia del soleador entre dos distribuciones de probabilidad x e y [11]. Finalmente incorporando la ecuación.2 Obtenemos el método para actualizar una distribución PT (ϕ | ϕ) al recibir un mensaje μ: PT+1 (ϕ | ϕ) = ΔI (D (ϕ | ϕ), R (μ)) (3) Este procedimientoSe ocupa de la descomposición de la integridad, y con dos probabilidades: primero, la probabilidad z en la expresión μ, y en segundo lugar con la creencia RT (α, β, μ) que α se unía a μ.4.3 Actualización de PT (φ | φ) Dada ϕ el método SIM: Dado como arriba μ = Illoc (α, β, ϕ, t) y la observación ϕk definimos el vector t por ti = pt (φi | φ) + ((1− | Sim (ϕk, ϕ) - Sim (φi, φ) |) · Sim (ϕk, φ) con {φ1, φ2 ,..., φP} El conjunto de todas las observaciones posibles en el contexto de φ e i = 1 ,..., pag.t no es una distribución de probabilidad. El factor de multiplicación SIM (ϕ, φ) limita la variación de la probabilidad a aquellas fórmulas cuyo contexto ontológico no está demasiado lejos de la observación. El PT+1 posterior (φ | φ) se obtiene con la ecuación 3 con R (μ) definida como la normalización de t.El método de valoración: para un φk dado, wexp (φk) = pm j = 1 pt (φJ | φk) · w (φJ) es una expectativa de αs del valor de lo que se observará dado que β ha declarado que se observará φk, por alguna medida w.Ahora suponga que, como antes, α observa ϕk después de que el agente β ha declarado ϕ.α revisa la estimación previa de la valoración esperada WEXP (φk) a la luz de la observación ϕk a: (wrev (φk) | (ϕk | ϕ)) = g (wexp (φk), sim (φk, ϕ), w(φk), w (ϕ), wi (ϕk)) para alguna función g - la idea es, por ejemplo, que si la ejecución, ϕk, del compromiso, ϕ, para suministrar queso se devaluaba, entonces αs expectativa del valorde un compromiso, φ, para suministrar vino debería disminuir. Estimamos el posterior aplicando el principio de entropía relativa mínima como para la ecuación 3, donde la distribución P (μ) = P (φ | φ) satisface la restricción: P X J = 1 P (ϕ, ϕ) J · Wi (φJ) = g (wexp (φk), sim (φk, ϕ), w (φk), w (ϕ), wi (ϕk)) 5. El resumen mide un diálogo, ψt, entre los agentes α y β es una secuencia de expresiones interrelacionadas en el contexto. Una relación, ψ ∗ t, es una secuencia de diálogos. Primero medimos la confianza que un agente tiene para otro al observar, por cada enunciado, la diferencia entre lo que se dice (el enunciado) y lo que 1034 el sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) ocurre posteriormente (la observación). En segundo lugar, evaluamos cada diálogo a medida que avanza en términos del marco lógico: esta evaluación emplea las medidas de confianza. Finalmente, definimos la intimidad de una relación como una agregación del valor de sus diálogos componentes.5.1 Confianza Las medidas de confianza generalizan lo que comúnmente se llama confianza, confiabilidad y reputación en un solo marco computacional que abarca las categorías lógicas. En la Sección 5.2, se aplican medidas de confianza para valorar el cumplimiento de las promesas en la categoría de legitimidad: anteriormente llamamos a este honor [14], a la ejecución de compromisos, anteriormente llamamos a esta confianza [13] y a valorar los diálogos en la categoría de metas -Anteriormente llamamos esta fiabilidad [14]. Observaciones ideales. Considere una distribución de observaciones que representan αs ideales en el sentido de que es lo mejor que α podría esperar razonablemente observar. Esta distribución será una función del contexto αS con β denotado por E, y es Pt I (ϕ | ϕ, E). Aquí medimos la entropía relativa entre esta distribución ideal, Pt I (ϕ | ϕ, E) y la distribución de las observaciones esperadas, Pt (ϕ | ϕ). Es decir: c (α, β, ϕ) = 1 - x ϕ pt I (ϕ | ϕ, e) log pt i (ϕ | ϕ, e) pt (ϕ | ϕ) (4) donde el 1 es arbitrariamente un arbitrariamenteLa constante elegida es el valor máximo que puede tener esta medida. Esta ecuación mide la confianza para una sola declaración ϕ. Tiene sentido agregar estos valores sobre una clase de declaraciones, por ejemplo, sobre aquellos ϕ que están en el contexto ontológico o, es decir, ≤ o: c (α, β, o) = 1 - p ϕ: ϕ≤ β β β(ϕ) [1 - c (α, β, ϕ)] p ϕ: ϕ≤– pt β (ϕ) donde pt β (ϕ) es una distribución de probabilidad sobre el espacio de declaraciones que la siguiente declaración β hará a αes ϕ. Del mismo modo, para una estimación general de βs confianza en α: c (α, β) = 1 - x ϕ pt β (ϕ) [1 - c (α, β, ϕ)] observaciones preferidas. La medida anterior requiere que una distribución ideal, Pt I (ϕ | ϕ, e), debe especificarse para cada ϕ. Aquí medimos la medida en que la observación ϕ es preferible a la declaración original ϕ. Dado un predicado preferir (C1, C2, E), lo que significa que α prefiere C1 a C2 en el entorno e.Entonces si ϕ ≤ o: c (α, β, ϕ) = x ϕ pt (prefiera (ϕ, ϕ, o)) pt (ϕ | ϕ) y: c (α, β, o) = p ϕ: ϕ≤o pt β (ϕ) C (α, β, ϕ) P ϕ: ϕ≤ β (ϕ) certeza en la observación. Aquí medimos la consistencia en las observaciones aceptables esperadas, o la falta de incertidumbre esperada en esas posibles observaciones que son mejores que la declaración original. Si ϕ ≤ o deje: φ+(ϕ, o, κ) = ˘ ϕ |Pt (preferir (ϕ, ϕ, o))> κ ¯ para alguna constante κ, y: c (α, β, β) = 1 + 1 b ∗ · x ϕ ∈φ + (ϕ, o, κ) pt +(ϕ | ϕ) log pt +(ϕ | ϕ) donde pt +(ϕ | ϕ) es la normalización de pt (ϕ | ϕ) para ϕ ∈ φ +(ϕ, o, κ), b ∗ = (1 si| Φ+(ϕ, o, κ) | = 1 log | φ+(ϕ, o, κ) | De lo contrario, como anteriormente agregamos esta medida para las observaciones en un contexto particular o, y medimos la confianza como antes. Nota computacional. Las diversas medidas dadas anteriormente implican extensos cálculos. Por ejemplo, Eqn.4 Continsp ϕ que suma sobre todas las observaciones posibles ϕ. Obtenemos una medida más amigable computacionalmente apelando a la estructura de la ontología descrita en la Sección 3.2, y el lado derecho de la ecuación.4 se puede aproximar a: 1 - x ϕ: sim (ϕ, ϕ) ≥η pt η, i (ϕ | ϕ, e) log pt η, i (ϕ | ϕ, e) pt η (ϕ | ϕ) dondePt η, i (ϕ | ϕ, e) es la normalización de pt I (ϕ | ϕ, e) para sim (ϕ, ϕ) ≥ η, y de manera similar para pt η (ϕ | ϕ). La extensión de este cálculo está controlada por el parámetro η. Se puede obtener una restricción aún más ajustada con: SIM (ϕ, ϕ) ≥ η y ϕ ≤ ψ para algunos ψ.5.2 Valoración de diálogos de negociación Suponga que una negociación comienza en el momento s, y con el tiempo en una serie de enunciados, φt = μ1 ,..., μn se ha intercambiado entre el agente α y el agente β. Este diálogo de negociación es evaluado por α en el contexto del modelo mundial de αS en el momento S, MS y el entorno E que incluye enunciados que pueden haberse recibido de otros agentes en el sistema, incluidas las fuentes de información {θi}. Sea ψt = (φt, MS, E), luego α estima el valor de este diálogo en el contexto de MS y E como una matriz 2 × 5 Vα (ψt) donde: VX (ψt) = „il x (ψt) Io x (ψt) ig x (ψt) ii x (ψt) ic x (ψt) ul x (ψt) uo x (ψt) ug x (ψt) ui x (ψt) uc x (ψt) «donde i la i(·) Y U (·) Las funciones son medidas basadas en información y basadas en servicios públicos respectivamente como ahora describimos.α estima el valor de este diálogo a β como Vβ (ψt) suponiendo que el aparato de razonamiento βs refleja el suyo. En términos generales, las valoraciones basadas en la información miden la reducción de la incertidumbre, o la ganancia de información, que el diálogo le da a cada agente, se expresan en términos de disminución de la entropía que siempre se puede calcular. Las valoraciones basadas en la utilidad miden la ganancia de utilidad se expresan en términos de alguna función de evaluación de utilidad adecuada U (·) que puede ser difícil de definir. Esta es una razón por la cual el enfoque utilitario no tiene una extensión natural para el manejo de la argumentación que se logra aquí por nuestro enfoque basado en la información. Por ejemplo, si α recibe el enunciado hoy es el martes, entonces esto puede traducirse en una restricción en una sola distribución, y la disminución resultante en la entropía es la ganancia de información. Adjuntar una medida utilitaria a este enunciado puede no ser tan simple. Usamos la matriz de término 2 × 5 libremente para describir Vα en que los elementos de la matriz son listas de medidas que serán determinadas por los requisitos de los agentes. La Tabla 2 muestra una medida de muestra para cada una de las diez categorías, en ella, el diálogo comienza en el momento s y termina en el momento t.En esa tabla, U (·) es una función de evaluación de utilidad adecuada, las necesidades (β, χ) significa que el agente β necesita la necesidad χ, Cho (β, χ, γ) significa que el agente β satisface la necesidad χ al elegir negociar el sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1035 Con el agente γ, N es el conjunto de necesidades elegidas de la ontología en un nivel adecuado de abstracción, TT es el conjunto de ofertas en la mesa en el tiempo T, com (β, γ, b) significa que el agente β tiene un compromiso sobresaliente con el agente γ para ejecutar el compromiso B donde B se define en la ontología en un nivel adecuado de abstracción, B es el número de tales compromisos y hay n + 1 agentes enel sistema.5.3 Intimidad y equilibrio El equilibrio en un diálogo de negociación, ψt, se define como: Bαβ (ψt) = Vα (ψt) Vβ (ψt) para un operador de diferencia de elemento por elemento que respeta la estructura de V (ψt). La intimidad entre los agentes α y β, i ∗ t αβ, es el patrón de las dos matrices 2 × 5 v ∗ t α y v ∗ t β que se calculan por una función de actualización a medida que cada redonde de negociación termina, i ∗ t αβ =`V ∗ t α, V ∗ t β ´. Si ψT termina en el tiempo t: v ∗ t + 1 x = ν × vx (ψt) + (1 - ν) × V ∗ t x (5) donde ν es la velocidad de aprendizaje, y x = α, β.Además, v ∗ t x descompone continuamente por: v ∗ t + 1 x = τ × v ∗ t x + (1 - τ) × dx, donde x = α, β;τ es la tasa de descomposición, y DX es una matriz 2 × 5 que es la distribución del límite de desintegración para el valor al agente X de la intimidad de la relación en ausencia de cualquier interacción. DX es la reputación del agente x. El equilibrio de la relación entre los agentes α y β es: b ∗ t αβ = v ∗ t α v ∗ t β. En particular, la intimidad determina los valores para los parámetros G y H en la ecuación 1. Como un ejemplo simple, si aumentan tanto IO α (ψ ∗ t) como β β (ψ ∗ t), entonces g disminuye, y a medida que aumentan los ocho componentes lógicos basados en la información restantes, H aumenta H. La noción de equilibrio puede aplicarse a pares de enunciados tratándolos como diálogos degenerados. En la negociación simple de múltiples números múltiples, la estrategia de revelación de información equitativa generaliza la estrategia TIT-for-Tat en la negociación de un solo tema, y se extiende a una estrategia de argumentación de TIT-for Tat aplicando el mismo principio en todo el marco lógico.6. Estrategias y tácticas Cada negociación debe lograr dos objetivos. Primero puede tener la intención de lograr algún resultado contractual. En segundo lugar, tendrá como objetivo contribuir al crecimiento o disminución de la intimidad de la relación. Ahora describimos con mayor detalle el contenido del cuadro de negociación en la Figura 1. La literatura de negociación aconseja constantemente que el comportamiento de los agentes no debe ser predecible incluso en relaciones cercanas e íntimas. La variación requerida del comportamiento normalmente se describe como variando la postura de negociación que varía informalmente de un tipo amigable a un tipo duro. La postura se muestra en la Figura 1, inyecta el ruido aleatorio limitado en el proceso, donde el límite se tensa a medida que aumenta la intimidad. La postura, ST αβ, es una matriz 2 × 5 de multiplicadores elegidos al azar, cada uno ≈ 1, que perturba las acciones αS. El valor en la posición (x, y) en la matriz, donde x = i, u e y = l, o, g, i, c, se elige al azar de [1 l (i ∗ t αβ, x, y), L (i ∗ t αβ, x, y)] donde l (i ∗ t αβ, x, y) es el límite, e i ∗ t αβ es la intimidad. La estrategia de negociación se refiere a mantener un conjunto de opciones de trabajo. Si el conjunto de opciones está vacía, entonces α dejará la negociación.α perturba la maquinaria de aceptación (ver Sección 4) derivando S de la matriz ST αβ, como el valor en la posición (I, O). En línea con el comentario en la nota 7, en las primeras etapas de la negociación, α puede decidir inflar sus opciones de apertura. Esto se logra aumentando el valor de S en la ecuación 1. La siguiente estrategia utiliza la maquinaria descrita en la Sección 4. FIJA H, G, S y C, establezca las opciones en el conjunto vacío, deje que dt S = {δ |Pt (ACC (α, β, χ, Δ)> C}, luego: • Repita lo siguiente tantas veces como desee: Agregue δ = arg maxx {pt (acc (β, α, x)) | x ∈ S} a las opciones, eliminar {y ∈ Dt s | sim (y, δ) <k} para algunos k de dt s usando PT (ACC (β, α, δ)) Esta estrategia reacciona a la historia de βs de proposición y rechazo de las expresiones. Las tácticas de negociación se refieren a seleccionar algunas opciones y envolverlas en la argumentación. Las interacciones previas con el agente β habrán producido un patrón de intimidad expresado en forma de `v ∗ t α, v ∗ t β ´. Suponga que el objetivo de relación es (t ∗ t α, t ∗ t β). Después de la ecuación 5, α querrá lograr un objetivo de negociación, Nβ (ψt) tal que: ν · nβ (ψt) + (1 - ν) · v ∗ t β es un poco en el lado t ∗ t β de V∗ T β: Nβ (ψt) = ν - κ ν v ∗ t β ⊕ κ ν t ∗ t β (6) para κ ∈ Small [0, ν] que representa la tasa de desarrollo deseada de αS para su relación con β.Nβ (ψT) es una matriz 2 × 5 que contiene variaciones en las dimensiones lógicas que α le gustaría revelar a β durante ψt (p. Pasaré un poco más de información sobre las opciones de lo habitual, seré más fuerte en comisiones en opciones, etc.). Es razonable esperar que β progrese hacia su objetivo a la misma velocidad y Nα (ψt) se calcula reemplazando β por α en la ecuación 6. Nα (ψt) es lo que α espera recibir de β durante ψt. Esto proporciona un objetivo de equilibrio de negociación de: Nα (ψt) Nβ (ψT) que puede usarse como base para las tácticas reactivas al esforzarse por mantener este equilibrio en las dimensiones lógicas. Una táctica cautelosa podría usar el equilibrio para unir la respuesta μ a cada expresión μ de β por la restricción: Vα (μ) Vβ (μ) ≈ st αβ ⊗ (Nα (ψt) nβ (ψt)), donde ⊗ es elemento-Multiplicación de la matriz del elemento parcial, y ST αβ es la postura. Una táctica menos neurótica podría intentar lograr el equilibrio de negociación objetivo sobre el diálogo completo anticipado. Si un límite de equilibrio requiere revelación de información negativa en una categoría lógica, entonces α no contribuirá nada a ella, y dejará esto a la decadencia natural a la reputación D como se describió anteriormente.7. Discusión En este documento, hemos introducido un enfoque novedoso para la negociación que utiliza información y medidas teóricas del juego basadas en estudios comerciales y psicológicos. Presenta los conceptos de intimidad y equilibrio como elementos clave para comprender qué es una estrategia de negociación y una táctica. La negociación se entiende como un diálogo que afecta cinco dimensiones básicas: legitimidad, opciones, objetivos, independencia y compromiso. Cada movimiento dialógico produce un cambio en una matriz 2 × 5 que evalúa el diálogo a lo largo de cinco medidas basadas en la información y cinco medidas basadas en servicios públicos. El equilibrio actual e niveles de intimidad y los niveles deseados, u objetivos, son utilizados por las tácticas para determinar qué decir a continuación. Actualmente estamos explorando el uso de este modelo como una extensión de un software Eprocurement Eprocurement actualizado actualmente comercializado por Isoco, una compañía spin-off del laboratorio de uno de los autores.1036 El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (aamas 07) IL α (ψt) = x ϕ∈ψt ct (α, β, ϕ)-cs (α, β, ϕ) ul α (ψt) = x ϕ∈ψt xϕ pt β (ϕ | ϕ) × uα (ϕ) α (ψt) = p Δ∈T t hs (ACC (β, α, Δ)) - p Δ∈T t ht (ACC (β, α, Δ, δ)) | Tt |Uo α (ψt) = x Δ∈T t pt (ACC (β, α, Δ)) × x δ pt (Δ | δ) uα (δ) IG α (ψt) = p χ∈N HS (necesidades (β (β, χ)) - Ht (necesidades (β, χ)) | N |UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) - HT (CHO (β, χ, βi)) n × | n |Ui α (ψt) = ox i = 1 x χ∈N Ut (Cho (β, χ, βi)) - EE. UU.B HS (COM (β, βi, b)) - HT (COM (β, βi, b)) n × | B |UC α (ψT) = ox I = 1 x Δ∈B Ut (com (β, βi, b)) - EE. UU. (COM (β, βi, b)) Tabla 2: Medidas de muestra para cada categoría en Vα (ψT).(Del mismo modo para Vβ (ψt).). Agradecimientos Carles Sierra es parcialmente respaldado por el Proyecto Europeo de Strep Open Knowledge y por el Proyecto IEA español.8. Referencias [1] Adams, J. S. Inequidad en el intercambio social. En Avances en Psicología Social Experimental, L. Berkowitz, ed., Vol.2. Nueva York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J. A., y Sierra, C. Ingeniería ambiental para sistemas multiagentes. Revista sobre aplicaciones de ingeniería de inteligencia artificial 18 (2005).[3] Bazerman, M. H., Loewenstein, G. F. y White, S. B. Reversión de la preferencia en las decisiones de asignación: juzgar una alternativa versus elección entre alternativas. Administration Science Quarterly, 37 (1992), 220-240.[4] Brandenburger, A. y Nalebuff, B. Coopetición: una mentalidad de revolución que combina competencia y cooperación. Doubleday, Nueva York, 1996. [5] Cheeseman, P. y Stutz, J. Inferencia bayesiana y métodos de entropía máxima en ciencia e ingeniería. Instituto Americano de Física, Melville, NY, EE. UU., 2004, cap. Sobre la relación entre la inferencia de entropía bayesiana y máxima, pp. 445461. [6] Debenham, J. Negociación con información. En procedimientos, tercera conferencia internacional sobre agentes autónomos y sistemas de agentes múltiples AAMAS -2004 (julio de 2004), N. Jennings, C. Sierra, L. Sonenberg y M. Tambe, eds., ACM Press, Nueva York, pp. 664 -671. [7] Fischer, R., Ury, W. y Patton, B. Llegar a sí: negociar acuerdos sin ceder. Penguin Books, 1995. [8] Kalfoglou, Y. y Schorlemmer, M. If-Map: un método de mapeo de ontología basado en la teoría de flujo de información. En Journal on Data Semantics I, S. Spaccapietra, S. March y K. Aberer, Eds., Vol.2800 de las notas de conferencia en informática. Springer-Verlag: Heidelberg, Alemania, 2003, pp. 98-127.[9] Lewicki, R. J., Saunders, D. M. y Minton, J. W. Essentials de la negociación. McGraw Hill, 2001. [10] Li, Y., Bandar, Z. A. y McLean, D. Un enfoque para medir la similitud semántica entre las palabras utilizando múltiples fuentes de información. Transacciones IEEE sobre conocimiento e ingeniería de datos 15, 4 (julio / agosto de 2003), 871 - 882. [11] Mackay, D. Teoría de la información, inferencia y algoritmos de aprendizaje. Cambridge University Press, 2003. [12] París, J. Sentido común y entropía máxima. Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C. y Debenham, J. Un modelo basado en información para la confianza. En Actas La cuarta conferencia internacional sobre agentes autónomos y sistemas de agentes múltiples AAMAS-2005 (Utrecht, Países Bajos, julio de 2005), F. dignum, V. dignum, S. Koenig, S. Kraus, M. Singh y M. Wooldridge,Eds., ACM Press, Nueva York, pp. 497 - 504. [14] Sierra, C. y Debenham, J. Confianza y honor en la agencia basada en la información. En Actas de la Quinta Conferencia Internacional sobre Agentes Autónomos y Sistemas de Agentes Múltiples AAMAS -2006 (Hakodate, Japón, mayo de 2006), P. Stone y G. Weiss, eds., ACM Press, Nueva York, pp. 1225 - 1232. [15]Sierra, C. y Debenham, J. Agencia basada en información. En Actas de la Twentieth International Connation Conference sobre Inteligencia Artificial IJCAI-07 (Hyderabad, India, enero de 2007), pp. 1513-1518.[16] Sondak, H., Neale, M. A. y Pinkley, R. Las asignaciones negociadas de beneficios y cargas: el impacto de la valencia de resultado, la contribución y la relación. Comportamiento organizacional y procesos de decisión humana, 3 (diciembre de 1995), 249-260.[17] Valley, K. L., Neale, M. A. y Mannix, E. A. Amigos, amantes, colegas, extraños: los efectos de las relaciones en el proceso y el resultado de las negociaciones. En investigación en negociación en organizaciones, R. Bies, R. Lewicki y B. Sheppard, eds., Vol.5. Jai Press, 1995, pp. 65-94. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1037",
    "original_sentences": [
        "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
        "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
        "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
        "The balance is the difference between these matrices.",
        "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
        "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
        "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
        "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
        "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
        "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
        "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
        "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
        "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
        "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
        "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
        "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
        "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
        "In this sense, humans aim at a general sense of fairness in an interaction.",
        "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
        "Then, in Section 3 we introduce the negotiation language.",
        "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
        "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
        "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
        "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
        "What information is relevant to the negotiation process?",
        "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
        "What are the possible agreements we can accept? • Goals.",
        "What are the underlying things we need or care about?",
        "What are our goals? • Independence.",
        "What will we do if the negotiation fails?",
        "What alternatives have we got? • Commitment.",
        "What outstanding commitments do we have?",
        "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
        "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
        "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
        "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
        "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
        "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
        "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
        "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
        "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
        "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
        "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
        "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
        "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
        "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
        "This might be part of our social evolution.",
        "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
        "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
        "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
        "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
        "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
        "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
        "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
        "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
        "C is a finite set of concept symbols (including basic data types); 2.",
        "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
        "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
        "Usually, food is accumulated at the shelter for future use.",
        "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
        "We will omit this here.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
        "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
        "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
        "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
        "Semantic distance plays a fundamental role in strategies for information-based agency.",
        "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
        "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
        "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
        "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
        "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
        "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
        "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
        "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
        "Norms, contracts, and information have an obvious temporal dimension.",
        "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
        "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
        "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
        "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
        "Representing an ontology as a set predicates in Prolog is simple.",
        "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
        "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
        "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
        "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
        "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
        "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
        "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
        "A generic information-based architecture is described in detail in [15].",
        "The LOGIC agent architecture is shown in Figure 1.",
        "Agent α acts in response to a need that is expressed in terms of the ontology.",
        "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
        "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
        "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
        "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
        "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
        "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
        "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
        "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
        "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
        "We now describe two of the distributions in Mt that support offer exchange.",
        "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
        "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
        "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
        "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
        "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
        "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
        "The parameters g and h are independent.",
        "We can imagine a relationship that begins with g = 1 and h = 0.",
        "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
        "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
        "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
        "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
        "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
        "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
        "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
        "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
        "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
        "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
        "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
        "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
        "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
        "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
        "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
        "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
        "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
        "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
        "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
        "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
        "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
        "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
        "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
        "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
        "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
        "A relationship, Ψ∗t , is a sequence of dialogues.",
        "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
        "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
        "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
        "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
        "Ideal observations.",
        "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
        "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
        "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
        "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
        "This equation measures confidence for a single statement ϕ.",
        "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
        "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
        "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
        "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
        "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
        "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
        "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
        "Computational Note.",
        "The various measures given above involve extensive calculations.",
        "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
        "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
        "The extent of this calculation is controlled by the parameter η.",
        "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
        "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
        "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
        "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
        "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
        "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
        "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
        "Attaching a utilitarian measure to this utterance may not be so simple.",
        "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
        "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
        "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
        "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
        "Dx is the reputation of agent x.",
        "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
        "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
        "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
        "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
        "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
        "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
        "First it may be intended to achieve some contractual outcome.",
        "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
        "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
        "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
        "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
        "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
        "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
        "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
        "The negotiation strategy is concerned with maintaining a working set of Options.",
        "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
        "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
        "This is achieved by increasing the value of s in Equation 1.",
        "The following strategy uses the machinery described in Section 4.",
        "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
        "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
        "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
        "Suppose that the relationship target is (T∗t α , T∗t β ).",
        "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
        "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
        "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
        "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
        "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
        "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
        "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
        "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
        "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
        "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
        "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
        "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
        "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
        "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
        "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
        "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
        "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
        "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
        "A., and Sierra, C. Environment engineering for multiagent systems.",
        "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
        "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
        "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
        "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
        "American Institute of Physics, Melville, NY, USA, 2004, ch.",
        "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
        "Bargaining with information.",
        "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
        "Getting to Yes: Negotiating agreements without giving in.",
        "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
        "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
        "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
        "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
        "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
        "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
        "Cambridge University Press, 2003. [12] Paris, J.",
        "Common sense and maximum entropy.",
        "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
        "An information-based model for trust.",
        "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
        "Trust and honour in information-based agency.",
        "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
        "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
        "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
        "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
        "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
        "JAI Press, 1995, pp. 65-94.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
    ],
    "error_count": 0,
    "keys": {
        "successive negotiation encounter": {
            "translated_key": "Encuentro de negociación sucesivo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on <br>successive negotiation encounter</br>s.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Introducción En este documento proponemos un nuevo modelo de negociación para tratar las relaciones a largo plazo que se basan en el \"encuentro de negociación sucesivo\" s."
            ],
            "translated_text": "",
            "candidates": [
                "Encuentro de negociación sucesivo",
                "encuentro de negociación sucesivo"
            ],
            "error": []
        },
        "long term relationship": {
            "translated_key": "relación a largo plazo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with <br>long term relationship</br>s that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Introducción En este documento proponemos un nuevo modelo de negociación para lidiar con las \"relaciones a largo plazo\" que se basan en sucesivos encuentros de negociación."
            ],
            "translated_text": "",
            "candidates": [
                "relación a largo plazo",
                "relaciones a largo plazo"
            ],
            "error": []
        },
        "utterance": {
            "translated_key": "declaración",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an <br>utterance</br> and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing <br>utterance</br> μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an <br>utterance</br> accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between <br>utterance</br>, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an <br>utterance</br> μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the <br>utterance</br> μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each <br>utterance</br>, the difference between what is said (the <br>utterance</br>) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the <br>utterance</br> Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this <br>utterance</br> may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each <br>utterance</br> μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En la Sección 4.1, esto nos permite medir la diferencia entre una \"expresión\" y una observación posterior.",
                "Cada negociación consiste en un diálogo, ψt, entre dos agentes con agente α que contribuye a \"enunciado\" μ y la parte 6 cada uno de los planes y reacciones αs contiene constructores para un modelo mundial inicial MT.",
                "Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1033 en el grado en que un \"enunciado\" describe con precisión lo que se observará posteriormente.",
                "Representamos la relación entre \"enunciado\", ϕ, y la observación posterior, ϕ, por Pt (ϕ | ϕ) ∈ Mt, donde ϕ y ϕ pueden ser categorías ontológicas en interés de la viabilidad computacional.",
                "Suponga que α recibe una \"expresión\" μ = ILLOC (α, β, ϕ, t) del agente β en el tiempo t.Suponga que α une una creencia epistémica RT (α, β, μ) a μ: esta probabilidad tiene en cuenta el nivel de precaución personal de αS.",
                "Finalmente incorporando la ecuación.2 Obtenemos el método para actualizar una distribución PT (ϕ | ϕ) al recibir un mensaje μ: PT+1 (ϕ | ϕ) = ΔI (D (ϕ | ϕ), R (μ)) (3) Este procedimientoSe ocupa de la descomposición de la integridad, y con dos probabilidades: primero, la probabilidad z en la \"expresión\" μ, y en segundo lugar con la creencia RT (α, β, μ) que α se unía a μ.4.3 Actualización de PT (φ | φ) Dada ϕ el método SIM: Dado como arriba μ = Illoc (α, β, ϕ, t) y la observación ϕk definimos el vector t por ti = pt (φi | φ) + ((1− | Sim (ϕk, ϕ) - Sim (φi, φ) |) · Sim (ϕk, φ) con {φ1, φ2 ,..., φP} El conjunto de todas las observaciones posibles en el contexto de φ e i = 1 ,..., pag.t no es una distribución de probabilidad.",
                "Primero medimos la confianza que un agente tiene para otro al observar, para cada \"enunciado\", la diferencia entre lo que se dice (la \"expresión\") y lo que 1034 el sexto intl.",
                "Por ejemplo, si α recibe el \"enunciado\" hoy es el martes, entonces esto puede traducirse en una restricción en una sola distribución, y la disminución resultante en la entropía es la ganancia de la información.",
                "Adjuntar una medida utilitaria a este \"enunciado\" puede no ser tan simple.",
                "Una táctica cautelosa podría usar el equilibrio para unir la respuesta μ a cada \"expresión\" μ de β por la restricción: Vα (μ) Vβ (μ) ≈ st αβ ⊗ (Nα (ψt) Nβ (ψt)), donde ⊗ ISMultiplicación de la matriz de elemento por elemento, y ST αβ es la postura."
            ],
            "translated_text": "",
            "candidates": [
                "declaración",
                "expresión",
                "declaración",
                "enunciado",
                "declaración",
                "enunciado",
                "declaración",
                "enunciado",
                "declaración",
                "expresión",
                "declaración",
                "expresión",
                "declaración",
                "enunciado",
                "expresión",
                "declaración",
                "enunciado",
                "declaración",
                "enunciado",
                "declaración",
                "expresión"
            ],
            "error": []
        },
        "utilitarian interpretation": {
            "translated_key": "interpretación utilitaria",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a <br>utilitarian interpretation</br> in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Además, varias expresiones pueden tener una \"interpretación utilitaria\" en el sentido de que un agente puede asociarles una ganancia preferencial."
            ],
            "translated_text": "",
            "candidates": [
                "interpretación utilitaria",
                "interpretación utilitaria"
            ],
            "error": []
        },
        "ontology": {
            "translated_key": "Ontología",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 <br>ontology</br> In order to define a language to structure agent dialogues we need an <br>ontology</br> that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an <br>ontology</br> depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an <br>ontology</br> as a set predicates in Prolog is simple.",
                "The set term contains instances of the <br>ontology</br> concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the <br>ontology</br>.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the <br>ontology</br> described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the <br>ontology</br> at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the <br>ontology</br> at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An <br>ontology</br>-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Modelo de comunicación 3.1 \"Ontología\" para definir un lenguaje para estructurar los diálogos de agentes necesitamos una \"ontología\" que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizado en un IS-ALa jerarquía (por ejemplo, el platipio es un mamífero, el dólar australiano es una moneda), y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio (cerveza, AUD)). 3 Modelamos ontologías después de un enfoque algebraico [8] como: una ontología esuna tupla o = (c, r, ≤, σ) donde: 1.",
                "La distancia semántica entre conceptos dentro de una \"ontología\" depende de cuán lejos estén en la estructura definida por la relación ≤.",
                "Representar una \"ontología\" como predicados establecidos en Prolog es simple.",
                "El término establecido contiene instancias de los conceptos y relaciones de \"ontología \".5 Por ejemplo, podemos representar la siguiente oferta: si gasta un total de más de E100 en mi tienda durante octubre, le daré un descuento del 10% en todosBienes en noviembre, como: Oferta (α, β, gastado (β, α, octubre, x) ∧ x ≥ E100 → ∀ y.",
                "El agente α actúa en respuesta a una necesidad que se expresa en términos de la \"ontología\".",
                "Obtenemos una medida más amigable computacionalmente apelando a la estructura de la \"ontología\" descrita en la Sección 3.2 y el lado derecho de la ecuación.4 se puede aproximar a: 1 - x ϕ: sim (ϕ, ϕ) ≥η pt η, i (ϕ | ϕ, e) log pt η, i (ϕ | ϕ, e) pt η (ϕ | ϕ) dondePt η, i (ϕ | ϕ, e) es la normalización de pt I (ϕ | ϕ, e) para sim (ϕ, ϕ) ≥ η, y de manera similar para pt η (ϕ | ϕ).",
                "Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1035 Con el agente γ, n es el conjunto de necesidades elegidas de la \"ontología\" en un nivel adecuado de abstracción, TT es el conjunto de ofertas en la mesa en el tiempo T, com es(β, γ, b) significa que el agente β tiene un compromiso sobresaliente con el agente γ para ejecutar el compromiso B donde B se define en la \"ontología\" en un nivel adecuado de abstracción, B es el número de tales compromisos, y hay N+ 1 Agentes en el sistema.5.3 Intimidad y equilibrio El equilibrio en un diálogo de negociación, ψt, se define como: Bαβ (ψt) = Vα (ψt) Vβ (ψt) para un operador de diferencia de elemento por elemento que respeta la estructura de V (ψt).",
                "Penguin Books, 1995. [8] Kalfoglou, Y. y Schorlemmer, M. if-map: un método de mapeo de \"ontología\" basado en la teoría del flujo de información."
            ],
            "translated_text": "",
            "candidates": [
                "Ontología",
                "Ontología",
                "ontología",
                "ontología",
                "ontología",
                "ontología",
                "ontología",
                "Ontología",
                "ontología ",
                "ontología",
                "ontología",
                "ontología",
                "ontología",
                "ontología",
                "ontología",
                "ontología",
                "ontología",
                "ontología"
            ],
            "error": []
        },
        "set predicate": {
            "translated_key": "predicado establecido",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a <br>set predicate</br>s in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Representar una ontología como un \"predicado establecido\" s en Prolog es simple."
            ],
            "translated_text": "",
            "candidates": [
                "Establecer predicado",
                "predicado establecido"
            ],
            "error": []
        },
        "multiagent system": {
            "translated_key": "sistema multiagente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A <br>multiagent system</br> {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Arquitectura del agente Un \"sistema multiagente\" {α, β1 ,..., βn, ξ, θ1 ,..., θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, información que proporciona agentes, θj y un agente institucional, ξ, que representa a la institución donde asumimos que las interacciones ocurren [2]."
            ],
            "translated_text": "",
            "candidates": [
                "sistema multiagente",
                "sistema multiagente"
            ],
            "error": []
        },
        "logic agent architecture": {
            "translated_key": "Arquitectura del agente lógico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The <br>logic agent architecture</br> 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The <br>logic agent architecture</br> is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Figura 1: La \"Arquitectura del agente lógico\" 4.",
                "La \"arquitectura del agente lógico\" se muestra en la Figura 1."
            ],
            "translated_text": "",
            "candidates": [
                "Arquitectura del agente lógico",
                "Arquitectura del agente lógico",
                "Arquitectura del agente lógico",
                "arquitectura del agente lógico"
            ],
            "error": []
        },
        "negotiation strategy": {
            "translated_key": "estrategia de negociación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The <br>negotiation strategy</br> maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The <br>negotiation strategy</br> then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The <br>negotiation strategy</br> is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a <br>negotiation strategy</br> and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La \"estrategia de negociación\" mantiene un conjunto de opciones que están en línea con el nivel de intimidad actual, y luego las tácticas envuelven las opciones en la argumentación con el objetivo de alcanzar un acuerdo exitoso y manipular los sucesivos saldos de negociación hacia la intimidad objetivo.",
                "La \"estrategia de negociación\" determina el conjunto actual de opciones {Δi}, y luego las tácticas, guiadas por el objetivo de negociación, deciden cuál, si alguna, de estas opciones, las presenta y las envuelve en el diálogo de argumentación: ver sección 6.",
                "La \"Estrategia de negociación\" se refiere a mantener un conjunto de opciones de trabajo.",
                "Presenta los conceptos de intimidad y equilibrio como elementos clave para comprender qué es una \"estrategia de negociación\" y táctica."
            ],
            "translated_text": "",
            "candidates": [
                "estrategia de negociación",
                "estrategia de negociación",
                "estrategia de negociación",
                "estrategia de negociación",
                "estrategia de negociación",
                "Estrategia de negociación",
                "estrategia de negociación",
                "estrategia de negociación"
            ],
            "error": []
        },
        "view of acceptability": {
            "translated_key": "Vista de la aceptabilidad",
            "is_in_text": false,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "acceptability view": {
            "translated_key": "Vista de aceptabilidad",
            "is_in_text": false,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "acceptance criterion": {
            "translated_key": "criterio de aceptación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the <br>acceptance criterion</br> has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La base del \"criterio de aceptación\" se ha desarrollado de la equidad a la igualdad, y luego a la necesidad."
            ],
            "translated_text": "",
            "candidates": [
                "criterio de aceptación",
                "criterio de aceptación"
            ],
            "error": []
        },
        "component dialogue": {
            "translated_key": "diálogo de componentes",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its <br>component dialogue</br>s. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Finalmente definimos la intimidad de una relación como una agregación del valor de su \"diálogo de componentes\" s.5.1 Confianza Las medidas de confianza generalizan lo que comúnmente se llama confianza, confiabilidad y reputación en un solo marco computacional que abarca las categorías lógicas."
            ],
            "translated_text": "",
            "candidates": [
                "diálogo de componentes",
                "diálogo de componentes"
            ],
            "error": []
        },
        "confidence measure": {
            "translated_key": "medida de confianza",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the <br>confidence measure</br>s.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 <br>confidence measure</br>s are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En segundo lugar, evaluamos cada diálogo a medida que avanza en términos del marco lógico: esta evaluación emplea la \"medida de confianza\" s.",
                "En la sección 5.2, se aplican \"medidas de confianza\" al valorar el cumplimiento de las promesas en la categoría de legitimidad: anteriormente llamamos este honor [14], a la ejecución de compromisos, anteriormente llamamos a esta confianza [13], y a valorar diálogos en elCategoría de objetivos: anteriormente llamamos esta fiabilidad [14]."
            ],
            "translated_text": "",
            "candidates": [
                "medida de confianza",
                "medida de confianza",
                "medida de confianza",
                "medidas de confianza"
            ],
            "error": []
        },
        "negotiation": {
            "translated_key": "negociación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC <br>negotiation</br> Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a <br>negotiation</br> model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The <br>negotiation</br> strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive <br>negotiation</br> balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new <br>negotiation</br> model to deal with long term relationships that are founded on successive <br>negotiation</br> encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that <br>negotiation</br> is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human <br>negotiation</br> procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under <br>negotiation</br>, but instead we belief that the willingness to co-operate/compete affect all aspects in the <br>negotiation</br> process.",
                "<br>negotiation</br> strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a <br>negotiation</br> with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human <br>negotiation</br> modelling that we cover in this work.",
                "Then, in Section 3 we introduce the <br>negotiation</br> language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the <br>negotiation</br>.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN <br>negotiation</br> Before a <br>negotiation</br> starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the <br>negotiation</br> process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the <br>negotiation</br> fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "<br>negotiation</br> dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any <br>negotiation</br> process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a <br>negotiation</br> give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our <br>negotiation</br> opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their <br>negotiation</br> relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in <br>negotiation</br> or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a <br>negotiation</br> dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the <br>negotiation</br> by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each <br>negotiation</br> consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human <br>negotiation</br>, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The <br>negotiation</br> strategy then determines the current set of Options {δi}, and then the tactics, guided by the <br>negotiation</br> target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent <br>negotiation</br> terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing <br>negotiation</br> dialogues Suppose that a <br>negotiation</br> commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This <br>negotiation</br> dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a <br>negotiation</br> dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each <br>negotiation</br> round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each <br>negotiation</br> has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the <br>negotiation</br> box in Figure 1.",
                "The <br>negotiation</br> literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the <br>negotiation</br> stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The <br>negotiation</br> strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the <br>negotiation</br>. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the <br>negotiation</br> α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "<br>negotiation</br> tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a <br>negotiation</br> target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a <br>negotiation</br> balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target <br>negotiation</br> balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to <br>negotiation</br> that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a <br>negotiation</br> strategy and tactic.",
                "<br>negotiation</br> is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of <br>negotiation</br>.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in <br>negotiation</br> in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El modelo de \"negociación\" de la lógica Carles Sierra Institut Dinvestigacio en Intel. Ligencia Consejo Artificial de Investigación Científica Española, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es Facultad de Tecnología de Tecnología de la Información John Debenham Universidad de Tecnología de la Información, Sydney NSW, Australia Debenham@It.uts.edu.au Resumen Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: legitimidad, opciones, objetivos, independencia y compromiso, (lógica).",
                "Introducimos un modelo de \"negociación\" basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad).",
                "La estrategia de \"negociación\" mantiene un conjunto de opciones que están en línea con el nivel de intimidad actual, y luego las tácticas envuelven las opciones en la argumentación con el objetivo de alcanzar un acuerdo exitoso y manipular la sucesiva \"negociación\" saldos hacia la intimidad objetivo.",
                "Introducción En este documento proponemos un nuevo modelo de \"negociación\" para lidiar con relaciones a largo plazo que se basan en encuentros sucesivos de \"negociación\".",
                "El modelo se basa en los resultados de los estudios comerciales y psicológicos [1, 16, 9], y reconoce que la \"negociación\" es un proceso de intercambio de información, así como un proceso de intercambio de servicios públicos [15, 14].",
                "Nuestro objetivo es modelar escenarios comerciales en los que los agentes representan a sus directores humanos y, por lo tanto, queremos que su comportamiento sea comprensible por los humanos y respetar los procedimientos habituales de \"negociación\" humanos, al tiempo que es consistente y de alguna manera extendiendo los resultados teóricos e informativos teóricos de la información..",
                "No seguimos la cooperación de APROACH [4] donde la cooperación y la competencia dependen del tema bajo \"negociación\", sino que creemos que la voluntad de cooperar/competir afecta todos los aspectos en el proceso de \"negociación\".",
                "Las estrategias de \"negociación\" pueden verse naturalmente como procedimientos que seleccionan tácticas utilizadas para lograr un acuerdo exitoso y alcanzar un nivel de intimidad objetivo.",
                "Es común en entornos humanos usar tácticas que compensan los desequilibrios en una dimensión de una \"negociación\" con desequilibrio en otra dimensión.",
                "En la Sección 2 describimos los aspectos del modelado de \"negociación\" humanos que cubrimos en este trabajo."
            ],
            "translated_text": "",
            "candidates": [
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación",
                "negociación"
            ],
            "error": []
        }
    }
}