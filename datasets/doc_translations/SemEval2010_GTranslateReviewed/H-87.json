{
    "id": "H-87",
    "original_text": "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering. Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings. We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function. Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11. Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1. INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval. The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest. Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate. The above conditions attempt to mimic realistic situations where an AF system would be used. That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback. Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing. These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs. None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once. The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13]. Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated. Addressing the third issue is the main focus in this paper. We argue that robustness is an important measure for evaluating and comparing AF methods. By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora. Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts. Available training examples, on the other hand, are often insufficient for tuning the parameters. In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective. This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set. Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other. Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters? Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported. In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR). Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13]. Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14]. It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1). Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus. Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing. Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora. The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study. Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences. Section 4 outlines the Rocchio and LR approaches to AF, respectively. Section 5 reports the experiments and results. Section 6 concludes the main findings in this study. 2. BENCHMARK CORPORA We used four benchmark corpora in our study. Table 1 shows the statistics about these data sets. TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7]. The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set. TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets. The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8]. TDT3 was the evaluation benchmark in the TDT2001 dry run1 . The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998. Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well. The splitting point for training-test sets is different for each topic in TDT. TDT5 was the evaluation benchmark in TDT2004 [4]. The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories. We only used the English versions of those documents in our experiments for this paper. The TDT topics differ from TREC topics both conceptually and statistically. Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories. The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics. Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one. The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting. For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa. Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3. METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents. The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002). For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively. The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics. For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk). To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms. In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric. To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper. Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function. Our objective is to maximize the former or to minimize the latter on test documents. The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions. For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU. The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU. More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU. That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme. At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w . However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus. Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets. Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking. To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth. Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk. Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1. The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied. Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall. This was a challenging part of the TDT2004 evaluation for AF. Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods. This is the first time this issue is explicitly analyzed, to our knowledge. 4. METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights. The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights. The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid. The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype. The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic. If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype. Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF). To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback. The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic. Multiple approaches have been developed. The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase. More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11]. It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF. Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization. Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase. This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1). Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic. Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14]. Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation. If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics. We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents. The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix. Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5]. How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range. The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5. EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004. Multiple research teams participated and multiple runs from each team were allowed. Ctrk and TDT5SU were used as the metrics. Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively. Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU. All the parameters of our runs were tuned on the TDT3 corpus. Results for other sites are also listed anonymously for comparison. Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.) We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ). CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.) Adaptive filtering without using true relevance feedback was also a part of the evaluations. In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made. Such a setting has been conventional for the Topic Tracking task in TDT until 2004. Figure 4 shows the summarized official submissions from each team. Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question. Both Rocchio and LR have parameters that must be prespecified before the AF process. The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector. The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR. Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation. Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f. Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2). We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004. We also tested our methods on TREC10 and TREC11 for further analysis. Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters. We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied. These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal. If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004. The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem. Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate. Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively. Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings. With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ. Table 2 summarizes the results 3 . We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU. For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11. From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0. This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report. More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR. The robustness, we believe, comes from the probabilistic nature of the system-generated scores. That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem. Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not. Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases. This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR. We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6. Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda. The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3. The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,. In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR. In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed. In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning. Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes. We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications. To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3. Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information. Incremental LR, on the other hand, was weaker but still impressive. Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR. For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost. The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk. Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio. Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6. CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization. Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme. For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting. Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No. NBCHD030010. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7. REFERENCES [1] J. Allan. Incremental relevance feedback for information filtering. In SIGIR-96, 1996. [2] J. Callan. Learning while filtering documents. In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington. Topic detection and tracking overview. In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley. Overview of the TDT 2004 Evaluation and Results. In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning. Springer, 2001. [6] S. Robertson and D. Hull. The TREC-9 filtering track final report. In TREC-9, 2000. [7] S. Robertson and I. Soboroff. The TREC-10 filtering track final report. In TREC-10, 2001. [8] S. Robertson and I. Soboroff. The TREC 2002 filtering track report. In TREC-11, 2002. [9] S. Robertson and S. Walker. Microsoft Cambridge at TREC-9. In TREC-9, 2000. [10] R. Schapire, Y. Singer and A. Singhal. Boosting and Rocchio applied to text filtering. In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel. Margin-based local regression for adaptive filtering. In CIKM-03, 2003. [12] Y. Zhang and J. Callan. Maximum likelihood estimation for filtering thresholds. In SIGIR-01, 2001. [13] Y. Zhang. Using Bayesian priors to combine classifiers for adaptive filtering. In SIGIR-04, 2004. [14] J. Zhang and Y. Yang. Robustness of regularized linear classification methods in text categorization. In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Text Categorization Based on Regularized Linear Classification Methods. Inf. Retr. 4(1): 5-31 (2001).",
    "original_translation": "Robustez de los métodos de filtrado adaptativo en una evaluación transversal Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. Resumen Este documento informa una evaluación transversal de la evaluación de billetes de deRegresión logística regularizada (LR) y rocho incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de detección y seguimiento de temas (TDT) y las conferencias de recuperación de texto (TREC) evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Descubrimos que LR funciona con fuerza y robusta en la optimización de T11SU (una función de utilidad TREC), mientras que Rocchio es mejor para optimizar CTRK (el costo de seguimiento de TDT), una función objetivo orientada de alta recuperación. Utilizando la optimización sistemática de parámetros de Cross-Corpus con ambos métodos, obtuvimos los mejores resultados jamás informados en TDT5, TREC10 y TREC11. La retroalimentación de relevancia sobre una pequeña porción (0.05 ~ 0.2%) de los documentos de prueba TDT5 arrojaron mejoras de rendimiento significativas, midiendo una reducción de hasta 54% en CTRK y un aumento del 20.9% en T11SU (con β = 0.1), en comparación con los resultados deEl sistema de rendimiento superior en TDT2004 sin información de retroalimentación de relevancia. Categorías y descriptores de temas H.3.3 [Búsqueda y recuperación de información]: filtrado de información, comentarios de relevancia, modelos de recuperación, proceso de selección;I.5.2 [Metodología de diseño]: Diseño y evaluación del clasificador Algoritmos de términos generales, medición, rendimiento, experimentación 1. Introducción El filtrado adaptativo (AF) ha sido un tema de investigación desafiante en la recuperación de la información. La tarea es que el sistema tome una decisión de membresía del tema en línea (sí o no) para cada documento, tan pronto como llega, con respecto a cada tema de interés predefinido. A partir de 1997, en el área de detección y seguimiento del tema (TDT) y 1998 en las conferencias de recuperación de texto (TREC), las evaluaciones de referencia han sido realizadas por NIST en las siguientes condiciones [6] [7] [8] [3] [4]: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivo para cada tema en el punto de partida.• La retroalimentación de relevancia estaba disponible, pero solo para los documentos del sistema (con una decisión Sí) en las evaluaciones de TREC para FA.• La retroalimentación de relevancia (RF) no se permitió en las evaluaciones de TDT para la AF (o el seguimiento de temas en la terminología TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas TREC y TDT se usaron conjuntamente para evaluar los métodos de AF en el mismo punto de referencia (El corpus TDT5) donde dominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas en las que se utilizaría un sistema AF. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no poder proporcionar un etiquetado adicional en una pequeña porción de documentos entrantes a través de la retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y los viejos temas reducidos y disminuidos. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) Es difícil aprender modelos precisos para la predicción basadas en datos de capacitación extremadamente escasos;2) No es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación;3) No se entiende bien cómo ajustar los parámetros de manera efectiva en los métodos de AF utilizando la validación de Cross-Corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o diferentes épocas. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para la clasificación por lotes, donde todos los datos de capacitación se dan a la vez. Los dos primeros problemas se han estudiado en la literatura de filtrado adaptativo, incluida la adaptación del perfil del tema utilizando roccio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbral utilizando calibración probabilística o técnicas de ajuste locales [1 1 [1 1] [2] [9] [10] [11] [12] [13]. Aunque estos trabajos proporcionan información valiosa para comprender los problemas y las posibles soluciones, es difícil sacar conclusiones con respecto a la efectividad y la solidez de los métodos actuales porque el tercer problema no se ha investigado a fondo. Abordar el tercer problema es el enfoque principal en este documento. Argumentamos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para la afinación de parámetros en múltiples corpus. La mayoría de los métodos AF tienen parámetros previamente especificados que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Los ejemplos de capacitación disponibles, por otro lado, a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al comienzo;La optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto solo deja una opción (suponiendo que el ajuste en el conjunto de pruebas no sea una alternativa), es decir, elegir un corpus externo como conjunto de validación. Observe que los temas de conjunto de validación a menudo no se superponen con los temas de conjunto de pruebas, por lo tanto, la optimización de los parámetros se realiza en la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿Qué métodos (si los hay) son robustos bajo la condición de usar la validación de Cross-Corpus para ajustar los parámetros? La literatura actual no ofrece una respuesta porque no se ha informado una investigación exhaustiva sobre la robustez de los métodos de AF. En este documento, abordamos la pregunta anterior realizando una evaluación transversal con dos enfoques efectivos en FA: rocio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en FA, con un buen rendimiento en las evaluaciones de referencia (TREC y TDT) si se usan los parámetros apropiados y si se combinan con una estrategia de calibración de umbral efectiva [2] [4] [7] [8] [9][11] [13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15] [14]. Recientemente se evaluó en el filtrado adaptativo y se descubrió que tenía un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de usar cada método solo en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de punto cruzado para pruebas de robustez. Específicamente, nos centramos en cuánto depende el rendimiento de estos métodos de la sintonización de los parámetros, cuáles son los parámetros más influyentes en estos métodos, cuán difícil (o lo fácil) optimizar estos parámetros influyentes utilizando la validación de Corporpus, qué tan fuertes funcionan estos métodos.En múltiples puntos de referencia con el ajuste sistemático de los parámetros en otros corpus, y cuán eficientes son estos métodos en ejecución de AF en grandes corpus de referencia. La organización del documento es la siguiente: la Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La Sección 3 analiza las diferencias entre las métricas TREC y TDT (servicios públicos y costos de seguimiento) y las posibles implicaciones de esas diferencias. La Sección 4 describe los enfoques Rocchio y LR para FA, respectivamente. La Sección 5 informa los experimentos y resultados. La Sección 6 concluye los principales hallazgos en este estudio.2. Benchmark Corporal Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consiste en aproximadamente 806,791 historias de noticias de Reuters desde agosto de 1997 con 84 etiquetas de temas (categorías de temas) [7]. Las primeras dos semanas (del 20 al 31 de agosto de 1996) de los documentos son el conjunto de capacitación, y el 11 y ½ meses restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) es el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, que consiste en el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas TREC11 (50) son bastante diferentes de los de TREC10;Son consultas para la recuperación con juicios de relevancia por parte de los evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en el TDT2001 Dry Run1. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y Pri the World) en el período de octubre a diciembre a diciembre de diciembre a diciembre de diciembre.1998. También se proporcionan versiones traducidas a máquina de las historias no inglesas (Xinhua, Zaobao y VOA mandarina). El punto de división para conjuntos de pruebas de entrenamiento es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas a máquina de las historias no inglesas. Solo usamos las versiones en inglés de esos documentos en nuestros experimentos para este documento. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de sujetos genéricas y siempre duraderas (como las de TREC), los temas de TDT se definen en un nivel más fino de granularidad, para eventos que ocurren en ciertos momentos y ubicaciones, y que nacen y mueren, típicamente asociados con una distribución estalladasobre noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeños que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema TREC (guerras civiles) y dos temas de TDT (reunión de la cumbre de disparos y APEC, respectivamente) durante un período de 3 meses, donde el área bajo cada curva se normaliza a una. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen que la evaluación transversal sea interesante. Por ejemplo, los algoritmos que favorecen los temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones transversales nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales para el filtrado adaptativo en el seguimiento de las tendencias de la deriva de los temas.1 http://www.ldc.upenn.edu/projects/tdt2001/topics.html Tabla 1: Estadísticas de los corpus de referencia para evaluaciones de filtrado adaptativo n (tr) es el número de los documentos de capacitación iniciales;N (TS) es el número de documentos de prueba;N+ es el número de ejemplos positivos de un tema predefinido;* es un promedio sobre todos los temas.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P (Tema | Semana) Gunnshot (TDT5) Reunión de la Cumbre APEC (TDT3) Guerra civil (TREC10) Figura 1:La naturaleza temporal de los temas 3. Métricas Para que nuestros resultados sean comparables a la literatura, decidimos utilizar las métricas de TREC convencionales y convencionales de TDT en nuestra evaluación.3.1 TREC11 Métricas Sea A, B, C y D, respectivamente, el número de verdaderos positivos, falsas alarmas, fallas y negativos verdaderos para un tema específico, y DCBAN +++ = ser el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: precisión)/(Baa +=, recuperación)/(caa +=) (2) 21 (CABA A F +++ + += β β β () η ηηβ ηβ-- +-= =1),/() (Max 11, SUT CABA donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, los puntajes de rendimiento se calculan primero para temas individuales y luego se promedian sobre temas (macroaverables).3.2 Métricas TDT La métrica convencional de TDT para el seguimiento de temas se define como: FamisStrk PTPWPTPWTC)) (1 () () (21-+= donde P (t) es el porcentaje de documentos sobre el tema T, MISS es la tasa de fallas deEl sistema sobre ese tema, FAP es la tasa de falsa alarma, y 1W y 2W son los costos (constantes preespecificadas) para una falla y una falsa alarma, respectivamente. Las evaluaciones de referencia TDT (desde 1997) han utilizado la configuración de 11 = W, 1.02 = W y 02.0) (= TP para todos los temas. Para evaluar el rendimiento de un sistema, CTRK se calcula para cada tema primero y luego los puntajes resultantes se promedian para una sola medida (el CTRK ponderado por el tema). Para que la intuición detrás de esta medida sea transparente, sustituimos los términos en la definición de CTRK de la siguiente manera: n Ca tp + =) (, n db tp + = -) (1, ca c pmiss + =, db b pfa + = =,) (1) (21 21 BWCW N DB B N DB W CA C N CA WTCTRK + ⋅ = + ⋅ + pastorRatio de penalización para fallas frente a falsas alarmas. Además de TRKC, TDT2004 también empleó 1.011 = βsut como métrica de utilidad. Para distinguir esto del 5.011 = βSUT en TREC11, llamamos al ex TDT5SU en el resto de este documento. Corpus #Topics N (TR) N (TS) AVG N+ (TR) AVG N+ (TS) MAX N+ (TS) MIN N+ (TS) #TOPICS POR DOC (TS) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias desde un punto de optimizaciónes una función de costo. Nuestro objetivo es maximizar el primero o minimizar el segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas se pueden analizar a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU se correlacionan positivamente con los valores de A y D, y se correlacionan negativamente con los valores de B y C;La única diferencia entre ellos es en sus proporciones de penalización para las falsificaciones versus falsas alarmas, es decir, 10: 1 en TDT5SU y 2: 1 en T11SU. La función CTRK, por otro lado, se correlaciona positivamente con los valores de C y B, y se correlaciona negativamente con los valores de A y D;Por lo tanto, se correlaciona negativamente con T11SU y TDT5SU. Más importante aún, existe una diferencia sutil y importante entre CTRK y las funciones de utilidad: T11SU y TDT5SU. Es decir, CTRK tiene una relación de penalización muy diferente para Misses frente a falsas alarmas: favorece los sistemas orientados al recuerdo a un extremo. A primera vista, uno pensaría que la relación de penalización en CTRK es 10: 1 desde 11 = W y 1.02 = W. Sin embargo, esto no es cierto si 02.0) (= TP es una estimación inexacta de los documentos en el tema en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje verdadero es: 002.0 37770 3.79) (≈ = + = n n tp donde n es el tamaño promedio de los conjuntos de pruebas en TDT3, y N + es el número promedio de ejemplos positivos por tema en la pruebaconjuntos. Usando 02.0) (ˆ = TP como una estimación (inexacta) de 0.002 amplía la relación de penalización prevista de 10: 1 a 100: 1, más o menos hablando. To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+× elegante =++× - ×+× • × − fue+× • = - ×+× • ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0) () (ˆ = === TP TP ρ es el factor de ampliación en la estimación de P (t) en comparación con la verdad. Comparando el resultado anterior con la Fórmula 2, podemos ver que la relación de penalización real para Misses versus falsas alarmas fue de 100: 1 en las evaluaciones en TDT3 usando CTRK. Del mismo modo, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0) () (ˆ === TP TP ρ, lo que significa la relación de penalización real para las falsificaciones en la evaluación en la evaluaciónen TDT5 usando CTRK fue de aproximadamente 583: 1. Las implicaciones del análisis anterior son bastante significativas: • CTRK definido en la misma fórmula no significa necesariamente la misma función objetivo en la evaluación;En cambio, el criterio de optimización depende del corpus de prueba.• Los sistemas optimizados para CTRK no optimizarían TDT5SU (y T11SU) porque el primero favorece la alta recuperación orientada a un extremo, mientras que el segundo no lo hace.• Los parámetros sintonizados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (por ejemplo, TDT5) a menos que tengamos en cuenta la dependencia sutil previamente desconocida de CTRK de los datos.• Los resultados en CTRK en los últimos años de evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la relación de penalización en CTRK varió. Aunque estos problemas con CTRK no se anticiparon originalmente, ofreció la oportunidad de examinar la capacidad de los sistemas en el comercio de precisión para un retiro extremo. Esta fue una parte desafiante de la evaluación TDT2004 para FA. Comparar las métricas en TDT y TREC desde un punto de vista de la utilidad o la optimización de costos es importante para comprender los resultados de la evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este problema se analiza explícitamente, hasta donde sabemos.4. Métodos 4.1 Rocchio incremental Para AF empleamos una versión común de clasificadores de estilo rocchio que calcula un vector prototipo por tema (t) de la siguiente manera: |) (|) (|) () () () (TD D TD D TQTPTddtdd - ∈+ ∈ ∑∑ -+ -+ = rr rr rr γβα El primer término en el RHS es la representación del vector ponderada de la descripción del tema cuyos elementos son los términos pesos. El segundo término es el centroide ponderado del conjunto) (TD+ de ejemplos de entrenamiento positivo, cada uno de los cuales es un vector de pesos de término con indocumento. El tercer término es el centroide ponderado del conjunto) (TD -de ejemplos de entrenamiento negativo que son los vecinos más cercanos del centroide positivo. Los tres términos reciben pesos preespecificados de βα y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión Sí en un nuevo documento para ese tema. Si la retroalimentación de relevancia está disponible (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de cualquiera de) (TD+ o) (TD−, y el prototipo se recomputa en consecuencia; si la retroalimentación de relevancia no está disponible (comoes el caso en el seguimiento de eventos TDT), la predicción de sistemas (sí) se trata como la verdad, y el nuevo documento se agrega a) (TD+ para actualizar el prototipo. Ambos casos son parte de nuestros experimentos en este documento (y parte de las evaluaciones de TDT 2004 para FA). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y el segundo caso PRF Rocchio, donde PRF representa la retroalimentación de pseudorelevancia. Las predicciones en un nuevo documento se realizan calculando la similitud coseno entre cada prototipo de tema y el vector de documento, y luego comparando las puntuaciones resultantes con un umbral: ⎩ ⎨ ⎧ - + = -) () ())), ((COS(No SÍ DTPSIGN NUEVA La calibración de umbral θ RR en Rocchio incremental es un tema de investigación desafiante. Se han desarrollado múltiples enfoques. Lo más simple es usar un umbral universal para todos los temas, sintonizado en un conjunto de validación y fijado durante la fase de prueba. Los métodos más elaborados incluyen la calibración de umbral probabilístico que convierte los puntajes de similitud no probabilísticos en probabilidades (es decir,) | (DTP R) para la optimización de servicios públicos [9] [13], y la regresión local basada en el margen para la reducción del riesgo [11]. Está más allá del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en términos de cuánto depende su rendimiento del ajuste elaborado del sistema y cuán difícil (o lo fácil) es obtener un buen rendimiento a través de la optimización de los parámetros cruzados. Por lo tanto, decidimos usar una versión relativamente simple de Rocchio como línea de base, es decir, con un umbral universal sintonizado en un corpus de validación y fijado para todos los temas en la fase de prueba. Esta versión simple de Rocchio se ha utilizado comúnmente en las evaluaciones de referencia TDT anteriores para el seguimiento de temas, y tuvo un rendimiento fuerte en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). Los resultados de variantes más complejas de Rocchio también se discuten cuando son correspondientes.4.2 Regresión logística para la regresión logística de AF (LR) estima la probabilidad posterior de un tema dado un documento que usa una función sigmoide) 1/(1), | 1 (XW EWXYP RRRR ⋅− +== donde x R es el vector de documento cuyos elementosson pesos de término, w r es el vector de coeficientes de regresión, y} 1,1 { -+∈Y es la variable de salida correspondiente a sí o no con respecto a un tema en particular. Dado un conjunto de entrenamiento de documentos etiquetados {}), (,) ,, (11 nn yxyxd r l r =, el problema de regresión estándar se define como para encontrar las estimaciones de máxima probabilidad de los coeficientes de regresión (los parámetros del modelo): {}{} {})) exp (1 (1LogMinarg) | (logmaxarg) | (maxarg ii xwyn i w wdp w wdp w mlw rr r r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que puede resolverAlgoritmo de gradiente conjugado en o (inf) tiempo para capacitación por tema, donde yo es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de capacitación y el número de características respectivamente [14]. Una vez que los coeficientes de regresión están optimizados en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: () ⎩ ⎨ ⎧ - + = -) () (), | (no sí wxypsign optnew θ rr señala que w r está constantementeActualizado cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de FA, mientras que el umbral óptimo OPTθ es constante, dependiendo solo de la función de utilidad predefinida (o costo) para la evaluación. Si T11SU es la métrica, por ejemplo, con la relación de penalización de 2: 1 para fallas y falsas alarmas (Sección 3.1), el umbral óptimo para LR es 33.0) 12/(1 =+ para todos los temas. Modificamos la versión estándar (arriba) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ ⎧ - ++ = ∑ = ⋅− 2 1) 1Log () (Minarg μλ Rrr rr r wyysw n i xwy i w mapii donde) (se considera que es α, β y γ para consulta, documentos positivos y negativos respectivamente, que son similares a los de Rocchio, dando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), ON-Documentos temáticos y documentos fuera del tema. El segundo término en la función objetivo es para la regularización, equivalente a agregar un gaussiano antes de los coeficientes de regresión con la matriz de varianza de covarianza media μ r y covarianza ι⋅λ2/1, donde ι es la matriz de identidad. La sintonización λ (≥0) está teóricamente justificada para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el ajuste en exceso en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un problema abierto para la investigación, dependiendo de la creencia de los usuarios sobre el espacio de los parámetros y el rango óptimo. La solución de la función objetivo modificada se denomina estimación máxima a posteriori (MAP), que se reduce a la solución de máxima probabilidad para LR estándar si 0 = λ.5. Evaluaciones Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial de TDT2004, los resultados de la optimización de parámetros cruzados y los resultados correspondientes a las cantidades de retroalimentación de relevancia.5.1 TDT2004 Resultados de referencia Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Participaron múltiples equipos de investigación y se permitieron múltiples carreras de cada equipo. CTRK y TDT5SU se usaron como métricas. La Figura 2 y la Figura 3 muestran los resultados;La mejor carrera de cada equipo fue seleccionada con respecto a CTRK o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero un umbral universal fijo para todos los temas) tuvo el mejor resultado en CTRK, y nuestra regresión logística tuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras carreras se sintonizaron en el corpus TDT3. Los resultados para otros sitios también se enumeran de forma anónima para la comparación. Ctrk our 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (cuanto más bajo el mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.2 0.25 0.3 0.35 0.4 Site2 Site2 Site3 Sitio4 Figura 2: TDT2004 Resultados en CTRK de CTRK de CTRK de CTRK de CTRK de CTRK de los sistemas de los sistemas.(El nuestro es el método Rocchio). También colocamos el primer y tercero cuartiles como palos para cada sitio.2 T11SU OUS 0.7328 SITIO3 0.7281 SITIO2 0.6672 SITO4 0.382 METRIC = TDT5SU (cuanto mayor es mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Site3 Site3 Site2 Figura 4 Figura 3: Figura 3:TDT2004 da como resultado TDT5SU de sistemas utilizando retroalimentación de relevancia verdadera.(El nuestro es LR con 0 = μ R y 005.0 = λ). Ctrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de trato de tema primario en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 OUUSSS Site2 Site5 Site4 Ctrk Figura 4: TDT2004 Resultados de CTRK.(El nuestro es PRF Rocchio). El filtrado adaptativo sin usar la retroalimentación de relevancia verdadera también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todos los procesos de capacitación y prueba, aunque los documentos de prueba no etiquetados podrían usarse tan pronto como se hicieron predicciones sobre ellos. Tal configuración ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra las presentaciones oficiales resumidas de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento.2 Usamos cuartiles en lugar de desviaciones estándar ya que la primera es más resistente a los valores atípicos.5.2 Optimización de parámetros de Cross-Corpus cuánto depende el rendimiento fuerte de nuestros sistemas es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados antes del proceso AF. Los parámetros compartidos incluyen la muestra de pesas α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativo (es decir,) (TD−), el esquema de peso de término y el número máximo de elementos distintos de cero en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ R, λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento TDT5, es imposible optimizar de manera efectiva estos parámetros en los datos de capacitación, y tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (C.F. Sección 2) Porque es más similar a TDT5 en términos de la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y arreglamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones a TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis posterior. Dado que las pruebas exhaustivas de todas las configuraciones de parámetros posibles son computacionalmente intratables, seguimos un procedimiento de encadenamiento hacia adelante paso a paso: prepecificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego sintonizamos un parámetro en el momento en quearreglando la configuración de los parámetros restantes. Repetimos este procedimiento para varios pases según lo permitido.0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 y TREC11 cuando el umbral de decisión varió. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima TDT3 es más cercana al TDT5-Optimal, mientras que el TREC10-Optimal y TREC1-Optimal están bastante lejos de la óptima TDT5. Si estuviéramos usando TREC10 o TREC11 en lugar de TDT3 como el corpus de validación para TDT5, o si el Corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de las puntuaciones ad-hoc (no probabilísticas) generadas por el método Rocchio: la distribución de las puntuaciones depende del corpus, lo que hace que la optimización del umbral de Corpor Corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto al ajuste del umbral porque produce puntajes probabilísticos de) | 1pr (xy = sobre el cual el umbral óptimo puede calcularse directamente si la estimación de probabilidad es precisa. Dada la relación de penalización para Misses frente a las falsas alarmas como 2: 1 en T11SU, 10: 1 en TDT5SU y 583: 1 en CTRK (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de entornos casi óptimos. Con estas configuraciones de umbral en nuestros experimentos para LR, nos centramos en la validación de CrossCorpus de los parámetros previos bayesianos, es decir, μ R y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 usando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 usando TDT5SU. A modo de comparación, también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados informados por NIST para TREC10 y TREC11. A partir de este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ = 0. Este hallazgo empírico es consistente con un informe anterior [13] para LR en TREC11, aunque nuestros resultados de LR (0.585 ~ 0.608 en T11SU) son más fuertes que los resultados (0.49 para LR estándar y 0.54 para LR usando Rocchio Prototype como el anterior)ese informe. Más importante aún, nuestra evaluación transversal da una fuerte evidencia de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de los puntajes generados por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de clasificadores de Rocchio no. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo Rocchio como media en el anterior;En cambio, el rendimiento disminuyó en algunos casos. Esta observación no es compatible con el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio son más precisos que los modelos LR para los temas en la etapa inicial del proceso AF, y creemos que el uso de un RocchioEl prototipo como la media en el Prior Gaussiano introduciría un sesgo indeseable a LR. También creemos que la reducción de la varianza (en la fase de prueba) debe controlarse por la elección de λ (pero no μ r), para los cuales realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes Priors Bayesian Corpus TDT3 TDT5 TREC10 TREC11 LR (μ = 0, λ = 0) 0.7562 0.7737 0.585 0.5715 LR (μ = 0, λ = 0.01) 0.8384 0.7812 0.6077 0.5747 LR (μ = ROC*,λ = 0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77 ~ 0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial TDT2004 (0.73) porque la optimización de los parámetros se ha mejorado después.4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, que no es directamente comparable a los puntajes en T11SU, solo indicativo.*: Se estableció μ r en el prototipo Rocchio 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Rendimiento Ctrk en TDT3 TDT5SU en TDT3 TDT5SU en TDT5 T11SU en TREC11 Figura 6: LR con lambda variando. El rendimiento de LR se resume con respecto a la sintonización λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y CTRK para las ejecuciones en TDT3,. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la LR regularizada es estable, igual o mejorado ligeramente sobre el rendimiento de la LR estándar. En el caso de minimizar CTRK, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 arrojó mejoras relativamente grandes sobre el rendimiento de la LR estándar porque el entrenamiento de un modelo para un retiro extremadamente alto es estadísticamente más complicado y, por lo tanto,Se necesita más regularización. En cualquier caso, la sintonización λ es relativamente segura y fácil de hacer con éxito mediante la sintonización cruzada. Otra opción influyente en la configuración de nuestro experimento es la ponderación de término: examinamos las elecciones de los esquemas binarios, TF y TF-IDF (la versión LTC). Encontramos el TF-IDF más efectivo tanto para Rocchio como para LR, y utilizamos este entorno en todos nuestros experimentos.5.3 Porcentajes de datos etiquetados cuánta retroalimentación de relevancia (RF) se necesitaría durante el proceso AF es una pregunta significativa en las aplicaciones del mundo real. Para responder, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación en absoluto • PRF Rocchio, actualización de perfiles de temas sin usar comentarios de relevancia verdadera;• Rocchio adaptativo, actualización de perfiles de temas utilizando comentarios de relevancia sobre documentos aceptados por el sistema más 10 documentos muestreados aleatoriamente desde el grupo de documentos SystemRected;• LR con 0 RR = μ, 01.0 = λ y umbral = 0.004;• Todos los parámetros en Rocchio sintonizados en TDT3. La Tabla 3 resume los resultados en CTRK: Rocchio adaptativo con retroalimentación de relevancia sobre el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% sobre el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. La LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que CTRK es una métrica de orientación de recuperación extremadamente alta, que causa una actualización frecuente de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón, establecemos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de cálculo inductable. El tiempo de cálculo en las horas de la máquina fue de 0.33 para la ejecución de Rocchio adaptativo y 14 para la ejecución de LR en TDT5 al optimizar CTRK. La Tabla 4 resume los resultados en TDT5SU;Adaptive LR fue el ganador en este caso, con retroalimentación de relevancia sobre el 0.05% de los documentos de prueba que mejoraron la utilidad en un 20.9% sobre los resultados de Rocchio de PRF. Tabla 3: Métodos AF en TDT5 (rendimiento en CTRK) ROC Base PRF ROC ADP ROC LR% de RF 0% 0% 0% 0.6% 0.2% CTRK 0.076 0.0707 0.0324 0.0382 ±% +7% (basal) -54% -46% Tabla4: Métodos AF en TDT5 (rendimiento en TDT5SU) Base ROC PRF ROC ADP ROC LR (λ = .01)% de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (basal) +6.9.9.9.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de usar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el objetivo principal de FA.5.4 Resumen del proceso de adaptación Después de que decidimos la configuración de los parámetros utilizando la validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/ROCCHIO utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados aleatoriamente;2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre relevancia y luego recibimos comentarios de relevancia para aquellos (predichos) documentos positivos.3) El modelo y las estadísticas de las FDI se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia.6. Observaciones finales presentamos una evaluación transversal de Rocchio incremental y LR incremental en el filtrado adaptativo, centrándonos en su robustez en términos de consistencia del rendimiento con respecto a la optimización de los parámetros cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de los parámetros en AF es un desafío abierto, pero no se ha estudiado a fondo en el pasado.• La robustez en el ajuste de los parámetros cruzados es importante para la evaluación y la comparación de métodos.• Encontramos LR más robusto que Rocchio;Tuvo los mejores resultados (en T11SU) jamás informados en TDT5, TREC10 y TREC11 sin un ajuste extenso.• Descubrimos que Rocchio funciona fuertemente cuando hay un buen corpus de validación disponible, y una opción preferida al optimizar CTRK es el objetivo, que favorece el retiro sobre precisión a un extremo. Para futuras investigaciones, queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y la deriva del contenido. Agradecimientos Este material se basa en el trabajo respaldado en piezas por la National Science Foundation (NSF) bajo la subvención IIS-0434035, por el DOD bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el No. NBCHD030010. Cualquier opinión, hallazgos y conclusiones o recomendaciones expresadas en este material son las del autor (s) y no reflejan necesariamente las opiniones de los patrocinadores.7. Referencias [1] J. Allan. Comentarios de relevancia incremental para el filtrado de información. En Sigir-96, 1996. [2] J. Callan. Aprendiendo mientras filtra documentos. En Sigir-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Detección de temas y descripción general de seguimiento. En detección y seguimiento de temas: Organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Descripción general de la evaluación y resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos de aprendizaje estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En Trec-10, 2001. [8] S. Robertson e I. Soboroff. El informe de pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En Trec-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Aumento y rocchio aplicado al filtrado de texto. En Sigir-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en el margen para el filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima probabilidad para los umbrales de filtrado. En Sigir-01, 2001. [13] Y. Zhang. Uso de antecedentes bayesianos para combinar clasificadores para el filtrado adaptativo. En Sigir-04, 2004. [14] J. Zhang e Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En Sigir-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. Inf. RECR.4 (1): 5-31 (2001).",
    "original_sentences": [
        "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
        "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
        "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
        "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
        "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
        "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
        "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
        "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
        "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
        "The above conditions attempt to mimic realistic situations where an AF system would be used.",
        "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
        "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
        "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
        "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
        "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
        "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
        "Addressing the third issue is the main focus in this paper.",
        "We argue that robustness is an important measure for evaluating and comparing AF methods.",
        "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
        "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
        "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
        "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
        "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
        "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
        "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
        "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
        "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
        "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
        "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
        "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
        "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
        "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
        "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
        "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
        "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
        "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
        "Section 5 reports the experiments and results.",
        "Section 6 concludes the main findings in this study. 2.",
        "BENCHMARK CORPORA We used four benchmark corpora in our study.",
        "Table 1 shows the statistics about these data sets.",
        "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
        "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
        "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
        "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
        "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
        "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
        "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
        "The splitting point for training-test sets is different for each topic in TDT.",
        "TDT5 was the evaluation benchmark in TDT2004 [4].",
        "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
        "We only used the English versions of those documents in our experiments for this paper.",
        "The TDT topics differ from TREC topics both conceptually and statistically.",
        "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
        "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
        "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
        "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
        "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
        "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
        "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
        "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
        "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
        "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
        "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
        "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
        "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
        "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
        "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
        "Our objective is to maximize the former or to minimize the latter on test documents.",
        "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
        "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
        "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
        "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
        "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
        "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
        "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
        "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
        "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
        "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
        "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
        "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
        "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
        "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
        "This was a challenging part of the TDT2004 evaluation for AF.",
        "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
        "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
        "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
        "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
        "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
        "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
        "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
        "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
        "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
        "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
        "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
        "Multiple approaches have been developed.",
        "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
        "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
        "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
        "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
        "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
        "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
        "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
        "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
        "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
        "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
        "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
        "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
        "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
        "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
        "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
        "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
        "Multiple research teams participated and multiple runs from each team were allowed.",
        "Ctrk and TDT5SU were used as the metrics.",
        "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
        "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
        "All the parameters of our runs were tuned on the TDT3 corpus.",
        "Results for other sites are also listed anonymously for comparison.",
        "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
        "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
        "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
        "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
        "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
        "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
        "Figure 4 shows the summarized official submissions from each team.",
        "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
        "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
        "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
        "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
        "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
        "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
        "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
        "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
        "We also tested our methods on TREC10 and TREC11 for further analysis.",
        "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
        "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
        "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
        "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
        "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
        "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
        "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
        "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
        "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
        "Table 2 summarizes the results 3 .",
        "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
        "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
        "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
        "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
        "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
        "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
        "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
        "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
        "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
        "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
        "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
        "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
        "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
        "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
        "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
        "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
        "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
        "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
        "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
        "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
        "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
        "Incremental LR, on the other hand, was weaker but still impressive.",
        "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
        "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
        "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
        "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
        "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
        "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
        "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
        "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
        "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
        "NBCHD030010.",
        "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
        "REFERENCES [1] J. Allan.",
        "Incremental relevance feedback for information filtering.",
        "In SIGIR-96, 1996. [2] J. Callan.",
        "Learning while filtering documents.",
        "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
        "Topic detection and tracking overview.",
        "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
        "Overview of the TDT 2004 Evaluation and Results.",
        "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
        "Elements of Statistical Learning.",
        "Springer, 2001. [6] S. Robertson and D. Hull.",
        "The TREC-9 filtering track final report.",
        "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
        "The TREC-10 filtering track final report.",
        "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
        "The TREC 2002 filtering track report.",
        "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
        "Microsoft Cambridge at TREC-9.",
        "In TREC-9, 2000. [10] R. Schapire, Y.",
        "Singer and A. Singhal.",
        "Boosting and Rocchio applied to text filtering.",
        "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
        "Margin-based local regression for adaptive filtering.",
        "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
        "Maximum likelihood estimation for filtering thresholds.",
        "In SIGIR-01, 2001. [13] Y. Zhang.",
        "Using Bayesian priors to combine classifiers for adaptive filtering.",
        "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
        "Robustness of regularized linear classification methods in text categorization.",
        "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
        "Text Categorization Based on Regularized Linear Classification Methods.",
        "Inf.",
        "Retr. 4(1): 5-31 (2001)."
    ],
    "error_count": 0,
    "keys": {
        "adaptive filtering": {
            "translated_key": "filtrado adaptativo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of <br>adaptive filtering</br> Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for <br>adaptive filtering</br>.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION <br>adaptive filtering</br> (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make <br>adaptive filtering</br> a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the <br>adaptive filtering</br> literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in <br>adaptive filtering</br> and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for <br>adaptive filtering</br> in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for <br>adaptive filtering</br> in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to <br>adaptive filtering</br> in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for <br>adaptive filtering</br> evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of <br>adaptive filtering</br> methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC <br>adaptive filtering</br>), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for <br>adaptive filtering</br> with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for <br>adaptive filtering</br> were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "<br>adaptive filtering</br> without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in <br>adaptive filtering</br>, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the <br>adaptive filtering</br> in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in <br>adaptive filtering</br>, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for <br>adaptive filtering</br>.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for <br>adaptive filtering</br>.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Robustez de los métodos de \"filtrado adaptativo\" en una evaluación transversal Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. Resumen Este documento informa un cambio transversal de un billete transversalEvaluación de la regresión logística regularizada (LR) y el rocho incremental para el \"filtrado adaptativo\".",
                "Introducción \"Filtrado adaptativo\" (AF) ha sido un tema de investigación desafiante en la recuperación de la información.",
                "Estas condiciones hacen que el \"filtrado adaptativo\" sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) Es difícil aprender modelos precisos para la predicción basadas en datos de capacitación extremadamente escasos;2) No es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación;3) No se entiende bien cómo ajustar los parámetros de manera efectiva en los métodos de AF utilizando la validación de Cross-Corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o diferentes épocas.",
                "Los dos primeros problemas se han estudiado en la literatura de \"filtrado adaptativo\", incluida la adaptación del perfil de temas utilizando roccio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc. y estrategias de optimización de umbral utilizando calibración probabilística o técnicas de ajuste locales locales[1] [2] [9] [10] [11] [12] [13].",
                "Recientemente se evaluó en \"filtrado adaptativo\" y se descubrió que tenía un rendimiento relativamente fuerte (Sección 5.1).",
                "TREC10 fue el punto de referencia de evaluación para el \"filtrado adaptativo\" en TREC 2001, que consiste en aproximadamente 806,791 historias de noticias de Reuters desde agosto de 1997 con 84 etiquetas de temas (categorías de temas) [7].",
                "TREC11 fue el punto de referencia de evaluación para el \"filtrado adaptativo\" en TREC 2002, que consiste en el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba.",
                "Las evaluaciones transversales nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales para el \"filtrado adaptativo\" en el seguimiento de las tendencias a la deriva de los temas.1 http://www.ldc.upenn.edu/projects/tdt2001/topics.html Tabla 1: Estadísticas de los corpus de referencia para las evaluaciones de \"filtrado adaptativo\" n (TR) es el número de documentos de capacitación iniciales;N (TS) es el número de documentos de prueba;N+ es el número de ejemplos positivos de un tema predefinido;* es un promedio sobre todos los temas.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P (Tema | Semana) Gunnshot (TDT5) Reunión de la Cumbre APEC (TDT3) Guerra civil (TREC10) Figura 1:La naturaleza temporal de los temas 3.",
                "Comparar las métricas en TDT y TREC desde un punto de vista de la utilidad o optimización de costos es importante para comprender los resultados de la evaluación de los métodos de \"filtrado adaptativo\".",
                "Si la retroalimentación de relevancia está disponible (como es el caso en el \"filtrado adaptativo\" de TREC), el nuevo documento se agrega al grupo de cualquiera de) (TD+ o) (TD−, y el prototipo se recomputa en consecuencia; si la retroalimentación de relevancia no está disponible(Como es el caso en el seguimiento de eventos TDT), la predicción de sistemas (sí) se trata como la verdad, y se agrega el nuevo documento) (TD+ para actualizar el prototipo.",
                "Esta versión simple de Rocchio se ha utilizado comúnmente en las evaluaciones de referencia TDT anteriores para el seguimiento de temas, y tuvo un rendimiento fuerte en las evaluaciones TDT2004 para el \"filtrado adaptativo\" con y sin comentarios de relevancia (Sección 5.1).",
                "Evaluaciones Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial de TDT2004, los resultados de la optimización de parámetros cruzados y los resultados correspondientes a las cantidades de retroalimentación de relevancia.5.1 TDT2004 Resultados de referencia Las evaluaciones TDT2004 para el \"filtrado adaptativo\" fueron realizadas por NIST en noviembre de 2004.",
                "El \"filtrado adaptativo\" sin usar la retroalimentación de relevancia verdadera también fue parte de las evaluaciones.",
                "Tabla 3: Métodos AF en TDT5 (rendimiento en CTRK) ROC Base PRF ROC ADP ROC LR% de RF 0% 0% 0% 0.6% 0.2% CTRK 0.076 0.0707 0.0324 0.0382 ±% +7% (basal) -54% -46% Tabla4: Métodos AF en TDT5 (rendimiento en TDT5SU) Base ROC PRF ROC ADP ROC LR (λ = .01)% de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (basal) +6.9.9.9.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el \"filtrado adaptativo\", en términos de usar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el objetivo principal de la FA.5.4 Resumen del proceso de adaptación Después de que decidimos la configuración de los parámetros utilizando la validación, realizamos el \"filtrado adaptativo\" en los siguientes pasos para cada tema: 1) capacitar al modelo LR/rocho utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos de muestras aleatoriamente;2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre relevancia y luego recibimos comentarios de relevancia para aquellos (predichos) documentos positivos.3) El modelo y las estadísticas de las FDI se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia.6.",
                "Observaciones finales presentamos una evaluación transversal de Rocchio incremental y LR incremental en el \"filtrado adaptativo\", centrándonos en su robustez en términos de consistencia del rendimiento con respecto a la optimización de los parámetros cruzados.",
                "Regresión local basada en margen para \"filtrado adaptativo\".",
                "Uso de Priors bayesianos para combinar clasificadores para \"filtrado adaptativo\"."
            ],
            "translated_text": "",
            "candidates": [
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "Filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "Filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "Filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "Filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "Filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo",
                "filtrado adaptativo"
            ],
            "error": []
        },
        "information retrieval": {
            "translated_key": "recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in <br>information retrieval</br>.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Introducción El filtrado adaptativo (AF) ha sido un tema de investigación desafiante en la \"recuperación de la información\"."
            ],
            "translated_text": "",
            "candidates": [
                "recuperación de información",
                "recuperación de la información"
            ],
            "error": []
        },
        "topic detection": {
            "translated_key": "detección de temas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the <br>topic detection</br> and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the <br>topic detection</br> and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "<br>topic detection</br> and tracking overview.",
                "In <br>topic detection</br> and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Utilizando cuatro corpus del foro \"Detección de temas\" y el foro de seguimiento (TDT) y las conferencias de recuperación de texto (TREC) evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad.",
                "A partir de 1997, en el área de \"Detección de temas\" y seguimiento (TDT) y 1998 en las conferencias de recuperación de texto (TREC), las evaluaciones de referencia han sido realizadas por NIST en las siguientes condiciones [6] [7] [8] [3] [[3] [4]: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivo para cada tema en el punto de partida.• La retroalimentación de relevancia estaba disponible, pero solo para los documentos del sistema (con una decisión Sí) en las evaluaciones de TREC para FA.• La retroalimentación de relevancia (RF) no se permitió en las evaluaciones de TDT para la AF (o el seguimiento de temas en la terminología TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas TREC y TDT se usaron conjuntamente para evaluar los métodos de AF en el mismo punto de referencia (El corpus TDT5) donde dominan los temas no estacionarios.",
                "\"Detección de temas\" y descripción general de seguimiento.",
                "En \"Detección de temas\" y seguimiento: Organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley."
            ],
            "translated_text": "",
            "candidates": [
                "detección de temas",
                "Detección de temas",
                "detección de temas",
                "Detección de temas",
                "detección de temas",
                "Detección de temas",
                "detección de temas",
                "Detección de temas"
            ],
            "error": []
        },
        "topic tracking": {
            "translated_key": "seguimiento de temas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or <br>topic tracking</br> in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for <br>topic tracking</br> is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for <br>topic tracking</br>, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the <br>topic tracking</br> task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for <br>topic tracking</br> without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "A partir de 1997, en el área de detección y seguimiento del tema (TDT) y 1998 en las conferencias de recuperación de texto (TREC), las evaluaciones de referencia han sido realizadas por NIST en las siguientes condiciones [6] [7] [8] [3] [4]: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivo para cada tema en el punto de partida.• La retroalimentación de relevancia estaba disponible, pero solo para los documentos del sistema (con una decisión Sí) en las evaluaciones de TREC para FA.• No se permitió retroalimentación de relevancia (RF) en las evaluaciones de TDT para FA (o \"seguimiento de temas\" en la terminología TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se usaron conjuntamente para evaluar los métodos AF en lo mismoBenchmark (el corpus TDT5) donde dominan los temas no estacionarios.",
                "Para evaluar el rendimiento de un sistema, los puntajes de rendimiento se calculan primero para temas individuales y luego se promedian sobre temas (macroaverables).3.2 Métricas TDT La métrica convencional de TDT para el \"seguimiento de temas\" se define como: FamisStrk PTPWPTPWTC)) (1 () () (21-+= donde P (t) es el porcentaje de documentos en el tema T, MISS es el MissTasa del sistema sobre ese tema, FAP es la tasa de falsa alarma, y 1W y 2W son los costos (constantes previamente especificadas) para una falla y una falsa alarma, respectivamente.",
                "Esta versión simple de Rocchio se ha utilizado comúnmente en las evaluaciones de referencia TDT anteriores para el \"seguimiento de temas\", y tuvo un fuerte rendimiento en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1).",
                "Tal configuración ha sido convencional para la tarea de \"seguimiento de temas\" en TDT hasta 2004.",
                "La Tabla 3 resume los resultados en CTRK: Rocchio adaptativo con retroalimentación de relevancia sobre el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% sobre el resultado del PRF Rocchio, el mejor sistema en la evaluación TDT2004 para el \"seguimiento de temas\" sin relevancia retroalimentación de relevanciainformación."
            ],
            "translated_text": "",
            "candidates": [
                "seguimiento de temas",
                "seguimiento de temas",
                "Seguimiento del tema",
                "seguimiento de temas",
                "seguimiento de temas",
                "seguimiento de temas",
                "Seguimiento de temas",
                "seguimiento de temas",
                "seguimiento de temas",
                "seguimiento de temas"
            ],
            "error": []
        },
        "relevance feedback": {
            "translated_key": "Comentarios de relevancia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "<br>relevance feedback</br> on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without <br>relevance feedback</br> information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, <br>relevance feedback</br>, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • <br>relevance feedback</br> was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • <br>relevance feedback</br> (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through <br>relevance feedback</br>.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., <br>relevance feedback</br> on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If <br>relevance feedback</br> is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if <br>relevance feedback</br> is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without <br>relevance feedback</br> (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of <br>relevance feedback</br>. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true <br>relevance feedback</br>. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true <br>relevance feedback</br>. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true <br>relevance feedback</br>. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true <br>relevance feedback</br> was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much <br>relevance feedback</br> (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true <br>relevance feedback</br>; • Adaptive Rocchio, updating topic profiles using <br>relevance feedback</br> on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with <br>relevance feedback</br> on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without <br>relevance feedback</br> information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with <br>relevance feedback</br> on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get <br>relevance feedback</br> for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true <br>relevance feedback</br>. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental <br>relevance feedback</br> for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La \"retroalimentación de relevancia\" en una porción pequeña (0.05 ~ 0.2%) de los documentos de prueba TDT5 arrojaron mejoras significativas de rendimiento, midiendo una reducción de hasta un 54% en CTRK y un aumento del 20.9% en T11SU (con β = 0.1), en comparación con laResultados del sistema de alto rendimiento en TDT2004 sin información de \"retroalimentación de relevancia\".",
                "Categorías y descriptores de sujetos H.3.3 [Búsqueda y recuperación de información]: filtrado de información, \"comentarios de relevancia\", modelos de recuperación, proceso de selección;I.5.2 [Metodología de diseño]: Diseño y evaluación del clasificador Algoritmos de términos generales, medición, rendimiento, experimentación 1.",
                "A partir de 1997, en el área de detección y seguimiento del tema (TDT) y 1998 en las conferencias de recuperación de texto (TREC), las evaluaciones de referencia han sido realizadas por NIST en las siguientes condiciones [6] [7] [8] [3] [4]: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivo para cada tema en el punto de partida.• \"La retroalimentación de relevancia\" estaba disponible, pero solo para los documentos del sistema (con una decisión Sí) en las evaluaciones de TREC para FA.• No se permitió \"retroalimentación de relevancia\" (RF) en las evaluaciones de TDT para la AF (o el seguimiento de temas en la terminología TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se usaron conjuntamente para evaluar los métodos AF en lo mismoBenchmark (el corpus TDT5) donde dominan los temas no estacionarios.",
                "Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no poder proporcionar un etiquetado adicional en una pequeña porción de documentos entrantes a través de \"comentarios de relevancia\".",
                "Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) Es difícil aprender modelos precisos para la predicción basadas en datos de capacitación extremadamente escasos;2) No es obvio cómo corregir el sesgo de muestreo (es decir, \"retroalimentación de relevancia\" solo en documentos aceptados por el sistema) durante el proceso de adaptación;3) No se entiende bien cómo ajustar los parámetros de manera efectiva en los métodos de AF utilizando la validación de Cross-Corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o diferentes épocas.",
                "Si se dispone de \"retroalimentación de relevancia\" (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de cualquiera de) (TD+ o) (TD−, y el prototipo se recomputa en consecuencia; si la \"retroalimentación de relevancia\" es esNo disponible (como es el caso en el seguimiento de eventos TDT), la predicción de sistemas (sí) se trata como la verdad, y se agrega el nuevo documento) (TD+ para actualizar el prototipo.",
                "Esta versión simple de Rocchio se ha utilizado comúnmente en las evaluaciones de referencia TDT anteriores para el seguimiento de temas, y tuvo un fuerte rendimiento en las evaluaciones TDT2004 para el filtrado adaptativo con y sin \"retroalimentación de relevancia\" (Sección 5.1).",
                "Evaluaciones Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial de TDT2004, los resultados de optimización de parámetros cruzados y los resultados correspondientes a las cantidades de \"retroalimentación de relevancia\".5.1 TDT2004 Resultados de referencia Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true \"relevance feedback\".(El nuestro es el método Rocchio.)",
                "También colocamos el primer y tercero cuartiles como palos para cada sitio.2 T11SU OUS 0.7328 SITIO3 0.7281 SITIO2 0.6672 SITO4 0.382 METRIC = TDT5SU (cuanto mayor es mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Site3 Site3 Site2 Figura 4 Figura 3: Figura 3:TDT2004 da como resultado TDT5SU de sistemas utilizando verdadero \"retroalimentación de relevancia\".(El nuestro es LR con 0 = μ R y 005.0 = λ).",
                "Ctrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Sitio6 0.8973 Resultados de trato de tema primario en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Ctrk Figura 4: TDT2004 Resultas en CTRK en los Sistemas de CttRK de CTRK.\".(El nuestro es PRF Rocchio.)",
                "El filtrado adaptativo sin usar la verdadera \"retroalimentación de relevancia\" también fue parte de las evaluaciones.",
                "Encontramos el TF-IDF más efectivo tanto para Rocchio como para LR, y utilizamos este entorno en todos nuestros experimentos.5.3 Porcentajes de datos etiquetados La cantidad de \"retroalimentación de relevancia\" (RF) se necesitaría durante el proceso AF es una pregunta significativa en las aplicaciones del mundo real.",
                "Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación en absoluto • PRF Rocchio, actualización de perfiles de temas sin usar \"comentarios de relevancia\";• Rocchio adaptativo, actualización de perfiles de temas utilizando \"retroalimentación de relevancia\" en documentos aceptados por el sistema más 10 documentos muestreados aleatoriamente desde el grupo de documentos SystemRected;• LR con 0 RR = μ, 01.0 = λ y umbral = 0.004;• Todos los parámetros en Rocchio sintonizados en TDT3.",
                "La Tabla 3 resume los resultados en CTRK: Rocchio adaptativo con \"retroalimentación de relevancia\" en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% sobre el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin \"relevanciaComentarios \"Información.",
                "La Tabla 4 resume los resultados en TDT5SU;Adaptive LR fue el ganador en este caso, con \"retroalimentación de relevancia\" en el 0.05% de los documentos de prueba que mejoran la utilidad en un 20.9% sobre los resultados de PRF Rocchio.",
                "Tabla 3: Métodos AF en TDT5 (rendimiento en CTRK) ROC Base PRF ROC ADP ROC LR% de RF 0% 0% 0% 0.6% 0.2% CTRK 0.076 0.0707 0.0324 0.0382 ±% +7% (basal) -54% -46% Tabla4: Métodos AF en TDT5 (rendimiento en TDT5SU) Base ROC PRF ROC ADP ROC LR (λ = .01)% de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (basal) +6.9.9.9.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de usar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el objetivo principal de FA.5.4 Resumen del proceso de adaptación Después de que decidimos la configuración de los parámetros utilizando la validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/ROCCHIO utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados aleatoriamente;2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre relevancia y luego obtenemos \"comentarios de relevancia\" para aquellos documentos positivos (predichos).3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadero \"retroalimentación de relevancia\".6.",
                "\"Comentarios de relevancia\" incrementales para el filtrado de información."
            ],
            "translated_text": "",
            "candidates": [
                "Comentarios de relevancia",
                "retroalimentación de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "comentarios de relevancia",
                "Comentarios de relevancia",
                "La retroalimentación de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "comentarios de relevancia",
                "Comentarios de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "retroalimentación de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "relevance feedback",
                "Comentarios de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "Comentarios de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "comentarios de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "retroalimentación de relevancia",
                "relevanciaComentarios ",
                "Comentarios de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "comentarios de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "Comentarios de relevancia"
            ],
            "error": []
        },
        "statistical learning": {
            "translated_key": "aprendizaje estadístico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in <br>statistical learning</br> (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of <br>statistical learning</br> for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in <br>statistical learning</br>, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in <br>statistical learning</br>, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of <br>statistical learning</br>.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el \"aprendizaje estadístico\" (clasificación en línea), por las siguientes razones: 1) Es difícil aprender modelos precisos para la predicción basadas en datos de capacitación extremadamente escasos;2) No es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación;3) No se entiende bien cómo ajustar los parámetros de manera efectiva en los métodos de AF utilizando la validación de Cross-Corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o diferentes épocas.",
                "Ninguno de estos problemas se aborda en la literatura de \"aprendizaje estadístico\" para la clasificación por lotes, donde todos los datos de capacitación se dan a la vez.",
                "La regresión logística es un método clásico en el \"aprendizaje estadístico\", y uno de los mejores en la categorización de texto en modo por lotes [15] [14].",
                "Tabla 3: Métodos AF en TDT5 (rendimiento en CTRK) ROC Base PRF ROC ADP ROC LR% de RF 0% 0% 0% 0.6% 0.2% CTRK 0.076 0.0707 0.0324 0.0382 ±% +7% (basal) -54% -46% Tabla4: Métodos AF en TDT5 (rendimiento en TDT5SU) Base ROC PRF ROC ADP ROC LR (λ = .01)% de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (basal) +6.9.9.9.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de usar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el \"aprendizaje estadístico\", que es el objetivo principal de FA.5.4 Resumen del proceso de adaptación Después de que decidimos la configuración de los parámetros utilizando la validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/ROCCHIO utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados aleatoriamente;2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre relevancia y luego recibimos comentarios de relevancia para aquellos (predichos) documentos positivos.3) El modelo y las estadísticas de las FDI se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia.6.",
                "Elementos de \"aprendizaje estadístico\"."
            ],
            "translated_text": "",
            "candidates": [
                "aprendizaje estadístico",
                "aprendizaje estadístico",
                "aprendizaje estadístico",
                "aprendizaje estadístico",
                "aprendizaje estadístico",
                "aprendizaje estadístico",
                "Aprendizaje estadístico",
                "aprendizaje estadístico",
                "aprendizaje estadístico",
                "aprendizaje estadístico"
            ],
            "error": []
        },
        "robustness": {
            "translated_key": "robustez",
            "is_in_text": true,
            "original_annotated_sentences": [
                "<br>robustness</br> of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and <br>robustness</br> of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that <br>robustness</br> is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the <br>robustness</br> of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for <br>robustness</br> testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the <br>robustness</br> of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the <br>robustness</br> of LR.",
                "The <br>robustness</br>, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their <br>robustness</br> in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • <br>robustness</br> in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "<br>robustness</br> of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "\"Robuste\" de los métodos de filtrado adaptativo en una evaluación de bencillo yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. Resumen Este artículo informa un cambio transversalEvaluación de la regresión logística regularizada (LR) y el rocho incremental para el filtrado adaptativo.",
                "Aunque estos trabajos proporcionan ideas valiosas para comprender los problemas y las posibles soluciones, es difícil sacar conclusiones con respecto a la efectividad y la \"robustez\" de los métodos actuales porque el tercer problema no se ha investigado a fondo.",
                "Argumentamos que la \"robustez\" es una medida importante para evaluar y comparar los métodos de AF.",
                "La literatura actual no ofrece una respuesta porque no se ha informado una investigación exhaustiva sobre la \"robustez\" de los métodos de AF.",
                "Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de punto cruzado para pruebas de \"robustez\".",
                "En cambio, nuestro enfoque aquí es investigar la \"robustez\" de los métodos de estilo Rocchio en términos de cuánto depende su rendimiento del ajuste elaborado del sistema y cuán difícil (o lo fácil) es obtener un buen rendimiento a través de la optimización de los parámetros de Corporpus.",
                "Más importante aún, nuestra evaluación transversal da una fuerte evidencia de la \"robustez\" de LR.",
                "La \"robustez\", creemos, proviene de la naturaleza probabilística de los puntajes generados por el sistema.",
                "Observaciones finales presentamos una evaluación transversal de Rocchio incremental y LR incremental en el filtrado adaptativo, centrándonos en su \"robustez\" en términos de consistencia del rendimiento con respecto a la optimización de los parámetros cruzados.",
                "Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de los parámetros en AF es un desafío abierto, pero no se ha estudiado a fondo en el pasado.• La \"robustez\" en el ajuste de los parámetros cruzados es importante para la evaluación y la comparación de métodos.• Encontramos LR más robusto que Rocchio;Tuvo los mejores resultados (en T11SU) jamás informados en TDT5, TREC10 y TREC11 sin un ajuste extenso.• Descubrimos que Rocchio funciona fuertemente cuando hay un buen corpus de validación disponible, y una opción preferida al optimizar CTRK es el objetivo, que favorece el retiro sobre precisión a un extremo.",
                "\"robustez\" de métodos de clasificación lineal regularizados en la categorización de texto."
            ],
            "translated_text": "",
            "candidates": [
                "robustez",
                "Robuste",
                "robustez",
                "robustez",
                "robustez",
                "robustez",
                "robustez",
                "robustez",
                "robustez",
                "robustez",
                "robustez",
                "robustez",
                "robustez",
                "robustez",
                "robustez",
                "robustez",
                "robustez",
                "robustez",
                "robustez",
                "robustez",
                "robustez",
                "robustez"
            ],
            "error": []
        },
        "systematic method for parameter tuning across multiple corpora": {
            "translated_key": "Método sistemático para el ajuste de los parámetros en múltiples corpus",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a <br>systematic method for parameter tuning across multiple corpora</br>.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un \"método sistemático para la afinación de parámetros en múltiples corpus\"."
            ],
            "translated_text": "",
            "candidates": [
                "Método sistemático para el ajuste de los parámetros en múltiples corpus",
                "método sistemático para la afinación de parámetros en múltiples corpus"
            ],
            "error": []
        },
        "external corpus": {
            "translated_key": "corpus externo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an <br>external corpus</br> as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an <br>external corpus</br> for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Esto solo deja una opción (suponiendo que el ajuste en el conjunto de pruebas no sea una alternativa), es decir, elegir un \"corpus externo\" como conjunto de validación.",
                "Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento TDT5, es imposible optimizar de manera efectiva estos parámetros en los datos de entrenamiento, y tuvimos que elegir un \"corpus externo\" para la validación."
            ],
            "translated_text": "",
            "candidates": [
                "cuerpo externo",
                "corpus externo",
                "cuerpo externo",
                "corpus externo"
            ],
            "error": []
        },
        "validation set": {
            "translated_key": "conjunto de validación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the <br>validation set</br>.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a <br>validation set</br> and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Esto solo deja una opción (suponiendo que el ajuste en el conjunto de pruebas no sea una alternativa), es decir, elegir un corpus externo como \"conjunto de validación\".",
                "Lo más simple es usar un umbral universal para todos los temas, sintonizado en un \"conjunto de validación\" y fijado durante la fase de prueba."
            ],
            "translated_text": "",
            "candidates": [
                "conjunto de validación",
                "conjunto de validación",
                "conjunto de validación",
                "conjunto de validación"
            ],
            "error": []
        },
        "logistic regression": {
            "translated_key": "Regresión logística",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized <br>logistic regression</br> (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, <br>logistic regression</br> in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized <br>logistic regression</br> (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "<br>logistic regression</br> is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 <br>logistic regression</br> for AF <br>logistic regression</br> (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our <br>logistic regression</br> had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "<br>logistic regression</br> has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, <br>logistic regression</br> is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Robustez de los métodos de filtrado adaptativo en una evaluación transversal Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. Resumen Este documento informa una evaluación transversal de la evaluación de billetes de de\"regresión logística\" regularizada (LR) y rocho incremental para el filtrado adaptativo.",
                "Los dos primeros problemas se han estudiado en la literatura de filtrado adaptativo, incluida la adaptación del perfil de temas utilizando roccio incremental, modelos de densidad gaussiana-exponencial, \"regresión logística\" en un marco bayesiano, etc., y estrategias de optimización de umbral utilizando calibración probabilística o técnicas de ajuste locales locales[1] [2] [9] [10] [11] [12] [13].",
                "En este artículo abordamos la pregunta anterior realizando una evaluación transversal con dos enfoques efectivos en FA: rocio incremental y \"regresión logística\" regularizada (LR).",
                "La \"regresión logística\" es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15] [14].",
                "",
                "Nuestro Rocchio (con perfiles adaptativos pero un umbral universal fijo para todos los temas) tuvo el mejor resultado en CTRK, y nuestra \"regresión logística\" tuvo el mejor resultado en TDT5SU.",
                "",
                "Además, se sabe que la \"regresión logística\" converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de clasificadores de Rocchio no."
            ],
            "translated_text": "",
            "candidates": [
                "Regresión logística",
                "regresión logística",
                "Regresión logística",
                "regresión logística",
                "Regresión logística",
                "regresión logística",
                "Regresión logística",
                "regresión logística",
                "",
                "Regresión logística",
                "Regresión logística",
                "Regresión logística",
                "regresión logística",
                "",
                "regresión logística",
                "Regresión logística",
                "regresión logística"
            ],
            "error": []
        },
        "granularity difference": {
            "translated_key": "diferencia de granularidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The <br>granularity difference</br>s among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La \"diferencia de granularidad\" entre los temas y las distribuciones no estacionarias correspondientes hacen que la evaluación transversal sea interesante."
            ],
            "translated_text": "",
            "candidates": [
                "diferencia de granularidad",
                "diferencia de granularidad"
            ],
            "error": []
        },
        "cross-benchmark evaluation": {
            "translated_key": "evaluación transversal",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a <br>cross-benchmark evaluation</br> Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a <br>cross-benchmark evaluation</br> of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a <br>cross-benchmark evaluation</br> with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the <br>cross-benchmark evaluation</br> interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our <br>cross-benchmark evaluation</br> gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a <br>cross-benchmark evaluation</br> of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Robustez de los métodos de filtrado adaptativo en una \"evaluación transversal\" Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. Resumen Este artículo informa una \"cruz--Evaluación de referencia \"de regresión logística regularizada (LR) y rocho incremental para el filtrado adaptativo.",
                "En este documento, abordamos la pregunta anterior realizando una \"evaluación transversal\" con dos enfoques efectivos en FA: roccio incremental y regresión logística regularizada (LR).",
                "Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen que la \"evaluación transversal\" sea interesante.",
                "Más importante aún, nuestra \"evaluación transversal\" da una fuerte evidencia de la robustez de LR.",
                "Observaciones finales presentamos una \"evaluación transversal\" de Rocchio incremental y LR incremental en el filtrado adaptativo, centrándonos en su robustez en términos de consistencia del rendimiento con respecto a la optimización de parámetros cruzados de Corpus."
            ],
            "translated_text": "",
            "candidates": [
                "Evaluación de bencillo",
                "evaluación transversal",
                "cruz--Evaluación de referencia ",
                "Evaluación de bencillo",
                "evaluación transversal",
                "Evaluación de bencillo",
                "evaluación transversal",
                "Evaluación de bencillo",
                "evaluación transversal",
                "Evaluación de bencillo",
                "evaluación transversal"
            ],
            "error": []
        },
        "utility function": {
            "translated_key": "función de utilidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC <br>utility function</br>) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Descubrimos que LR funciona con fuerza y robusta en la optimización de T11SU (una \"función de utilidad\" de TREC), mientras que Rocchio es mejor para optimizar CTRK (el costo de seguimiento de TDT), una función objetivo orientada de alta recuperación."
            ],
            "translated_text": "",
            "candidates": [
                "función de utilidad",
                "función de utilidad"
            ],
            "error": []
        },
        "cost function": {
            "translated_key": "función de costos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a <br>cost function</br>.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Corpus #Topics N (TR) N (TS) AVG N+ (TR) AVG N+ (TS) MAX N+ (TS) MIN N+ (TS) #TOPICS POR DOC (TS) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias desde un punto de optimizaciónes una \"función de costo\"."
            ],
            "translated_text": "",
            "candidates": [
                "función de costos",
                "función de costo"
            ],
            "error": []
        },
        "penalty ratio": {
            "translated_key": "Ratio de penalización",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the <br>penalty ratio</br> for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different <br>penalty ratio</br> for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the <br>penalty ratio</br> in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended <br>penalty ratio</br> of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual <br>penalty ratio</br> for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual <br>penalty ratio</br> for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the <br>penalty ratio</br> in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the <br>penalty ratio</br> of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the <br>penalty ratio</br> for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Para que la intuición detrás de esta medida sea transparente, sustituimos los términos en la definición de CTRK de la siguiente manera: n Ca tp + =) (, n db tp + = -) (1, ca c pmiss + =, db b pfa + = =,) (1) (21 21 BWCW N DB B N DB W CA C N CA WTCTRK + ⋅ = + ⋅ + pastor\"Ratio de penalización\" para Misses versus falsas alarmas.",
                "Es decir, CTRK tiene una \"relación de penalización\" muy diferente para Misses frente a falsas alarmas: favorece los sistemas orientados al recuerdo a un extremo.",
                "A primera vista, uno pensaría que la \"relación de penalización\" en CTRK es de 10: 1 desde 11 = W y 1.02 = W.",
                "",
                "Comparando el resultado anterior con la Fórmula 2, podemos ver que la \"relación de penalización\" real para Misses versus falsas alarmas fue de 100: 1 en las evaluaciones en TDT3 usando CTRL.",
                "Del mismo modo, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0) () (ˆ === TP TP ρ, lo que significa la \"relación de penalización\" real para las falsas alarmas enLa evaluación en TDT5 usando CTRK fue de aproximadamente 583: 1.",
                "Las implicaciones del análisis anterior son bastante significativas: • CTRK definido en la misma fórmula no significa necesariamente la misma función objetivo en la evaluación;En cambio, el criterio de optimización depende del corpus de prueba.• Los sistemas optimizados para CTRK no optimizarían TDT5SU (y T11SU) porque el primero favorece la alta recuperación orientada a un extremo, mientras que el segundo no lo hace.• Los parámetros sintonizados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (por ejemplo, TDT5) a menos que tengamos en cuenta la dependencia sutil previamente desconocida de CTRK de los datos.• Los resultados en CTRK en los últimos años de evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la \"relación de penalización\" en CTRK varió.",
                "",
                "Dada la \"relación de penalización\" para las falsificaciones versus falsas alarmas como 2: 1 en T11SU, 10: 1 en TDT5SU y 583: 1 en CTRK (Sección 3.3), los umbrales óptimos (t) correspondientes son 0.33, 0.091 y 0.0017 respectivamente."
            ],
            "translated_text": "",
            "candidates": [
                "Ratio de penalización",
                "Ratio de penalización",
                "relación de penalización",
                "relación de penalización",
                "relación de penalización",
                "relación de penalización",
                "",
                "relación de penalización",
                "relación de penalización",
                "relación de penalización",
                "Relación de penalización",
                "relación de penalización",
                "relación de penalización",
                "relación de penalización",
                "",
                "relación de penalización",
                "relación de penalización",
                "relación de penalización"
            ],
            "error": []
        },
        "optimization criterion": {
            "translated_key": "criterio de optimización",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the <br>optimization criterion</br> depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Las implicaciones del análisis anterior son bastante significativas: • CTRK definido en la misma fórmula no significa necesariamente la misma función objetivo en la evaluación;En cambio, el \"criterio de optimización\" depende del corpus de prueba.• Los sistemas optimizados para CTRK no optimizarían TDT5SU (y T11SU) porque el primero favorece la alta recuperación orientada a un extremo, mientras que el segundo no lo hace.• Los parámetros sintonizados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (por ejemplo, TDT5) a menos que tengamos en cuenta la dependencia sutil previamente desconocida de CTRK de los datos.• Los resultados en CTRK en los últimos años de evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la relación de penalización en CTRK varió."
            ],
            "translated_text": "",
            "candidates": [
                "criterio de optimización",
                "criterio de optimización"
            ],
            "error": []
        },
        "probabilistic threshold calibration": {
            "translated_key": "Calibración de umbral probabilístico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include <br>probabilistic threshold calibration</br> which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Los métodos más elaborados incluyen \"calibración de umbral probabilístico\" que convierte los puntajes de similitud no probabilísticos en probabilidades (es decir,) | (DTP R) para la optimización de servicios públicos [9] [13], y la regresión local basada en el margen para la reducción del riesgo [11]."
            ],
            "translated_text": "",
            "candidates": [
                "Calibración de umbral probabilístico",
                "calibración de umbral probabilístico"
            ],
            "error": []
        },
        "rocchio-style method": {
            "translated_key": "método de estilo rocho",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "cross-corpus parameter optimization": {
            "translated_key": "optimización de parámetros cruzados",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic <br>cross-corpus parameter optimization</br> with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through <br>cross-corpus parameter optimization</br>.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the <br>cross-corpus parameter optimization</br> results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 <br>cross-corpus parameter optimization</br> How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to <br>cross-corpus parameter optimization</br>.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Utilizando la \"optimización de parámetros de Cross-Corpus\" sistemáticos con ambos métodos, obtuvimos los mejores resultados jamás informados en TDT5, TREC10 y TREC11.",
                "En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en términos de cuánto depende su rendimiento de la elaborado ajuste del sistema y cuán difícil (o lo fácil) es obtener un buen rendimiento a través de la \"optimización de parámetros de Corporpus\".",
                "Evaluaciones Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial de TDT2004, los resultados de la \"optimización de parámetros cruzados\" y los resultados correspondientes a las cantidades de retroalimentación de relevancia.5.1 TDT2004 Resultados de referencia Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004.",
                "Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento.2 Usamos cuartiles en lugar de desviaciones estándar ya que la primera es más resistente a los valores atípicos.5.2 \"Optimización de parámetros cruzados\", cuánto depende el rendimiento fuerte de nuestros sistemas es una pregunta importante.",
                "Observaciones finales presentamos una evaluación transversal de Rocchio incremental y LR incremental en el filtrado adaptativo, centrándonos en su robustez en términos de consistencia del rendimiento con respecto a la \"optimización de parámetros cruzados\"."
            ],
            "translated_text": "",
            "candidates": [
                "optimización de parámetros cruzados",
                "optimización de parámetros de Cross-Corpus",
                "optimización de parámetros cruzados",
                "optimización de parámetros de Corporpus",
                "Optimización de parámetros cruzados",
                "optimización de parámetros cruzados",
                "optimización de parámetros cruzados",
                "Optimización de parámetros cruzados",
                "optimización de parámetros cruzados",
                "optimización de parámetros cruzados"
            ],
            "error": []
        },
        "posterior probability": {
            "translated_key": "probabilidad posterior",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the <br>posterior probability</br> of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                ""
            ],
            "translated_text": "",
            "candidates": [
                "",
                "probabilidad posterior"
            ],
            "error": []
        },
        "sigmoid function": {
            "translated_key": "Función sigmoidea",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a <br>sigmoid function</br> )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Los resultados de variantes más complejas de Rocchio también se discuten cuando son correspondientes.4.2 Regresión logística para la regresión logística de AF (LR) estima la probabilidad posterior de un tema dado un documento que usa una \"función sigmoidea\") 1/(1), | 1 (XW EWXYP RRRR ⋅ - +== donde x R es el vector del documentocuyos elementos son pesos de términos, w r es el vector de los coeficientes de regresión, y} 1,1 { -+∈Y es la variable de salida correspondiente a sí o no con respecto a un tema particular."
            ],
            "translated_text": "",
            "candidates": [
                "Función sigmoidea",
                "función sigmoidea"
            ],
            "error": []
        },
        "bias": {
            "translated_key": "sesgo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling <br>bias</br> (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable <br>bias</br> to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) Es difícil aprender modelos precisos para la predicción basadas en datos de capacitación extremadamente escasos;2) No es obvio cómo corregir el \"sesgo\" de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación;3) No se entiende bien cómo ajustar los parámetros de manera efectiva en los métodos de AF utilizando la validación de Cross-Corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o diferentes épocas.",
                "Esta observación no es compatible con el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio son más precisos que los modelos LR para los temas en la etapa inicial del proceso AF, y creemos que el uso de un RocchioEl prototipo como medio en el Prior Gaussiano introduciría un \"sesgo\" indeseable a LR."
            ],
            "translated_text": "",
            "candidates": [
                "inclinación",
                "sesgo",
                "inclinación",
                "sesgo"
            ],
            "error": []
        },
        "gaussian": {
            "translated_key": "gaussiano",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, <br>gaussian</br>-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a <br>gaussian</br> prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the <br>gaussian</br> prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Los dos primeros problemas se han estudiado en la literatura de filtrado adaptativo, incluida la adaptación del perfil de temas utilizando roccio incremental, modelos de densidad exponencial \"gaussiana\", regresión logística en un marco bayesiano, etc. y estrategias de optimización de umbral utilizando calibración probabilística o técnicas de ajuste locales locales[1] [2] [9] [10] [11] [12] [13].",
                "El segundo término en la función objetivo es para la regularización, equivalente a agregar un \"gaussiano\" antes de los coeficientes de regresión con la matriz de varianza de covarianza media μ r y covarianza ι⋅λ2/1, donde ι es la matriz de identidad.",
                "Esta observación no es compatible con el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio son más precisos que los modelos LR para los temas en la etapa inicial del proceso AF, y creemos que el uso de un RocchioEl prototipo como la media en el \"gaussiano\" antes introduciría un sesgo indeseable a LR."
            ],
            "translated_text": "",
            "candidates": [
                "gaussiano",
                "gaussiana",
                "gaussiano",
                "gaussiano",
                "gaussiano",
                "gaussiano"
            ],
            "error": []
        },
        "regularization": {
            "translated_key": "regularización",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for <br>regularization</br>, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more <br>regularization</br> is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El segundo término en la función objetivo es para la \"regularización\", equivalente a agregar un gaussiano antes de los coeficientes de regresión con la matriz de varianza de covarianza media μ r y covarianza ι⋅λ2/1, donde ι es la matriz de identidad.",
                "En el caso de minimizar CTRK, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 arrojó mejoras relativamente grandes sobre el rendimiento de la LR estándar porque el entrenamiento de un modelo para un retiro extremadamente alto es estadísticamente más complicado y, por lo tanto,Se necesita más \"regularización\"."
            ],
            "translated_text": "",
            "candidates": [
                "regularización",
                "regularización",
                "regularización",
                "regularización"
            ],
            "error": []
        },
        "lr": {
            "translated_key": "lr",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (<br>lr</br>) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that <br>lr</br> performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (<br>lr</br>).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and <br>lr</br> in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and <br>lr</br> in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and <br>lr</br> approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (<br>lr</br>) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for <br>lr</br> is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of <br>lr</br> to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard <br>lr</br> if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is <br>lr</br> with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and <br>lr</br> have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in <br>lr</br>.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or <br>lr</br>), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for <br>lr</br>, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that <br>lr</br> significantly outperformed Rocchio on all the corpora, even in the runs of standard <br>lr</br> without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for <br>lr</br> on TREC11 although our results of <br>lr</br> (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of <br>lr</br>.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in <br>lr</br> a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of <br>lr</br> did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than <br>lr</br> models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to <br>lr</br>.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of <br>lr</br> with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 <br>lr</br>(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of <br>lr</br> is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized <br>lr</br> is stable, the same as or improved slightly over the performance of standard <br>lr</br>.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard <br>lr</br> because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and <br>lr</br>, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and <br>lr</br> on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • <br>lr</br> with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental <br>lr</br>, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in <br>lr</br>.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in <br>lr</br> to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of <br>lr</br> on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive <br>lr</br> was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc <br>lr</br> % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc <br>lr</br>(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental <br>lr</br> in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found <br>lr</br> more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Robustez de los métodos de filtrado adaptativo en una evaluación transversal Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. Resumen Este documento informa una evaluación transversal de la evaluación de billetes de deregresión logística regularizada (\"LR\") y rocho incremental para el filtrado adaptativo.",
                "Descubrimos que \"LR\" funciona con fuerza y robusta en la optimización de T11SU (una función de utilidad TREC), mientras que Rocchio es mejor para optimizar CTRK (el costo de seguimiento de TDT), una función objetivo orientada de alta recuperación.",
                "En este documento, abordamos la pregunta anterior realizando una evaluación transversal con dos enfoques efectivos en FA: rocio incremental y regresión logística regularizada (\"LR\").",
                "Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y \"LR\" en un marco bayesiano superó los resultados de usar cada método solo en el corpus TREC11.",
                "Estimulados por esos hallazgos, decidimos incluir a Rocchio y \"LR\" en nuestra evaluación transversal para pruebas de robustez.",
                "La Sección 4 describe los enfoques de Rocchio y \"LR\" para AF, respectivamente.",
                "Los resultados de variantes más complejas de Rocchio también se discuten cuando son correspondientes.4.2 Regresión logística para la regresión logística de AF (\"LR\") estima la probabilidad posterior de un tema dado un documento usando una función sigmoide) 1/(1), | 1 (XW EWXYP RRRR ⋅ - +== donde x R es el vector del documentocuyos elementos son pesos de término, w r es el vector de los coeficientes de regresión, y} 1,1 { -+∈Y es la variable de salida correspondiente a sí o no con respecto a un tema en particular.",
                "",
                "Modificamos la versión estándar (anterior) de \"LR\" para permitir criterios de optimización más flexibles de la siguienteI W Map II donde) (se considera que IYS es α, β y γ para consulta, documentos positivos y negativos respectivamente, que son similares a los de Rocchio, dando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas),,Documentos en el tema y documentos fuera del tema.",
                "La solución de la función objetivo modificada se denomina estimación máxima de A Posteriori (MAP), que se reduce a la solución de máxima probabilidad para \"LR\" estándar si 0 = λ.5.",
                "También colocamos el primer y tercero cuartiles como palos para cada sitio.2 T11SU OUS 0.7328 SITIO3 0.7281 SITIO2 0.6672 SITO4 0.382 METRIC = TDT5SU (cuanto mayor es mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Site3 Site3 Site2 Figura 4 Figura 3: Figura 3:TDT2004 da como resultado TDT5SU de sistemas utilizando retroalimentación de relevancia verdadera.(El nuestro es \"LR\" con 0 = μ R y 005.0 = λ).",
                "Tanto Rocchio como \"LR\" tienen parámetros que deben ser especificados antes del proceso AF.",
                "Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ R, λ y MI (el número máximo de iteraciones en el entrenamiento) en \"LR\".",
                "Dado que las pruebas exhaustivas de todas las configuraciones de parámetros posibles son computacionalmente intratables, seguimos un procedimiento de encadenamiento hacia adelante paso a paso: prepecificamos previamente un orden de los parámetros en un método (rocchio o \"LR\"), y luego sintonizamos un parámetro en eltiempo al fijar la configuración de los parámetros restantes.",
                "Con estas configuraciones de umbral en nuestros experimentos para \"LR\", nos centramos en la validación de CrossCorpus de los parámetros previos bayesianos, es decir, μ R y λ.",
                "A partir de este conjunto de resultados, vemos que \"LR\" superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de \"LR\" estándar sin ningún ajuste, es decir, λ = 0.",
                "Este hallazgo empírico es consistente con un informe anterior [13] para \"LR\" en TREC11, aunque nuestros resultados de \"LR\" (0.585 ~ 0.608 en T11SU) son más fuertes que los resultados (0.49 para LR estándar y 0.54 para LR usando prototipo Rocchio como comoel anterior) en ese informe.",
                "Más importante aún, nuestra evaluación transversal da una fuerte evidencia de la solidez de \"LR\".",
                "Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización umbral en \"LR\" sea un problema mucho más fácil.",
                "Otra observación interesante en estos resultados es que el rendimiento de \"LR\" no mejoró al usar un prototipo Rocchio como media en el anterior;En cambio, el rendimiento disminuyó en algunos casos.",
                "Esta observación no es compatible con el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos roccho son más precisos que los modelos \"LR\" para los temas en la etapa inicial del proceso AF, y creemos que usarUn prototipo de Rocchio como media en el Gaussian Prior introduciría un sesgo indeseable a \"LR\".",
                "Tabla 2: Resultados de \"LR\" con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 \"LR\" (μ = 0, λ = 0) 0.7562 0.7737 0.585 0.5715 LR (μ = 0, λ = 0.01) 0.8384 0.7812 0.6077 0.5747 LR (μ (μ = 0, 0.01) 0.8384 0.7812 0.6077 0.5747 LR (μ (μ = 0, 0.01)= ROC*, λ = 0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77 ~ 0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial TDT2004 (0.73) porque el parámetro ha mejorado después.4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, que no es directamente comparable a los puntajes en T11SU, solo indicativo.*: Se estableció μ r en el prototipo Rocchio 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Rendimiento Ctrk en TDT3 TDT5SU en TDT3 TDT5SU en TDT5 T11SU en TREC11 Figura 6: LR con lambda variando.",
                "El rendimiento de \"LR\" se resume con respecto a la sintonización λ en los corpus de TREC10, TREC11 y TDT3.",
                "En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la \"LR\" regularizada es estable, igual o mejorada ligeramente sobre el rendimiento de la \"LR\" estándar.",
                "En el caso de minimizar CTRK, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 arrojó mejoras relativamente grandes sobre el rendimiento de la \"LR\" estándar porque el entrenamiento de un modelo para un retiro extremadamente alto es estadísticamente más complicado,y, por lo tanto, se necesita más regularización.",
                "Encontramos el TF-IDF más efectivo tanto para Rocchio como para \"LR\", y utilizamos este entorno en todos nuestros experimentos.5.3 Porcentajes de datos etiquetados cuánta retroalimentación de relevancia (RF) se necesitaría durante el proceso AF es una pregunta significativa en las aplicaciones del mundo real.",
                "Para responder, evaluamos Rocchio y \"LR\" en TDT con la siguiente configuración: • Rocchio básico, sin adaptación en absoluto • PRF Rocchio, actualización de perfiles de temas sin usar comentarios de relevancia verdadera;• Rocchio adaptativo, actualización de perfiles de temas utilizando comentarios de relevancia sobre documentos aceptados por el sistema más 10 documentos muestreados aleatoriamente desde el grupo de documentos SystemRected;• \"LR\" con 0 RR = μ, 01.0 = λ y umbral = 0.004;• Todos los parámetros en Rocchio sintonizados en TDT3.",
                "La \"LR\" incremental, por otro lado, era más débil pero aún impresionante.",
                "Recuerde que CTRK es una métrica de orientación de recuperación extremadamente alta, lo que causa una actualización frecuente de perfiles y, por lo tanto, un problema de eficiencia en \"LR\".",
                "Por esta razón, establecemos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en \"LR\" para evitar un costo de cálculo insolador.",
                "El tiempo de cálculo en las horas de la máquina fue de 0.33 para la ejecución de Rocchio adaptativo y 14 para la ejecución de \"LR\" en TDT5 al optimizar CTRK.",
                "La Tabla 4 resume los resultados en TDT5SU;Adaptive \"LR\" fue el ganador en este caso, con retroalimentación de relevancia sobre el 0.05% de los documentos de prueba que mejoran la utilidad en un 20.9% sobre los resultados de PRF Rocchio.",
                "Tabla 3: Métodos AF en TDT5 (rendimiento en CTRK) Base ROC PRF ROC ADP ROC \"LR\"% de RF 0% 0% 0.6% 0.2% CTRK 0.076 0.0707 0.0324 0.0382 ±% +7% (línea de base) -54% -46% Tabla 4: Métodos AF en TDT5 (rendimiento en TDT5SU) Base ROC PRF ROC ADP ROC \"LR\" (λ = .01)% de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% ((línea de base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de usar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el objetivo principal de FA.5.4 Resumen del proceso de adaptación Después de que decidimos la configuración de los parámetros utilizando la validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/ROCCHIO utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados aleatoriamente;2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre relevancia y luego recibimos comentarios de relevancia para aquellos (predichos) documentos positivos.3) El modelo y las estadísticas de las FDI se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia.6.",
                "Observaciones finales presentamos una evaluación transversal de Rocchio incremental y \"LR\" incremental en el filtrado adaptativo, centrándonos en su robustez en términos de consistencia del rendimiento con respecto a la optimización de parámetros cruzados de Corpus.",
                "Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de los parámetros en AF es un desafío abierto, pero no se ha estudiado a fondo en el pasado.• La robustez en el ajuste de los parámetros cruzados es importante para la evaluación y la comparación de métodos.• Encontramos \"LR\" más robusto que Rocchio;Tuvo los mejores resultados (en T11SU) jamás informados en TDT5, TREC10 y TREC11 sin un ajuste extenso.• Descubrimos que Rocchio funciona fuertemente cuando hay un buen corpus de validación disponible, y una opción preferida al optimizar CTRK es el objetivo, que favorece el retiro sobre precisión a un extremo."
            ],
            "translated_text": "",
            "candidates": [
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR",
                "LR"
            ],
            "error": []
        },
        "adaptive filter": {
            "translated_key": "filtro adaptativo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for <br>adaptive filter</br>ing.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make <br>adaptive filter</br>ing a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the <br>adaptive filter</br>ing literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in <br>adaptive filter</br>ing and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for <br>adaptive filter</br>ing in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for <br>adaptive filter</br>ing in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to <br>adaptive filter</br>ing in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for <br>adaptive filter</br>ing evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of <br>adaptive filter</br>ing methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC <br>adaptive filter</br>ing), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for <br>adaptive filter</br>ing with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for <br>adaptive filter</br>ing were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in <br>adaptive filter</br>ing, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the <br>adaptive filter</br>ing in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in <br>adaptive filter</br>ing, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for <br>adaptive filter</br>ing.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for <br>adaptive filter</br>ing.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Robustez de los métodos de filtrado adaptativo en una evaluación transversal Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. Resumen Este documento informa una evaluación transversal de la evaluación de billetes de deRegresión logística regularizada (LR) y rocho incremental para \"filtro adaptativo\" ing.",
                "Estas condiciones hacen que el \"filtro adaptativo\" sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) Es difícil aprender modelos precisos para la predicción basadas en datos de capacitación extremadamente escasos;2) No es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación;3) No se entiende bien cómo ajustar los parámetros de manera efectiva en los métodos de AF utilizando la validación de Cross-Corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o diferentes épocas.",
                "Los dos primeros problemas se han estudiado en la literatura de \"filtro adaptativo\", incluida la adaptación del perfil de temas utilizando roccio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbral utilizando calibración probabilística o ajuste localTécnicas [1] [2] [9] [10] [11] [12] [13].",
                "Recientemente se evaluó en el \"filtro adaptativo\" y se descubrió que tenía un rendimiento relativamente fuerte (Sección 5.1).",
                "TREC10 fue el punto de referencia de evaluación para el \"filtro adaptativo\" en TREC 2001, que consta de aproximadamente 806,791 historias de noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de temas) [7].",
                "TREC11 fue el punto de referencia de evaluación para el \"filtro adaptativo\" en TREC 2002, que consiste en el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba.",
                "Las evaluaciones de bengleas transversales nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de \"filtro adaptativo\" en el seguimiento de las tendencias a la deriva de los temas.1 http://www.ldc.upenn.edu/projects/tdt2001/topics.html Tabla 1: Estadísticas de los corpus de referencia para las evaluaciones de \"filtro adaptativo\" N (TR) es el número de documentos de capacitación iniciales;N (TS) es el número de documentos de prueba;N+ es el número de ejemplos positivos de un tema predefinido;* es un promedio sobre todos los temas.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P (Tema | Semana) Gunnshot (TDT5) Reunión de la Cumbre APEC (TDT3) Guerra civil (TREC10) Figura 1:La naturaleza temporal de los temas 3.",
                "Comparar las métricas en TDT y TREC desde un punto de vista de la utilidad o la optimización de costos es importante para comprender los resultados de la evaluación de los métodos de \"filtro adaptativo\".",
                "Si la retroalimentación de relevancia está disponible (como es el caso en el \"filtro adaptativo\" de trec), el nuevo documento se agrega al grupo de cualquiera de los dos) (TD+ o) (TD−, y el prototipo se recomputa en consecuencia; si la retroalimentación de relevancia no esDisponible (como es el caso en el seguimiento de eventos TDT), la predicción de sistemas (sí) se trata como la verdad, y se agrega el nuevo documento) (TD+ para actualizar el prototipo.",
                "Esta versión simple de Rocchio se ha utilizado comúnmente en las evaluaciones de referencia TDT anteriores para el seguimiento de temas, y tuvo un rendimiento fuerte en las evaluaciones TDT2004 para \"filtrar adaptativo\" con y sin comentarios de relevancia (Sección 5.1).",
                "Evaluaciones Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial de TDT2004, los resultados de la optimización de parámetros cruzados y los resultados correspondientes a las cantidades de retroalimentación de relevancia.5.1 TDT2004 Resultados de referencia Las evaluaciones TDT2004 para el \"filtro adaptativo\" ING realizaron en noviembre de 2004.",
                "Tabla 3: Métodos AF en TDT5 (rendimiento en CTRK) ROC Base PRF ROC ADP ROC LR% de RF 0% 0% 0% 0.6% 0.2% CTRK 0.076 0.0707 0.0324 0.0382 ±% +7% (basal) -54% -46% Tabla4: Métodos AF en TDT5 (rendimiento en TDT5SU) Base ROC PRF ROC ADP ROC LR (λ = .01)% de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (basal) +6.9.9.9.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el \"filtro adaptativo\", en términos de usar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el objetivo principal de la FA.5.4 Resumen del proceso de adaptación Después de que decidimos la configuración de los parámetros utilizando la validación, realizamos el \"filtro adaptativo\" en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocho utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos de muestras aleatorias al azar;2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre relevancia y luego recibimos comentarios de relevancia para aquellos (predichos) documentos positivos.3) El modelo y las estadísticas de las FDI se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia.6.",
                "Observaciones finales presentamos una evaluación transversal de Rocchio incremental y LR incremental en el \"filtro adaptativo\", centrándose en su robustez en términos de consistencia del rendimiento con respecto a la optimización de los parámetros cruzados.",
                "Regresión local basada en margen para \"filtro adaptativo\" ing.",
                "Uso de antecedentes bayesianos para combinar clasificadores para \"filtro adaptativo\" ing."
            ],
            "translated_text": "",
            "candidates": [
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "Filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "Filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtrar adaptativo",
                "Filtro adaptativo",
                "filtro adaptativo",
                "Filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo",
                "filtro adaptativo"
            ],
            "error": []
        },
        "topic track": {
            "translated_key": "seguimiento del tema",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or <br>topic track</br>ing in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR).",
                "Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the Rocchio and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for <br>topic track</br>ing is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental Rocchio is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for <br>topic track</br>ing, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both Rocchio and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for <br>topic track</br>ing without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and Rocchio applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "A partir de 1997, en el área de detección y seguimiento del tema (TDT) y 1998 en las conferencias de recuperación de texto (TREC), las evaluaciones de referencia han sido realizadas por NIST en las siguientes condiciones [6] [7] [8] [3] [4]: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivo para cada tema en el punto de partida.• La retroalimentación de relevancia estaba disponible, pero solo para los documentos del sistema (con una decisión Sí) en las evaluaciones de TREC para FA.• No se permitió retroalimentación de relevancia (RF) en las evaluaciones de TDT para FA (o \"seguimiento del tema\" en la terminología TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se usaron conjuntamente para evaluar los métodos AF en los métodos de AF en losEl mismo punto de referencia (el corpus TDT5) donde dominan los temas no estacionarios.",
                "",
                "Esta versión simple de Rocchio se ha utilizado comúnmente en las evaluaciones de referencia TDT pasadas para \"Track Topic Rastreing\", y tuvo un rendimiento fuerte en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1).",
                "La Tabla 3 resume los resultados en CTRK: Rocchio adaptativo con retroalimentación de relevancia sobre el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% sobre el resultado del PRF Rocchio, el mejor sistema en la evaluación TDT2004 para \"seguimiento del tema\" sin relevanciaInformación de retroalimentación."
            ],
            "translated_text": "",
            "candidates": [
                "pista del tema",
                "seguimiento del tema",
                "",
                "Track de temas",
                "pista del tema",
                "Track Topic Rastreing",
                "pista del tema",
                "seguimiento del tema"
            ],
            "error": []
        },
        "rocchio": {
            "translated_key": "rocchio",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA ABSTRACT This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental <br>rocchio</br> for adaptive filtering.",
                "Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.",
                "We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while <br>rocchio</br> is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function.",
                "Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11.",
                "Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with β=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 [Design Methodology]: Classifier design and evaluation General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval.",
                "The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.",
                "Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions[6][7][8][3][4]: • A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. • Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. • Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology) until 2004. • TDT2004 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate.",
                "The above conditions attempt to mimic realistic situations where an AF system would be used.",
                "That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.",
                "Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.",
                "These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.",
                "None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.",
                "The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental <br>rocchio</br>, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].",
                "Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.",
                "Addressing the third issue is the main focus in this paper.",
                "We argue that robustness is an important measure for evaluating and comparing AF methods.",
                "By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.",
                "Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.",
                "Available training examples, on the other hand, are often insufficient for tuning the parameters.",
                "In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.",
                "This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.",
                "Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.",
                "Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?",
                "Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported.",
                "In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental <br>rocchio</br> and regularized logistic regression (LR).",
                "<br>rocchio</br>-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].",
                "Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].",
                "It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1).",
                "Furthermore, a recent paper [13] reported that the joint use of <br>rocchio</br> and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus.",
                "Stimulated by those findings, we decided to include <br>rocchio</br> and LR in our crossbenchmark evaluation for robustness testing.",
                "Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora.",
                "The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study.",
                "Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences.",
                "Section 4 outlines the <br>rocchio</br> and LR approaches to AF, respectively.",
                "Section 5 reports the experiments and results.",
                "Section 6 concludes the main findings in this study. 2.",
                "BENCHMARK CORPORA We used four benchmark corpora in our study.",
                "Table 1 shows the statistics about these data sets.",
                "TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].",
                "The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & ½ months (from September 1st , 1996 to August 19th , 1997) is the test set.",
                "TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.",
                "The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].",
                "TDT3 was the evaluation benchmark in the TDT2001 dry run1 .",
                "The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.",
                "Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.",
                "The splitting point for training-test sets is different for each topic in TDT.",
                "TDT5 was the evaluation benchmark in TDT2004 [4].",
                "The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.",
                "We only used the English versions of those documents in our experiments for this paper.",
                "The TDT topics differ from TREC topics both conceptually and statistically.",
                "Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.",
                "The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.",
                "Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.",
                "The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.",
                "For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa.",
                "Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3.",
                "METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents.",
                "The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT where parameters β and η were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002).",
                "For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1()()( 21 −+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively.",
                "The TDT benchmark evaluations (since 1997) have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics.",
                "For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk).",
                "To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms.",
                "In addition to trkC , TDT2004 also employed 1.011 =βSUT as a utility metric.",
                "To distinguish this from the 5.011 =βSUT in TREC11, we call former TDT5SU in the rest of this paper.",
                "Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function.",
                "Our objective is to maximize the former or to minimize the latter on test documents.",
                "The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions.",
                "For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.",
                "The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU.",
                "More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU.",
                "That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme.",
                "At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w .",
                "However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus.",
                "Using TDT3 as an example, the true percentage is: 002.0 37770 3.79 )( ≈= + = N n TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets.",
                "Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking.",
                "To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ where 10 002.0 02.0 )( )(ˆ === TP TP ρ is the factor of enlargement in the estimation of P(T) compared to the truth.",
                "Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk.",
                "Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1.",
                "The implications of the above analysis are rather significant: • Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. • Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. • Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. • Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied.",
                "Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall.",
                "This was a challenging part of the TDT2004 evaluation for AF.",
                "Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods.",
                "This is the first time this issue is explicitly analyzed, to our knowledge. 4.",
                "METHODS 4.1 Incremental <br>rocchio</br> for AF We employed a common version of <br>rocchio</br>-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights.",
                "The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights.",
                "The third term is the weighted centroid of the set )(TD− of negative training examples which are the nearest neighbors of the positive centroid.",
                "The three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.",
                "The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.",
                "If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the systems prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype.",
                "Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF).",
                "To distinguish the two, we call the first case simply <br>rocchio</br> and the second case PRF <br>rocchio</br> where PRF stands for pseudorelevance feedback.",
                "The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr Threshold calibration in incremental <br>rocchio</br> is a challenging research topic.",
                "Multiple approaches have been developed.",
                "The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.",
                "More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].",
                "It is beyond the scope of this paper to compare all the different ways to adapt <br>rocchio</br>-style methods for AF.",
                "Instead, our focus here is to investigate the robustness of <br>rocchio</br>-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.",
                "Hence, we decided to use a relatively simple version of <br>rocchio</br> as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.",
                "This simple version of <br>rocchio</br> has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1).",
                "Results of more complex variants of <br>rocchio</br> are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.",
                "Given a training set of labeled documents { }),(,),,( 11 nn yxyxD r L r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14].",
                "Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxyPsign optnew θ rr Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold optθ is constant, depending only on the predefined utility (or cost) function for evaluation.",
                "If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics.",
                "We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in <br>rocchio</br>, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.",
                "The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean μ r and covariance variance matrix Ι⋅λ2/1 , where Ι is the identity matrix.",
                "Tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].",
                "How to find an effective μ r is an open issue for research, depending on the users belief about the parameter space and the optimal range.",
                "The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=λ . 5.",
                "EVALUATIONS We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.",
                "Multiple research teams participated and multiple runs from each team were allowed.",
                "Ctrk and TDT5SU were used as the metrics.",
                "Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.",
                "Our <br>rocchio</br> (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.",
                "All the parameters of our runs were tuned on the TDT3 corpus.",
                "Results for other sites are also listed anonymously for comparison.",
                "Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. (Ours is the <br>rocchio</br> method.)",
                "We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=μ r and 005.0=λ ).",
                "CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. (Ours is PRF <br>rocchio</br>.)",
                "Adaptive filtering without using true relevance feedback was also a part of the evaluations.",
                "In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.",
                "Such a setting has been conventional for the Topic Tracking task in TDT until 2004.",
                "Figure 4 shows the summarized official submissions from each team.",
                "Our PRF <br>rocchio</br> (with a fixed threshold for all the topics) had the best performance. 2 We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.",
                "Both <br>rocchio</br> and LR have parameters that must be prespecified before the AF process.",
                "The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.",
                "The method-specific parameters include the decision threshold in <br>rocchio</br>, and μ r , λ and MI (the maximum number of iterations in training) in LR.",
                "Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.",
                "Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.",
                "Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).",
                "We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.",
                "We also tested our methods on TREC10 and TREC11 for further analysis.",
                "Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (<br>rocchio</br> or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.",
                "We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive <br>rocchio</br> Figure 5 compares the performance curves in TDT5SU for <br>rocchio</br> on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.",
                "These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.",
                "If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for <br>rocchio</br> in TDT2004.",
                "The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the <br>rocchio</br> method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.",
                "Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.",
                "Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.",
                "Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.",
                "With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.",
                "Table 2 summarizes the results 3 .",
                "We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.",
                "For comparison we also include the best results of <br>rocchio</br>-based methods on these corpora, which are our own results of <br>rocchio</br> on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.",
                "From this set of results, we see that LR significantly outperformed <br>rocchio</br> on all the corpora, even in the runs of standard LR without any tuning, i.e. λ=0.",
                "This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using <br>rocchio</br> prototype as the prior) in that report.",
                "More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.",
                "The robustness, we believe, comes from the probabilistic nature of the system-generated scores.",
                "That is, compared to the ad-hoc scores in <br>rocchio</br>, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.",
                "Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while <br>rocchio</br> classifiers parameters do not.",
                "Another interesting observation in these results is that the performance of LR did not improve when using a <br>rocchio</br> prototype as the mean in the prior; instead, the performance decreased in some cases.",
                "This observation does not support the previous report by [13], but we are not surprised because we are not convinced that <br>rocchio</br> prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a <br>rocchio</br> prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.",
                "We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.",
                "Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best <br>rocchio</br> 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards. 4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: μ r was set to the <br>rocchio</br> prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.",
                "The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.",
                "The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.",
                "In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.",
                "In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.",
                "In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.",
                "Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.",
                "We found TF-IDF most effective for both <br>rocchio</br> and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.",
                "To answer it, we evaluated <br>rocchio</br> and LR on TDT with the following settings: • Basic <br>rocchio</br>, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.",
                "Table 3 summarizes the results in Ctrk: Adaptive <br>rocchio</br> with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF <br>rocchio</br>, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.",
                "Incremental LR, on the other hand, was weaker but still impressive.",
                "Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.",
                "For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.",
                "The computation time in machine-hours was 0.33 for the run of adaptive <br>rocchio</br> and 14 for the run of LR on TDT5 when optimizing Ctrk.",
                "Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF <br>rocchio</br>.",
                "Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both <br>rocchio</br> and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/<br>rocchio</br> model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6.",
                "CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental <br>rocchio</br> and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.",
                "Our main conclusions from this study are the following: • Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. • Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. • We found LR more robust than <br>rocchio</br>; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. • We found <br>rocchio</br> performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.",
                "For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.",
                "Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. 7.",
                "REFERENCES [1] J. Allan.",
                "Incremental relevance feedback for information filtering.",
                "In SIGIR-96, 1996. [2] J. Callan.",
                "Learning while filtering documents.",
                "In SIGIR-98, 224-231, 1998. [3] J. Fiscus and G. Duddington.",
                "Topic detection and tracking overview.",
                "In Topic detection and tracking: event-based information organization, 17-31, 2002. [4] J. Fiscus and B. Wheatley.",
                "Overview of the TDT 2004 Evaluation and Results.",
                "In TDT-04, 2004. [5] T. Hastie, R. Tibshirani and J. Friedman.",
                "Elements of Statistical Learning.",
                "Springer, 2001. [6] S. Robertson and D. Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC-9, 2000. [7] S. Robertson and I. Soboroff.",
                "The TREC-10 filtering track final report.",
                "In TREC-10, 2001. [8] S. Robertson and I. Soboroff.",
                "The TREC 2002 filtering track report.",
                "In TREC-11, 2002. [9] S. Robertson and S. Walker.",
                "Microsoft Cambridge at TREC-9.",
                "In TREC-9, 2000. [10] R. Schapire, Y.",
                "Singer and A. Singhal.",
                "Boosting and <br>rocchio</br> applied to text filtering.",
                "In SIGIR-98, 215-223, 1998. [11] Y. Yang and B. Kisiel.",
                "Margin-based local regression for adaptive filtering.",
                "In CIKM-03, 2003. [12] Y. Zhang and J. Callan.",
                "Maximum likelihood estimation for filtering thresholds.",
                "In SIGIR-01, 2001. [13] Y. Zhang.",
                "Using Bayesian priors to combine classifiers for adaptive filtering.",
                "In SIGIR-04, 2004. [14] J. Zhang and Y. Yang.",
                "Robustness of regularized linear classification methods in text categorization.",
                "In SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles.",
                "Text Categorization Based on Regularized Linear Classification Methods.",
                "Inf.",
                "Retr. 4(1): 5-31 (2001)."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Robustez de los métodos de filtrado adaptativo en una evaluación transversal Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. Resumen Este documento informa una evaluación transversal de la evaluación de billetes de deRegresión logística regularizada (LR) y \"rocchio\" incremental para el filtrado adaptativo.",
                "Descubrimos que LR funciona con fuerza y robusta en la optimización de T11SU (una función de utilidad TREC), mientras que \"Rocchio\" es mejor para optimizar CTRK (el costo de seguimiento de TDT), una función objetivo orientada de alta recuperación.",
                "Los dos primeros problemas se han estudiado en la literatura de filtrado adaptativo, incluida la adaptación del perfil del tema utilizando \"rocchio\" incremental, modelos de densidad gaussianas-exponenciales, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbral utilizando calibración probabilística o técnicas de ajuste locales[1] [2] [9] [10] [11] [12] [13].",
                "En este documento, abordamos la pregunta anterior realizando una evaluación transversal con dos enfoques efectivos en AF: \"rocho\" incremental y regresión logística regularizada (LR).",
                "Los clasificadores de estilo \"rocchio\" han sido populares en AF, con un buen rendimiento en las evaluaciones de referencia (TREC y TDT) si se usan los parámetros apropiados y si se combinan con una estrategia de calibración de umbral efectiva [2] [4] [7] [8] [8] [9] [11] [13].",
                "Además, un artículo reciente [13] informó que el uso conjunto de \"Rocchio\" y LR en un marco bayesiano superó los resultados de usar cada método solo en el corpus TREC11.",
                "Estimulados por esos hallazgos, decidimos incluir \"Rocchio\" y LR en nuestra evaluación de punto cruzado para pruebas de robustez.",
                "La Sección 4 describe los enfoques \"Rocchio\" y LR para la AF, respectivamente.",
                "Métodos 4.1 \"Rocchio\" incremental para AF empleamos una versión común de clasificadores de estilo \"rocchio\" que calcula un vector prototipo por tema (t) de la siguiente manera: |) (|) (|) () () () (TDd td d tqtp tddtdd - ∈+ ∈ ∑∑–+ -+ = rr rr rr γβα El primer término en el RHS es la representación del vector ponderado de la descripción del tema cuyos elementos son los términos pesos.",
                "Para distinguir los dos, llamamos el primer caso simplemente \"Rocchio\" y el segundo caso PRF \"Rocchio\" donde PRF representa la retroalimentación de pseudorelevancia.",
                "Las predicciones en un nuevo documento se realizan calculando la similitud coseno entre cada prototipo de tema y el vector de documento, y luego comparando las puntuaciones resultantes con un umbral: ⎩ ⎨ ⎧ - + = -) () ())), ((COS(No Sí Dtpsign New θ RR El umbral de calibración en \"rocchio\" incremental es un tema de investigación desafiante.",
                "Está más allá del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo \"rocho\" para AF.",
                "En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo \"rocchio\" en términos de cuánto depende su rendimiento de ajuste elaborado del sistema y cuán difícil (o lo fácil) es obtener un buen rendimiento a través de la optimización de parámetros de Corpor.",
                "Por lo tanto, decidimos usar una versión relativamente simple de \"Rocchio\" como línea de base, es decir, con un umbral universal sintonizado en un corpus de validación y fijado para todos los temas en la fase de prueba.",
                "Esta versión simple de \"Rocchio\" se ha utilizado comúnmente en las evaluaciones de referencia TDT anteriores para el seguimiento de temas, y tuvo un fuerte rendimiento en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1).",
                "Los resultados de variantes más complejas de \"Rocchio\" también se discuten cuando son correspondientes.4.2 Regresión logística para la regresión logística de AF (LR) estima la probabilidad posterior de un tema dado un documento que usa una función sigmoide) 1/(1), | 1 (XW EWXYP RRRR ⋅− +== donde x R es el vector de documento cuyos elementosson pesos de término, w r es el vector de los coeficientes de regresión, y} 1,1 { -+∈Y es la variable de salida correspondiente a sí o no con respecto a un tema particular.",
                "Modificamos la versión estándar (arriba) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ ⎧ - ++ = ∑ = ⋅− 2 1) 1Log () (Minarg μλ Rrr rr r wyysw n i xwy i w mapii donde) (se considera que es α, β y γ para consulta, documentos positivos y negativos, respectivamente, que son similares a los de \"rocho\", dando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas),,Documentos en el tema y documentos fuera del tema.",
                "Nuestro \"Rocchio\" (con perfiles adaptativos pero un umbral universal fijo para todos los temas) tuvo el mejor resultado en CTRK, y nuestra regresión logística tuvo el mejor resultado en TDT5SU.",
                "Ctrk our 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (cuanto más bajo el mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.2 0.25 0.3 0.35 0.4 Site2 Site2 Site3 Sitio4 Figura 2: TDT2004 Resultados en CTRK de CTRK de CTRK de CTRK de CTRK de CTRK de los sistemas de los sistemas.(El nuestro es el método \"Rocchio\".)",
                "Ctrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de trato de tema primario en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 OUUSSS Site2 Site5 Site4 Ctrk Figura 4: TDT2004 Resultados de CTRK.(El nuestro es PRF \"Rocchio\".)",
                "Nuestro PRF \"Rocchio\" (con un umbral fijo para todos los temas) tuvo el mejor rendimiento.2 Usamos cuartiles en lugar de desviaciones estándar ya que la primera es más resistente a los valores atípicos.5.2 Optimización de parámetros de Cross-Corpus cuánto depende el rendimiento fuerte de nuestros sistemas es una pregunta importante.",
                "Tanto \"Rocchio\" como LR tienen parámetros que deben ser especificados antes del proceso AF.",
                "Los parámetros específicos del método incluyen el umbral de decisión en \"Rocchio\", y μ R, λ y MI (el número máximo de iteraciones en el entrenamiento) en LR.",
                "Dado que las pruebas exhaustivas de todas las configuraciones de parámetros posibles son computacionalmente intratables, seguimos un procedimiento de encadenamiento hacia adelante paso a paso: prepecificamos previamente un orden de los parámetros en un método (\"rocchio\" o LR), y luego sintonizamos un parámetro en eltiempo al fijar la configuración de los parámetros restantes.",
                "Repetimos este procedimiento para varios pases según lo permitido.0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive \"rocchio\" Figure 5 compares the performance curves in TDT5SU for \"Rocchio \"en TDT3, TDT5, TREC10 y TREC11 cuando el umbral de decisión varió.",
                "Si estuviéramos usando TREC10 o TREC11 en lugar de TDT3 como el corpus de validación para TDT5, o si el Corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para \"Rocchio\" en TDT2004.",
                "La dificultad proviene de las puntuaciones ad-hoc (no probabilísticas) generadas por el método \"rocchio\": la distribución de las puntuaciones depende del corpus, lo que hace que la optimización del umbral de Cross-Corpus sea un problema complicado.",
                "A modo de comparación, también incluimos los mejores resultados de los métodos basados en \"Rocchio\" en estos corpus, que son nuestros propios resultados de \"Rocchio\" en TDT3 y TDT5, y los mejores resultados informados por NIST para TREC10 y TREC11.",
                "A partir de este conjunto de resultados, vemos que LR superó significativamente a \"Rocchio\" en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ = 0.",
                "Este hallazgo empírico es consistente con un informe anterior [13] para LR en TREC11, aunque nuestros resultados de LR (0.585 ~ 0.608 en T11SU) son más fuertes que los resultados (0.49 para LR estándar y 0.54 para LR usando prototipo \"rocchio\" como el anterior) en ese informe.",
                "Es decir, en comparación con las puntuaciones ad-hoc en \"rocho\", las probabilidades posteriores normalizadas hacen que la optimización umbral en LR sea un problema mucho más fácil.",
                "Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de clasificadores \"rocchio\" no.",
                "Otra observación interesante en estos resultados es que el rendimiento de la LR no mejoró al usar un prototipo \"rocho\" como la media en el anterior;En cambio, el rendimiento disminuyó en algunos casos.",
                "Esta observación no es compatible con el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos \"rocho\" son más precisos que los modelos LR para los temas en la etapa inicial del proceso AF, y creemos que usarUn prototipo de \"rocho\" como media en el prior gaussiano introduciría un sesgo indeseable a LR.",
                "Tabla 2: Resultados de LR con diferentes Priors Bayesian Corpus TDT3 TDT5 TREC10 TREC11 LR (μ = 0, λ = 0) 0.7562 0.7737 0.585 0.5715 LR (μ = 0, λ = 0.01) 0.8384 0.7812 0.6077 0.5747 LR (μ = ROC*,λ = 0.01) 0.8138 0.7811 0.5803 0.5698 Mejor \"Rocchio\" 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77 ~ 0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial TDT2004 (0.73) porque la optimización de la parámetro ha mejorado después.4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, que no es directamente comparable a los puntajes en T11SU, solo indicativo.*: se estableció μ r en el prototipo \"Rocchio\" 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Rendimiento Ctrk en TDT3 TDT5SU en TDT3 TDT5SU en TDT5 T11SU en TREC11 Figura 6: LR con Varying Lambda.",
                "Encontramos el TF-IDF más efectivo tanto para \"Rocchio\" como para LR, y utilizamos este entorno en todos nuestros experimentos.5.3 Porcentajes de datos etiquetados cuánta retroalimentación de relevancia (RF) se necesitaría durante el proceso AF es una pregunta significativa en las aplicaciones del mundo real.",
                "Para responder, evaluamos \"Rocchio\" y LR en TDT con la siguiente configuración: • \"Rocchio\" básico, sin adaptación en absoluto • PRF Rocchio, actualización de perfiles de temas sin usar comentarios de relevancia verdadera;• Rocchio adaptativo, actualización de perfiles de temas utilizando comentarios de relevancia sobre documentos aceptados por el sistema más 10 documentos muestreados aleatoriamente desde el grupo de documentos SystemRected;• LR con 0 RR = μ, 01.0 = λ y umbral = 0.004;• Todos los parámetros en Rocchio sintonizados en TDT3.",
                "La Tabla 3 resume los resultados en CTRK: \"Rocchio\" adaptativo con retroalimentación de relevancia sobre el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% sobre el resultado del PRF \"Rocchio\", el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sinInformación de retroalimentación de relevancia.",
                "El tiempo de cálculo en las horas de la máquina fue de 0.33 para la ejecución de \"rocchio\" adaptativo y 14 para la ejecución de LR en TDT5 al optimizar CTRK.",
                "La Tabla 4 resume los resultados en TDT5SU;Adaptive LR fue el ganador en este caso, con retroalimentación de relevancia sobre el 0.05% de los documentos de prueba que mejoran la utilidad en un 20.9% sobre los resultados de PRF \"Rocchio\".",
                "Tabla 3: Métodos AF en TDT5 (rendimiento en CTRK) ROC Base PRF ROC ADP ROC LR% de RF 0% 0% 0% 0.6% 0.2% CTRK 0.076 0.0707 0.0324 0.0382 ±% +7% (basal) -54% -46% Tabla4: Métodos AF en TDT5 (rendimiento en TDT5SU) Base ROC PRF ROC ADP ROC LR (λ = .01)% de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (basal) +6.9.9.9.9% +20.9% Evidentemente, tanto \"Rocchio\" como LR son altamente efectivos en el filtrado adaptativo, en términos de usar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el objetivo principal de FA.5.4 Resumen del proceso de adaptación Después de que decidimos la configuración de los parámetros utilizando la validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/\"Rocchio\" utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos de muestras aleatoriamente;2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre relevancia y luego recibimos comentarios de relevancia para aquellos (predichos) documentos positivos.3) El modelo y las estadísticas de las FDI se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia.6.",
                "Observaciones finales presentamos una evaluación transversal de \"rocchio\" incremental y LR incremental en el filtrado adaptativo, centrándonos en su robustez en términos de consistencia del rendimiento con respecto a la optimización de los parámetros cruzados.",
                "Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de los parámetros en AF es un desafío abierto, pero no se ha estudiado a fondo en el pasado.• La robustez en el ajuste de los parámetros cruzados es importante para la evaluación y la comparación de métodos.• Encontramos LR más robusto que \"Rocchio\";Tuvo los mejores resultados (en T11SU) jamás informados en TDT5, TREC10 y TREC11 sin un ajuste extenso.• Descubrimos que \"Rocchio\" funciona fuertemente cuando hay un buen corpus de validación disponible, y una opción preferida al optimizar CTRK es el objetivo, favorecer el retiro sobre precisión a un extremo.",
                "Aumento y \"Rocchio\" aplicado al filtrado de texto."
            ],
            "translated_text": "",
            "candidates": [
                "rocio",
                "rocchio",
                "rocio",
                "Rocchio",
                "rocio",
                "rocchio",
                "rocio",
                "rocho",
                "rocio",
                "rocchio",
                "rocio",
                "Rocchio",
                "rocio",
                "Rocchio",
                "rocio",
                "Rocchio",
                "rocchio",
                "Rocchio",
                "rocchio",
                "rocio",
                "Rocchio",
                "Rocchio",
                "Rocchio",
                "rocchio",
                "rocio",
                "rocho",
                "rocio",
                "rocchio",
                "rocio",
                "Rocchio",
                "rocio",
                "Rocchio",
                "rocho",
                "Rocchio",
                "Rocchio",
                "rocho",
                "rocio",
                "Rocchio",
                "Rocchio",
                "Rocchio",
                "Rocchio",
                "Rocchio",
                "rocio",
                "Rocchio",
                "rocio",
                "Rocchio",
                "rocio",
                "Rocchio",
                "rocio",
                "rocchio",
                "rocio",
                "rocchio",
                "Rocchio ",
                "rocio",
                "Rocchio",
                "rocio",
                "rocchio",
                "rocio",
                "Rocchio",
                "Rocchio",
                "rocio",
                "Rocchio",
                "rocio",
                "rocchio",
                "rocio",
                "rocho",
                "rocio",
                "rocchio",
                "rocio",
                "rocho",
                "rocio",
                "rocho",
                "rocho",
                "rocio",
                "Rocchio",
                "Rocchio",
                "rocio",
                "Rocchio",
                "rocio",
                "Rocchio",
                "Rocchio",
                "rocio",
                "Rocchio",
                "Rocchio",
                "rocio",
                "rocchio",
                "rocio",
                "Rocchio",
                "Rocchio",
                "Rocchio",
                "Rocchio",
                "rocio",
                "rocchio",
                "rocio",
                "Rocchio",
                "Rocchio",
                "rocio",
                "Rocchio"
            ],
            "error": []
        }
    }
}