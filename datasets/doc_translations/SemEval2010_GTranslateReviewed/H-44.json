{
    "id": "H-44",
    "original_text": "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem. As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search. We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results. In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance. These techniques can be formulated as optimization problems that can be solved to near-optimality. Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets. Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections. Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1. INTRODUCTION In this work we address time-travel text search over temporally versioned document collections. Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds. Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents. Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing. Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates. For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians. Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives. If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need. Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered. Looking at their evolutionary history, we are faced with even larger data volumes. As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes. This paper presents an efficient solution to time-travel text search by making the following key contributions: 1. The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2. Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3. We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4. In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents. The remainder of this paper is organized as follows. The presented work is put in context with related work in Section 2. We delineate our model of a temporally versioned document collection in Section 3. We present our time-travel inverted index in Section 4. Building on it, temporal coalescing is described in Section 5. In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2. RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index. We briefly review work under these categories here. To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents. Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries. Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past. Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents. Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results. Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives. This adaptation, however, does not provide the intended time-travel text search functionality. In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28]. Unlike the inverted file index, their applicability to text search is not well understood. Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size. Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context. More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size. None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality. Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result. They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction. It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3. MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following. Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . . Each version dti has an associated timestamp ti reflecting when the version was created. Each version is a vector of searchable terms or features. Any modification to a document version results in the insertion of a new version with corresponding timestamp. We employ a discrete definition of time, so that timestamps are non-negative integers. The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥. The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now). Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity. As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well. For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined. We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered. The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) . It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti. The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively. The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4. TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems. In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list. The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload. The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document. The sort-order of index lists depends on which queries are to be supported efficiently. For Boolean queries it is favorable to sort index lists in document-order. Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31]. A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists. For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information. The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid. The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval. As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) . Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree. Unlike the tf-score, the idf-score of every term could vary with every change in the corpus. Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary. Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings. We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing. To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)). Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost. As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly. We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists. As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5. TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version. For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance. The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size. It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched. As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all. Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded. This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document. Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example. The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered. We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions. Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv. As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) . Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately. We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm. Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold . In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ . In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| . Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee. Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20]. Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence. In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings. Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time. Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5]. The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work. As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21]. This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution. Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . . O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I. While doing so, it coalesces sequences of postings having maximal length. The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details). When reading the next posting, the algorithm tries to add it to the current sequence of postings. It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee. If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized. The time complexity of the algorithm is in O(n). Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6. SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings. Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains. In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index. Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist. Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists. Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2. The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3. For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10. Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list. Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case. Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively. Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section. However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone. The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries. Further, the two techniques, can be applied separately and are independent. If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller. We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} . To aid the presentation in the rest of the paper, we first provide some definitions. Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv. Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals. We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed. We also assume that intervals in M are disjoint. We can make this assumption without ruling out any optimal solution with regard to space or performance defined below. The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1). The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ). Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved. Therefore, we will refer to this approach as Popt in the remainder. The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder. This approach requires minimal space, since it keeps each posting exactly once. Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance. The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists. In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting. If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3). The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained. In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1. Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| . An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| . Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee. Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5]. The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed. The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space. In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit. The technique presented next, which is named SB, tackles this very problem. The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt. The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance). In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1). Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t. X m∈M |Lv : m| ≤ κ |Lv| . The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization. A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5]. Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets. We obtain an approximate solution to the problem using simulated annealing [22, 23]. Simulated annealing takes a fixed number R of rounds to explore the solution space. In each round a random successor of the current solution is looked at. If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept). A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution. If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds. In addition, throughout all rounds, the method keeps track of the best solution seen so far. The solution space for the problem at hand can be efficiently explored. As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals. We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set. Note that b1 and bn are always set to true. Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }. A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables. The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round. Its space complexity is in O(n) - for keeping the n boolean variables. As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7. EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5. All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003. All data and indexes are kept in an Oracle 10g database that runs on the same machine. For our experiments we used two different datasets. The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file. This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download). We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.). This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18. We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.). The thus extracted queries contained a total of 422 distinct terms. For each extracted query, we randomly picked a time point for each month covered by the dataset. This resulted in a total of 18, 000 (= 300 × 60) time-travel queries. The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data. We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper. This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79). We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc.), and randomly sampling a time point for every month within the two year period spanned by the dataset. Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset. In total 522 terms appear in the extracted queries. The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality. For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline. WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings. As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size. Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude. Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size. Index size continues to reduce on both datasets, as we increase the value of . How does the reduction in index size affect the query results? In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively. We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement). Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01. Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph. It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits. For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index. Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95. For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively. On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6. For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10. In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations. However, note that the postings in the materialized sublists still retain their original timestamps. For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows. The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists. To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points. We report the mean EPC, as well as the 5%- and 95%-percentile. In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload. The Sopt and Popt approaches are, by their definition, parameter-free. For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0. Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0. Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds. Table 2 lists the obtained space and performance figures. Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus. Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption. Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost. The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude. We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8. CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections. Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results. The present work opens up many interesting questions for future research, e.g.: How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?. How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point? How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9. ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10. REFERENCES [1] V. N. Anh and A. Moffat. Pruned Query Evaluation Using Pre-Computed Impacts. In SIGIR, 2006. [2] V. N. Anh and A. Moffat. Pruning Strategies for Mixed-Mode Querying. In CIKM, 2006. WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn. Versioning a Full-Text Information Retrieval System. In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum. A Time Machine for Text search. Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo. Coalescing in Temporal Databases. In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna. Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations. In WAW, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita. Indexing Shared Content in Information Retrieval Systems. In EDBT, 2006. [9] C. Buckley and A. F. Lewit. Optimization of Inverted Vector Searches. In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen. Method and Apparatus for Generating and Searching Range-Based Index of Word Locations. U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke. A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems. In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer. Static Index Pruning for Information Retrieval Systems. In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar. Comparing Top k Lists. SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor. Optimal Aggregation Algorithms for Middleware. J. Comput. Syst. Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J. Woo. REHIST: Relative Error Histogram Construction Algorithms. In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev. Efficient Indexing of Versioned Document Sequences. In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala. Balancing Histogram Optimality and Practicality for Query Result Size Estimation. In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel. Optimal Histograms with Quality Guarantees. In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani. An Online Algorithm for Segmenting Time Series. In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi. Optimization by Simulated Annealing. Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos. Algorithm Design. Addison-Wesley, 2005. [24] U. Manber. Introduction to Algorithms: A Creative Approach. Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø. DyST: Dynamic and Scalable Temporal Text Indexing. In TIME, 2006. [26] J. M. Ponte and W. B. Croft. A Language Modeling Approach to Information Retrieval. In SIGIR, 1998. [27] S. E. Robertson and S. Walker. Okapi/Keenbow at TREC-8. In TREC, 1999. [28] B. Salzberg and V. J. Tsotras. Comparison of Access Methods for Time-Evolving Data. ACM Comput. Surv., 31(2):158-221, 1999. [29] M. Stack. Full Text Search of Web Archive Collections. In IWAW, 2006. [30] E. Terzi and P. Tsaparas. Efficient Algorithms for Sequence Segmentation. In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel. Top-k Query Evaluation with Probabilistic Guarantees. In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images. Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel. Efficient Search in Large Textual Collections with Redundancy. In WWW, 2007. [35] J. Zobel and A. Moffat. Inverted Files for Text Search Engines. ACM Comput. Surv., 38(2):6, 2006.",
    "original_translation": "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Instituto de Informática Saarbr¨ucken, Alemania {Kberberi, Bedathur, Neumann, Weikum}@mpi-inf.mpg.de Resumen de texto Abierto sobre colecciones documentadas con versión temporalcomo los archivos web ha recibido poca atención como problema de investigación. Como consecuencia, no existe una solución escalable y de principios para buscar dicha colección a partir de un tiempo especificado t.En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo extendiendo el índice de archivos invertidos para prepararlo para la búsqueda temporal. Introducimos a la coalescación temporal aproximada como un método sintonizable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, presentamos dos técnicas de principios para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que se pueden resolver a casi óptimos. Finalmente, nuestro enfoque se evalúa en una serie completa de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran inequívocamente que nuestros métodos permiten construir una máquina de tiempo eficiente escalable a grandes colecciones de texto de versiones. Categorías y descriptores de sujetos H.3.1 [Análisis e indexación de contenido]: métodos de indexación;H.3.3 [Búsqueda y recuperación de información]: modelos de recuperación, algoritmos de términos generales del proceso de búsqueda, experimentación, rendimiento 1. Introducción En este trabajo, abordamos la búsqueda de texto de viaje en el tiempo a través de colecciones de documentos versionadas temporalmente. Dada una consulta de palabras clave, Q y un tiempo en nuestro objetivo es identificar y clasificar los documentos relevantes como si la colección estuviera en su estado a partir del tiempo t.Un número creciente de tales colecciones de documentos versionadas está disponible hoy, incluidos archivos web, entornos de autoría colaborativa como wikis o feeds de información de tiempo de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es principalmente firme en el tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo la versión más reciente de un documento está indexada o las versiones se indexan de forma independiente y se tratan como documentos separados. Peor aún, para algunas colecciones, en particular los archivos web como el Archivo de Internet [18], una funcionalidad integral de búsqueda de texto a menudo falta por completo. La búsqueda de texto de viaje en el tiempo, como lo desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desarrollar su máximo potencial como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar opiniones y declaraciones tempranas hechas por los políticos involucrados. Al enviar una consulta apropiada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo una cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en los archivos web. Si la consulta pudiera enriquecerse con un punto de tiempo, digamos el 20 de agosto de 2003, ya que el día después del escándalo se reveló, y se emitió contra un archivo web, solo las páginas que existían específicamente en ese momento podrían recuperarse, satisfaciendo mejor la necesidad de la información de los periodistas.. Las colecciones de documentos como Web o Wikipedia [32], como las orientamos aquí, ya son grandes si solo se considera una sola instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques de la búsqueda de texto de viajes en el tiempo fallan, y los enfoques viables deben ampliarse bien a tales grandes volúmenes de datos. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular índice de archivos invertidos bien estudiados [35] se extiende de manera transparente para habilitar la búsqueda de texto de viaje en tiempo.2. Se introduce la coalescencia temporal para evitar una explosión de índice y mantener los resultados altamente precisos.3. Desarrollamos dos técnicas de materialización sublista para mejorar el rendimiento del índice que permiten el comercio fuera del espacio frente al rendimiento.4. En una evaluación experimental integral, nuestro enfoque se evalúa en la wikipedia inglesa y las partes del archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se pone en contexto con el trabajo relacionado en la Sección 2. Delineamos nuestro modelo de una recopilación de documentos versionada temporalmente en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Sobre la base de él, la fusión temporal se describe en la Sección 5. En la Sección 6 describimos técnicas de principios para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. 2. Trabajo relacionado Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) Métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice explotando la superposición del contenido de documento o la superposición o la superposiciónAl podar porciones del índice. Revisamos brevemente el trabajo en estas categorías aquí. Hasta donde sabemos, hay muy poco trabajo previo que se ocupe de la búsqueda histórica sobre documentos versionados temporalmente. Anick y Flynn [3], mientras son pioneros en esta investigación, describen un sistema de compensación de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para los accesos a las versiones más recientes y aumentan a medida que uno se mueve más al pasado. Burrows y Hisgen [10], en una descripción de la patente, delinean un método para indexar valores basados en el rango y mencionar su uso potencial para buscar en función de las fechas asociadas con los documentos. Trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se concentran en el problema relativamente más simple de apoyar solo consultas de contenido de texto y descuidar la puntuación de relevancia de los resultados. Stack [29] informa experiencias prácticas hechas al adaptar la Nutch de motores de búsqueda de código abierto para buscar archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad prevista de búsqueda de texto de viaje en el tiempo. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índice adaptadas para bases de datos que evolucionan en el tiempo;Una descripción completa del estado de arte está disponible en [28]. A diferencia del índice de archivos invertidos, su aplicabilidad a la búsqueda de texto no se entiende bien. Pasando a la segunda categoría de trabajo relacionado, Broder et al.[8] Describa una técnica que explota un gran contenido se superpone entre los documentos para lograr una reducción en el tamaño del índice. Su técnica hace supuestos fuertes sobre la estructura de las superposiciones de documentos que lo hace inaplicable a nuestro contexto. Enfoques más recientes de Hyovici et al.[17] y Zhang y suel [34] explotan el contenido arbitrario se superponen entre documentos para reducir el tamaño del índice. Sin embargo, ninguno de los enfoques considera el tiempo explícitamente o proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo deseado. Las técnicas de impulso de índice estático [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando partes del índice que se espera que tengan un bajo impacto en el resultado de la consulta. Tampoco consideran aspectos temporales de los documentos y, por lo tanto, son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe señalar que las técnicas de invitación de índice pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí.3. Modelo En el presente trabajo, tratamos con una colección de documentos versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1, dt2 ,.... Cada versión DTI tiene una marca de tiempo asociada TI que refleja cuándo se creó la versión. Cada versión es un vector de términos o características de búsqueda. Cualquier modificación a una versión de documento da como resultado la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta de tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo TI, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. La validez de tiempo intervalo de tiempo Val (DTI) de una versión DTI es [Ti, Ti+1), si existe una versión más nueva con la marca de tiempo Asociada Ti+1, y [TI, ahora), de lo contrario, ahora apunta al mayor valor posible deuna marca de tiempo (es decir, ∀t: t <ahora). Al poner todo esto, definimos el estado DT de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son deleciones) como dt = [d∈D {dti ∈ D |t ∈ Val (dti) ∧ dti = ⊥}. Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave Q con una marca de tiempo T, de modo que Q se evalúe sobre DT, es decir, el estado de la colección en el momento t.La consulta enriquecida de viajes en el tiempo está escrita como q t para brevedad. Como modelo de recuperación en este trabajo, adoptamos OKAPI BM25 [27], pero tenga en cuenta que las técnicas propuestas no dependen de esta elección y también son aplicables a otros modelos de recuperación como TF-IDF [4] o modelos de idiomas [26]. Para nuestra configuración considerada, adaptamos ligeramente OKAPI BM25 como W (Q T, Dti) = x V∈Q WTF (V, DTI) · Widf (V, T). En la fórmula anterior, se define la relevancia W (Q T, DTI) de una versión de documento DTI a la consulta de viaje en el tiempo Q t. Reiteramos que Q T se evalúa sobre DT para que solo se considera la versión DTI válida en el tiempo T. El primer factor WTF (V, DTI) en la suma, que se conoce más además, el TFScore se define como wtf (v, dti) = (k1 + 1) · tf (v, dti) k1 · ((1 - b) +b · dl (d ti) avdl (ti)) + tf (v, dti). Considera la frecuencia de término simple TF (V, DTI) del término V en la versión DTI que lo normaliza, teniendo en cuenta tanto la longitud DL (DTI) de la versión como la longitud promedio de la longitud del documento (TI) en la colección en el tiempo TI. El parámetro de longitud-normalización B y el parámetro de saturación TF K1 se heredan del OKAPI BM25 original y se establecen comúnmente en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como el puntaje de IDF en el resto, transmite la frecuencia del documento inverso del término V en la colección en el tiempo t y se define como widf (v, t) = log n(t) - df (v, t) + 0.5 df (v, t) + 0.5 donde n (t) = | dt |es el tamaño de la colección en el tiempo t y df (v, t) proporciona el número de documentos en la colección que contienen el término V en el momento t.Mientras que la puntuación de IDF depende de todo el corpus a partir del tiempo de consulta T, el puntaje TF es específico de cada versión.4. Time-TravelInvertedFileIndex El índice de archivos invertidos es una técnica estándar para la indexación de texto, implementada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivos invertidos que lo preparan para la búsqueda de texto de viaje en el tiempo.4.1 Índice de archivos invertidos Un índice de archivos invertidos consiste en un vocabulario, comúnmente organizado como un b+-tree, que asigna cada término a su IDFScore y una lista invertida. La lista de índices LV que pertenece al término V contiene publicaciones del formulario (d, p) donde D es un identificador de documentos y P es la llamada carga útil. La carga útil P contiene información sobre el término frecuencia de V en D, pero también puede incluir información posicional sobre dónde aparece el término en el documento. El orden de clasificación de las listas de índice depende de qué consultas deben ser compatibles de manera eficiente. Para consultas booleanas, es favorable clasificar las listas de índices en el orden de documento. Las listas de índices ordenados de orden de frecuencia y orden de impacto son beneficiosos para las consultas clasificadas y permiten el procesamiento de consultas optimizado que se detiene temprano después de haber identificado los K documentos más relevantes [1, 2, 9, 15, 31]. Se han propuesto una variedad de técnicas de compresión, como la codificación de identificadores de documentos de manera más compacta, [33, 35] para reducir el tamaño de las listas de índices. Para una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos [35].4.2 Índice de archivos invertido de viaje en el tiempo Para preparar un índice de archivos invertidos para viajes en tiempo, ampliamos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal. La idea principal de las listas invertidas es que incluimos un interval de validez [TB, TE) en las publicaciones para denotar cuándo era válida la información de carga útil. Las publicaciones en nuestro índice de archivos invertido de viaje en tiempo son, por lo tanto, del formulario (d, p, [tb, te)) donde d y p se definen como en el índice de archivos invertido estándar anterior y [tb, te) es el tiempo de validez-intervalo. Como ejemplo concreto, en nuestra implementación, para una versión DTI que tiene el OKAPI BM25 TF-score WTF (V, DTI) para el término V, la lista de índice LV contiene la publicación (D, WTF (V, DTI), [TI,ti+1)). Del mismo modo, la estructura de vocabulario extendida mantiene para cada término una serie de tiempo de puntajes IDF organizados como un árbol B+. A diferencia de la puntuación TF, el puntaje IDF de cada término podría variar con cada cambio en el corpus. Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento del puntaje de IDF, calculando las puntuaciones de las FDI para todos los términos en el corpus en momentos específicos (posiblemente periódicos).4.3 Procesamiento de consultas durante el procesamiento de una consulta de viaje en el tiempo Q T, para cada término de consulta, el puntaje IDF correspondiente válido en el tiempo T se recupera del vocabulario extendido. Luego, las listas de índice se leen secuencialmente del disco, acumulando así la información contenida en las publicaciones. Extendemos de manera transparente la lectura secuencial, que es, lo mejor de nuestro conocimiento, a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, lo que las hace adecuadas para el procesamiento de consultas de viaje en el tiempo. Para este fin, la lectura secuencial se extiende omitiendo todas las publicaciones cuya validez intervalo de tiempo no contiene t (es decir, t ∈ [Tb, TE)). Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, aún incurra en un costo significativo de E/S. Como remedio, proponemos técnicas de organización índice en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S. Observamos que nuestra extensión propuesta del índice de archivos invertidos no hace suposiciones sobre el orden de clasificación de las listas de índice. Como consecuencia, las técnicas de procesamiento de consultas existentes y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables.5. Counsescing temporal Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, a una recopilación de documentos versionada, obtenemos una publicación por término por versión de documento. Para términos frecuentes y grandes colecciones altamente dinámicas, esta puntuación de tiempo no coalimentó la Figura 1: la coalescación temporal aproximada conduce a listas de índices extremadamente largas con un rendimiento de consulta muy pobre. La técnica de fusión temporal aproximada que proponemos en esta sección contrarresta esta explosión en el tamaño de la lista de índices. Se basa en la observación de que la mayoría de los cambios en una recopilación de documentos versionada son menores, dejando intactos grandes partes del documento. Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no. La coalcación temporal aproximada reduce el número de publicaciones en una lista de índices fusionando dicha secuencia de publicaciones que tienen cargas útiles casi iguales, mientras mantienen el error máximo limitado. Esta idea se ilustra en la Figura 1, que traza puntajes no coalimentados y fusionados de publicaciones que pertenecen a un solo documento. La fusión temporal aproximada es muy efectiva dada tales cargas útiles fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo. La noción de fusión temporal se introdujo originalmente en la investigación de la base de datos temporal por B¨ohlen et al.[6], donde se consideró el problema más simple de fusionar solo información igual. A continuación, declaramos formalmente el problema tratado en una fusión temporal aproximada y discutimos el cálculo de soluciones óptimas y aproximadas. Tenga en cuenta que la técnica se aplica a cada lista de índices por separado, de modo que las siguientes explicaciones asumen un término V y una lista de índice LV. Como entrada, se nos da una secuencia de publicaciones temporalmente adyacentes i = (d, pi, [ti, ti+1)) ,..., (d, pn - 1, [tn - 1, tn))). Cada secuencia representa un período de tiempo contiguo durante el cual el término estuvo presente en un solo documento d.Si un término desaparece de D pero reaparece más tarde, obtenemos múltiples secuencias de entrada que se tratan por separado. Buscamos generar la secuencia de salida de longitud mínima de las publicaciones o = (d, pj, [tj, tj+1)) ,..., (d, pm-1, [tm-1, tm))), que se adhiere a las siguientes restricciones: Primero, O y yo debemos cubrir el mismo rango de tiempo, es decir, Ti = tj y tn = tm. En segundo lugar, al fusionar una subsecuencia de publicaciones de la entrada en una sola publicación de la salida, queremos que el error de aproximación esté por debajo de un umbral. En otras palabras, If (D, Pi, [Ti, Ti+1)) y (D, PJ, [TJ, TJ+1)) son publicaciones de I y O respectivamente, entonces lo siguiente debe mantener una función de error elegiday un umbral: tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error (pi, pj) ≤. En este documento, como función de error, empleamos el error relativo entre las cargas útiles (es decir, puntajes TF) de un documento en I y O, definido como: Errrel (Pi, Pj) = | Pi-Pj |/ | Pi |. Encontrar una secuencia óptima de salida de publicaciones se puede lanzar en la búsqueda de una representación en constante parte para los puntos (Ti, PI) que utiliza un número mínimo de segmentos mientras conserva la garantía de aproximación anterior. Se producen problemas similares en la segmentación de la serie temporal [21, 30] y la construcción de histogramas [19, 20]. La programación típicamente dinámica se aplica para obtener una solución óptima en el tiempo O (N2 M ∗) [20, 30] con M ∗ el número de segmentos en una secuencia óptima. En nuestro entorno, como una diferencia clave, solo se conserva una garantía del error local, en contraste con una garantía sobre el error global en la configuración mencionada anteriormente. Explotando este hecho, una solución óptima es computable mediante la inducción [24] en el tiempo O (N2). Los detalles del algoritmo óptimo se omiten aquí, pero se pueden encontrar en el informe técnico acompañante [5]. La complejidad cuadrática del algoritmo óptimo lo hace inapropiado para los grandes conjuntos de datos encontrados en este trabajo. Como alternativa, presentamos un algoritmo aproximado de tiempo lineal que se basa en el algoritmo de ventana deslizante dada en [21]. Este algoritmo produce secuencias de salida casi óptimas que conservan el límite en el error relativo, pero posiblemente requieren algunos segmentos adicionales más que una solución óptima. Algoritmo 1 Coalcing temporal (aproximado) 1: i = (d, pi, [ti, ti+1)) ,... O = 2: pmin = pi pMax = pi p = pi tb = ti te = ti+1 3: para (d, pj, [tj, tj+1)) ∈ I do 4: pmin = min (pmin, pj)pMax = max (pMax, pj) 5: p = optrep (pmin, pMax) 6: if errrel (pmin, p) ≤ ∧ errrel (pMax, p) ≤ entonces 7: pmin = pmin pMax = pMax p = p te =tj+1 8: else 9: o = o ∪ (d, p, [tb, te)) 10: pmin = pj pMax = pj p = pj tb = tj te = tj+1 11: final si 12: final para13: O = O ∪ (D, P, [Tb, TE)) Algoritmo 1 hace que uno pase sobre la secuencia de entrada I. Mientras lo hace, fusiona secuencias de publicaciones que tienen una longitud máxima. El representante óptimo para una secuencia de publicaciones depende solo de su carga útil mínima y máxima (PMIN y PMAX) y se puede buscar con Optrep en O (1) (ver [16] para más detalles). Al leer la próxima publicación, el algoritmo intenta agregarlo a la secuencia actual de publicaciones. Calcula el nuevo representante hipotético P y verifica si conservaría la garantía de aproximación. Si esta prueba falla, se agrega una publicación fusionada del antiguo representante a la secuencia de salida O y, después de eso, la contabilidad se reinicializa. La complejidad del tiempo del algoritmo está en O (N). Tenga en cuenta que, dado que no hacemos suposiciones sobre el orden de clasificación de las listas de índices, los algoritmos temporales de coalesidad tienen un costo de preprocesamiento adicional en o (| lv | log | lv |) para clasificar la lista de índices y cortarlo en subsecuencias para cada documento.6. La eficiencia de materialización sublista de procesar una consulta Q t en nuestro índice invertido de viaje en el tiempo está influenciada adversamente por la E/S desperdiciada debido a las publicaciones de lectura pero omitidas. La fusión temporal aborda implícitamente este problema al reducir el tamaño general de la lista de índices, pero sigue siendo una sobrecarga significativa. En esta sección, abordamos este problema proponiendo la idea de materializar sublistas, cada uno de los cuales corresponde a un subintervalo contiguo de tiempo abarcado por el índice completo. Cada uno de estos sublists contiene todas las publicaciones fusionadas que se superponen con el intervalo de tiempo correspondiente del sublista. Tenga en cuenta que todas las publicaciones cuya intervalo de tiempo de validez abarca los límites temporales de varios sublistas se replican en cada uno de los sublistas abarcados. Por lo tanto, para procesar la consulta Q t Tiempo T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 D1 D2 D3 Documento 1 2 3 4 5 6 7 8 9 10 Figura 2: Materialización Sublista Es suficiente para escanear cualquier sublist materializado cuyo TimeInterval contienet.Ilustramos la idea de la materialización sublista utilizando un ejemplo que se muestra en la Figura 2. La lista de índice LV visualizado en la figura contiene un total de 10 publicaciones de tres documentos D1, D2 y D3. Para facilitar la descripción, tenemos límites numerados de intervalos de tiempo de validez, en el aumento de tiempo de tiempo, como T1 ,..., T10 y numeró las publicaciones en sí como 1 ,..., 10. Ahora, considere el procesamiento de una consulta Q t con t ∈ [T1, T2) usando esta lista invertida. Aunque solo tres publicaciones (Publicaciones 1, 5 y 8) son válidas en el tiempo T, toda la lista invertida debe leerse en el peor de los casos. Supongamos que dividimos el eje de tiempo de la lista en el tiempo T2, formando dos sublistas con publicaciones {1, 5, 8} y {2, 3, 4, 5, 6, 7, 8, 9, 10} respectivamente. Luego, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existieron en esta t.A primera vista, puede parecer contradictorio reducir el tamaño del índice en el primer paso (usando un fase temporal), y luego aumentarlo nuevamente utilizando las técnicas de materialización sublista presentadas en esta sección. Sin embargo, reiteramos que nuestro objetivo principal es mejorar la eficiencia de las consultas de procesamiento, no reducir el tamaño del índice solo. El uso de la fusión temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la materialización sublista mejora el rendimiento al replicar juiciosamente las entradas. Además, las dos técnicas se pueden aplicar por separado y son independientes. Sin embargo, si se aplica en conjunto, existe un efecto sinérgico: los sublistas que se materializan a partir de un índice temporalmente fusionado son generalmente más pequeños. Empleamos la notación LV: [Ti, TJ) para referirnos al sublista materializado para el intervalo de tiempo [Ti, TJ), que se define formalmente como, LV: [Ti, Tj) = {(D, P, [Tb,te)) ∈ LV |tb <tj ∧ te> ti}. Para ayudar a la presentación en el resto del documento, primero proporcionamos algunas definiciones. Sea t = t1...Sea la secuencia ordenada de todos los límites únicos de intervalo de tiempo de una lista invertida LV. Entonces definimos e = {[ti, ti+1) |1 ≤ i <n} para ser el conjunto de intervalos de tiempo elementales. Nos referimos al conjunto de intervalos de tiempo para los cuales los sublistas se materializan como M ⊆ {[Ti, TJ) |1 ≤ i <j ≤ n}, y demanda ∀ t ∈ [t1, tn) ∃ m ∈ M: t ∈ M, es decir, los intervalos de tiempo en M deben cubrir completamente el intervalo de tiempo [T1, TN), de modo que ese tiempo-Se se puede procesar las consultas de contrato q t para todos t ∈ [t1, tn). También suponemos que los intervalos en M son disjuntos. Podemos hacer esta suposición sin descartar ninguna solución óptima con respecto al espacio o el rendimiento definido a continuación. El espacio requerido para la materialización de los sublistas en un conjunto m se define como s (m) = x m∈M | lv: m |, es decir, la longitud total de todas las listas en M. Dada un conjunto m, dejamos π ([ti, ti+1)) = [tj, tk) ∈ M: [ti, ti+1) ⊆ [tj, tk) denota el intervalo de tiempo que se utiliza para procesar consultas Q t con t ∈ [ti, ti+1). El rendimiento de las consultas de procesamiento q t para t ∈ [ti, ti+1) inversamente depende de su costo de procesamiento PC ([Ti, ti+1)) = | LV: π ([Ti, Ti+1)) |, que se supone que es proporcional a la longitud de la lista LV: π ([ti, ti+1)). Por lo tanto, para optimizar el rendimiento de las consultas de procesamiento, minimizamos sus costos de procesamiento.6.1 Enfoques de rendimiento/espacio-óptimo Una estrategia para eliminar el problema de las publicaciones omitidas es materializar con entusiasmo los sublistas para todos los intervalos de tiempo elementales, es decir, elegir M = E. Al hacerlo, para cada consulta solo las publicaciones válidas en el momento T sonLeer y, por lo tanto, se logra el mejor rendimiento posible. Por lo tanto, nos referiremos a este enfoque como Popt en el resto. El enfoque inicial descrito anteriormente que mantiene solo la lista completa LV y, por lo tanto, elige M = {[t1, tn)} se conoce como SOPT en el resto. Este enfoque requiere un espacio mínimo, ya que mantiene cada una publicación exactamente una vez. POPT y SOPT son extremos: el primero proporciona el mejor rendimiento posible pero no es eficiente en el espacio, este último requiere un espacio mínimo pero no proporciona un buen rendimiento. Los dos enfoques presentados en el resto de esta sección permiten operar mutuamente el espacio y el rendimiento y, por lo tanto, pueden considerarse como un medio para explorar el espectro de configuración entre el enfoque POPT y SOPT.6.2 Enfoque de garantía de rendimiento El enfoque POPT claramente desperdicia mucho espacio que materializa muchos sublistas casi idénticos. En el ejemplo ilustrado en la Figura 2 sublistas materializados para [T1, T2) y [T2, T3) difieren solo por una publicación. Si el sublista para [T1, T3) se materializara en su lugar, se podría ahorrar espacio significativo al tiempo que incurra solo en una sobrecarga de una publicación omitida para todos T ∈ [T1, T3). La técnica presentada a continuación está impulsada por la idea de que se pueden lograr ahorros de espacio significativos sobre POPT, si se puede tolerar una pérdida superior en el rendimiento, o para decirlo de manera diferente, si se debe conservar una garantía de rendimiento en relación con la óptima. En detalle, la técnica, a la que nos referimos como PG (garantía de rendimiento) en el resto, encuentra un conjunto M que tiene un espacio mínimo requerido, pero garantiza cualquier intervalo de tiempo elemental [Ti, Ti+1) (y, por lo tanto, para cualquier consultaq t con t ∈ [ti, ti+1)) que el rendimiento es peor que el óptimo por lo sumo un factor de γ ≥ 1. Formalmente, este problema puede declararse como argmin m s (m) s.t.∀ [Ti, ti+1) ∈ E: PC ([Ti, Ti+1)) ≤ γ · | LV: [Ti, Ti+1) |. Se puede calcular una solución óptima al problema mediante la inducción utilizando la recurrencia c ([t1, tk+1)) = min {c ([t1, tj))+| lv: [tj, tk+1) ||1 ≤ j ≤ k ∧ condición}, donde c ([t1, tj)) es el costo óptimo (es decir, el espacio requerido) para el subproblema de prefijo {[ti, ti+1) ∈ E |[Ti, ti+1) ⊆ [t1, tj)} y la condición significa ∀ [ti, ti+1) ∈ E: [Ti, Ti+1) ⊆ [tj, tk+1) ⇒ | lv: [tj, tk+1) |≤ γ · | lv: [ti, ti+1) |. Intuitivamente, la recurrencia establece que una solución óptima para [T1, TK+1) se combina de una solución óptima a un subproblema de prefijo C ([T1, TJ)) y un intervalo de tiempo [TJ, TK+1) que se puede materializarsin violar la garantía de rendimiento. El seudocódigo del algoritmo se omite por razones de espacio, pero se puede encontrar en el informe técnico acompañante [5]. La complejidad del tiempo del algoritmo está en O (N2): para cada subproblema de prefijo se debe evaluar la recurrencia anterior, lo cual es posible en tiempo lineal si los tamaños de la lista | L: [Ti, TJ) |son precomputados. La complejidad del espacio está en O (N2): el costo de mantener las longitudes sublistas precomputadas y memorando soluciones óptimas para los subproblemas prefijos.6.3 Enfoque en el espacio hasta ahora consideramos el problema de materializar a los sublistas que ofrecen una garantía sobre el rendimiento al tiempo que requieren un espacio mínimo. Sin embargo, en muchas situaciones, el espacio de almacenamiento es muy importante y el objetivo sería materializar un conjunto de sublistas que optimice el rendimiento esperado sin exceder un límite de espacio dado. La técnica presentada a continuación, que se llama SB, aborda este mismo problema. La restricción de espacio se modela mediante un parámetro especificado por el usuario κ ≥ 1 que limita la explosión máxima permitida en el tamaño del índice de la solución óptima de espacio proporcionada por SOPT. La técnica SB busca encontrar un conjunto M que se adhiera a este límite de espacio pero minimiza el costo de procesamiento esperado (y, por lo tanto, optimiza el rendimiento esperado). En la definición del costo de procesamiento esperado, P ([Ti, Ti+1)) denota la probabilidad de que un punto de tiempo de consulta esté en [Ti, Ti+1). Formalmente, este problema de materiales sublistas unidos al espacio puede establecerse como argmin m x [ti, ti+1) ∈ E P ([Ti, Ti+1)) · PC ([Ti, Ti+1)) S.T. X m∈M | lv: m |≤ κ | LV |. El problema se puede resolver mediante el uso de la programación dinámica en un número cada vez mayor de intervalos de tiempo: en cada intervalo de tiempo en E, los algoritmos decide si comenzar un nuevo intervalo de tiempo de materialización, utilizando la mejor decisión de materialización conocida de los intervalos de tiempo anteriores y manteniendorastrear del consumo de espacio requerido para la materialización. Aquí se omite una descripción detallada del algoritmo, pero se puede encontrar en el informe técnico acompañante [5]. Desafortunadamente, el algoritmo tiene complejidad de tiempo en O (N3 | lv |) y su complejidad espacial está en O (N2 | LV |), que no es práctica para grandes conjuntos de datos. Obtenemos una solución aproximada al problema utilizando recocido simulado [22, 23]. El recocido simulado toma un número fijo R de rondas para explorar el espacio de la solución. En cada ronda se analiza un sucesor aleatorio de la solución actual. Si el sucesor no se adhiere al límite de espacio, siempre se rechaza (es decir, la solución actual se mantiene). Siempre se acepta un sucesor que se adhiere al límite de espacio si logra un costo de procesamiento esperado más bajo que la solución actual. Si logra un mayor costo de procesamiento esperado, se acepta aleatoriamente con probabilidad e - ∆/R donde ∆ es el aumento en el costo de procesamiento esperado y R ≥ R ≥ 1 denota el número de rondas restantes. Además, en todas las rondas, el método realiza un seguimiento de la mejor solución que se ve hasta ahora. El espacio de solución para el problema en cuestión se puede explorar eficientemente. Como argumentamos anteriormente, solo tenemos que mirar los conjuntos M que cubren completamente el intervalo de tiempo [T1, TN) y no contienen intervalos de tiempo superpuestos. Representamos tal conjunto m como una matriz de n variables booleanas b1...bn que transmiten los límites de los intervalos de tiempo en el conjunto. Tenga en cuenta que B1 y Bn siempre están configurados en verdadero. Inicialmente, todas las variables intermedias n - 2 suponen falsas, que corresponde al conjunto M = {[t1, tn)}. Un sucesor aleatorio ahora se puede generar fácilmente cambiando el valor de una de las variables intermedias N - 2. La complejidad del tiempo del método está en O (N2): el costo de procesamiento esperado debe calcularse en cada ronda. Su complejidad espacial está en o (n), para mantener las variables n booleanas. Como observación secundaria, tenga en cuenta que para κ = 1.0 el método SB no necesariamente produce la solución que se obtiene de SOPT, pero puede producir una solución que requiere la misma cantidad de espacio al tiempo que alcanza un mejor rendimiento esperado.7. Evaluación experimental realizamos una serie completa de experimentos en dos conjuntos de datos del mundo real para evaluar las técnicas propuestas en este documento.7.1 Configuración y conjuntos de datos Las técnicas descritas en este documento se implementaron en un sistema prototipo utilizando Java JDK 1.5. Todos los experimentos descritos a continuación se ejecutaron en una sola máquina SUN V40Z con cuatro CPU de Opteron AMD, 16 GB de RAM, una gran matriz de disco RAID-5 conectado a la red y ejecutando Microsoft Windows Server 2003. Todos los datos e índices se mantienen en una base de datos Oracle 10G que se ejecuta en la misma máquina. Para nuestros experimentos utilizamos dos conjuntos de datos diferentes. El historial de revisión de Wikipedia en inglés (denominado wiki en el resto) está disponible para descargar gratuita como un solo archivo XML. Este gran conjunto de datos, con un total de 0.7 Tbytes, contiene la historia de edición completa de la Wikipedia inglesa de enero de 2001 a diciembre de 2005 (la hora de nuestra descarga). Indexamos todos los artículos de Enciclopedia, excluyendo versiones que fueron marcadas como resultado de una edición menor (por ejemplo, la corrección de errores de ortografía, etc.). Esto produjo un total de 892,255 documentos con 13,976,915 versiones que tienen una media (µ) de 15.67 versiones por documento en desviación estándar (σ) de 59.18. Construimos una carga de trabajo de consulta de viaje en el tiempo utilizando el registro de consultas disponible temporalmente recientemente por AOL Research de la siguiente manera: primero extrajimos las 300 consultas de palabras clave más frecuentes que arrojaron un resultado, haga clic en un artículo de Wikipedia (por ejemplo, la revolución francesa, la temporada 2005 de huracanes, la temporada 2005, Código Da Vinci, etc.). Las consultas extraídas contenían un total de 422 términos distintos. Para cada consulta extraída, elegimos aleatoriamente un punto de tiempo para cada mes cubierto por el conjunto de datos. Esto dio como resultado un total de 18, 000 (= 300 × 60) consultas de viaje en el tiempo. El segundo conjunto de datos utilizado en nuestros experimentos se basó en un subconjunto del archivo europeo [13], que contiene rastreos semanales de 11 sitios web .gov.uk a lo largo de los años 2004 y 2005 que equivalen a 2 tbytes de datos sin procesar. Filtramos documentos que no pertenecen al texto de tipos mime/simple y texto/html, para obtener un conjunto de datos que totaliza 0.4 tbytes y que nos referimos como UKGOV en el resto del documento. Esto incluyó un total de 502,617 documentos con 8,687,108 versiones (µ = 17.28 y σ = 13.79). Construimos una carga de trabajo de consulta correspondiente como se mencionó anteriormente, esta vez eligiendo consultas de palabras clave que condujeron a un sitio en el dominio .gov.uk (por ejemplo, salario mínimo, impuesto a la herencia, fechas de la ceremonia de ciudadanía, etc.) y muestras aleatorios de un punto de tiempo paraCada mes dentro del período de dos años abarcado por el conjunto de datos. Por lo tanto, obtuvimos un total de 7,200 (= 300 × 24) consultas de viaje en el tiempo para el conjunto de datos UKGOV. En total, 522 términos aparecen en las consultas extraídas. Las estadísticas de recolección (es decir, N y AVDL) y las estadísticas de término (es decir, DF) se calcularon en granularidad mensual para ambos conjuntos de datos.7.2 Impacto de la fusión temporal Nuestro primer conjunto de experimentos tiene como objetivo evaluar la técnica de fusión temporal aproximada, descrita en la Sección 5, en términos de reducción del tamaño del índice y su efecto sobre la calidad de los resultados. Para los conjuntos de datos Wiki y Ukgov, comparamos los índices temporalmente fusionados para diferentes valores del umbral de error calculado usando el algoritmo 1 con el índice no coalconizado como línea de base. Wiki UKGov # Publicaciones Ratio # Publicaciones Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.6.69.69,44,44,84199999999.69% 79. % 0.05 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 0.10 379,962,802 4,39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 2.00% al% 2.00%% al% 2.00%%% al% 2.00% al% 2.00% al% 2.00% al% 2.00% al% 2.00%%% al% 2.00% al%.0.50 203,269,464 2.35% 155,434,617 1.97% Tabla 1: Tamaños de índice para índice no coalcado (-) e índices fusionados para diferentes valores de la Tabla 1 resume los tamaños de índice medidos como el número total de publicaciones. Como estos resultados demuestran, la coalescación temporal aproximada es altamente efectiva para reducir el tamaño del índice. Incluso un pequeño valor umbral, p.= 0.01, tiene un efecto considerable al reducir el tamaño del índice casi por orden de magnitud. Tenga en cuenta que en el conjunto de datos de UKGOV, incluso un coalcado preciso (= 0) logra reducir el tamaño del índice a menos del 38% del tamaño original. El tamaño del índice continúa reduciendo en ambos conjuntos de datos, a medida que aumentamos el valor de. ¿Cómo afecta la reducción en el tamaño del índice los resultados de la consulta? Para evaluar este aspecto, comparamos los resultados de Top-K calculados utilizando un índice fusionado contra el resultado de la verdad en tierra obtenida del índice original, para diferentes niveles de corte k.Deje que GK y CK sean los documentos de Top-K del resultado de la verdad en tierra y del índice fusionado respectivamente. Utilizamos las siguientes dos medidas para la comparación: (i) retiro relativo en el nivel de corte K (RR@K), que mide la superposición entre GK y CK, que varía en [0, 1] y se define como RR@K = |Gk ∩ ck |/k.(ii) Kendalls τ (ver [7, 14] para una definición detallada) en el nivel de corte K (KT@K), midiendo el acuerdo entre dos resultados en el orden relativo de los elementos en GK ∩ CK, con el valor 1 (o - o -1) Indicando un acuerdo total (o desacuerdo). Figura 3 Gráficos, para los niveles de corte 10 y 100, la media de RR@K y Kt@K junto con percentiles de 5% y 95%, para diferentes valores del umbral a partir de 0.01. Tenga en cuenta que para = 0, los resultados coinciden con los obtenidos por el índice original y, por lo tanto, se omiten del gráfico. Es tranquilizador ver a partir de estos resultados que la coalización temporal aproximada induce una interrupción mínima a los resultados de la consulta, ya que RR@K y Kt@K están dentro de los límites razonables. Para = 0.01, el valor más pequeño de en nuestros experimentos, RR @ 100 para wiki es 0.98 que indica que los resultados son -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 RECUERDO RELATIVO @ 10 (wiki) Kendalls τ @ 10 (wiki) RECUERDO RELATIVO @ 10 (UKGOV) KENDALLS τ @ 10 (UKGOV) (a) @ 10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 RECURSO RELATIVO @ 100 (Wiki) Kendalls τ @ 100 (Wiki) Relativo @100 (UKGOV) Kendalls τ @ 100 (Ukgov) (b) @ 100 Figura 3: RECREINO RELATIVO Y KENDALLS τ observados en índices fusionados para diferentes valores de casi indistinguibles de los obtenidos a través del índice original. Incluso el orden relativo de estos resultados comunes es bastante alto, ya que la media KT@100 está cerca de 0.95. Para el valor extremo de = 0.5, que da como resultado un tamaño de índice de solo el 2.35% del original, el RR@100 y KT@100 son aproximadamente 0.8 y 0.6 respectivamente. En el conjunto de datos UKGOV relativamente menos dinámico (como se puede ver en los valores σ anteriores), los resultados fueron aún mejores, con altos valores de RR y KT observados en todo el espectro de valores para ambos valores de corte.7.3 Materialización sublista Ahora dirigimos nuestra atención hacia la evaluación de las técnicas de materialización sublista introducidas en la Sección 6. Para ambos conjuntos de datos, comenzamos con el índice fusionado producido por una configuración de umbral moderado de = 0.10. Para reducir el esfuerzo computacional, los límites de los intervalos de tiempo elementales se redondearon a la granularidad del día antes de calcular las materializaciones sublistas. Sin embargo, tenga en cuenta que las publicaciones en los sublistas materializados aún conservan sus marcas de tiempo originales. Para una evaluación comparativa de los cuatro enfoques: POPT, SOPT, PG y SB, medimos el espacio y el rendimiento de la siguiente manera. El espacio requerido S (M), como se definió anteriormente, es igual al número total de publicaciones en los sublistas materializados. Para evaluar el rendimiento, calculamos el costo de procesamiento esperado (EPC) para todos los términos en la carga de trabajo de consulta respectiva, suponiendo una distribución de probabilidad uniforme entre los puntos de tiempo de consulta. Reportamos la EPC media, así como el 5%y 95%-porcentil. En otras palabras, la EPC media refleja la longitud esperada de la lista de índices (en términos de publicaciones de índice) que deben escanear para un punto de tiempo aleatorio y un término aleatorio de la carga de trabajo de consulta. Los enfoques SOPT y POPT son, por su definición, sin parámetros. Para el enfoque PG, variamos su parámetro γ, lo que limita la degradación máxima del rendimiento, entre 1.0 y 3.0. Análogamente, para el enfoque SB, el parámetro κ, como un límite superior en la explosión espacial permitida, varió entre 1.0 y 3.0. Se obtuvieron soluciones para el enfoque SB que ejecuta el recocido simulado para R = 50, 000 rondas. La Tabla 2 enumera las cifras de espacio y rendimiento obtenidas. Tenga en cuenta que los valores de EPC son más pequeños en Wiki que en Ukgov, ya que los términos en la carga de trabajo de consulta empleada para wiki son relativamente más raras en el corpus. Según los resultados representados, hacemos las siguientes observaciones clave.i) Como se esperaba, Popt logra un rendimiento óptimo a costa de un enorme consumo de espacio. Sopt, por el contrario, al consumir una cantidad óptima de espacio, proporciona solo un costo de procesamiento esperado deficiente. Los métodos PG y SB, para diferentes valores de su parámetro respectivo, producen soluciones cuyo espacio y rendimiento se encuentran entre los extremos que representan Popt y SOPT.ii) Para el método PG, vemos que para una degradación de rendimiento aceptable de solo 10% (es decir, γ = 1.10), el espacio requerido cae en más de un orden de magnitud en comparación con POPT en ambos conjuntos de datos.iii) el enfoque SB logra un rendimiento cercano a óptimo en ambos conjuntos de datos, si se permite consumir como máximo tres veces la cantidad óptima de espacio (es decir, κ = 3.0), que en nuestros conjuntos de datos aún corresponde a una reducción de espacio sobre POPT porMás de un orden de magnitud. También medimos los tiempos de reloj de pared en una muestra de las consultas con resultados que indican mejoras en el tiempo de ejecución hasta un factor de 12. 8. Conclusiones En este trabajo, hemos desarrollado una solución eficiente para la búsqueda de texto de viaje en tiempo a través de colecciones de documentos versionadas temporalmente. Los experimentos en dos conjuntos de datos del mundo real mostraron que una combinación de las técnicas propuestas puede reducir el tamaño del índice hasta un orden de magnitud al tiempo que logran un rendimiento casi óptimo y resultados altamente precisos. El presente trabajo abre muchas preguntas interesantes para futuras investigaciones, p.: ¿Cómo podemos mejorar aún más el rendimiento aplicando (y posiblemente extendiendo) la codificación, la compresión y las técnicas de omisión [35]? ¿Cómo podemos extender el enfoque de consultas Q [TB, TE] especificando un intervalo de tiempo en lugar de un punto de tiempo? ¿Cómo puede la funcionalidad de búsqueda de texto de viaje en el tiempo descrito habilitar o acelerar la minería de texto a lo largo del eje de tiempo (por ejemplo, rastrear los cambios de sentimiento en las opiniones de los clientes)?9. Agradecimientos Agradecemos a los revisores anónimos por sus valiosos comentarios, en particular al revisor que señaló la oportunidad de mejoras algorítmicas en la Sección 5 y la Sección 6.2.10. Referencias [1] V. N. Anh y A. Moffat. Evaluación de consultas podadas utilizando impactos precomputados. En Sigir, 2006. [2] V. N. Anh y A. Moffat. Estrategias de poda para consultas en modo mixto. En Cikm, 2006. Wiki Ukgov S (M) EPC S (M) EPC 5% Media 95% 5% Media 95% POPT 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 SOPT 379,962,8014.021. 9,820.1 187,387,342 63.15 22,852.67 102,923.85 pg γ = 1.10 3,814,444,654 11.30 3.306.71 16,512.88 1,155,833,516 40.6616,105.61 71,134.99 pg γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 pg γ = 1.50 1,121,661,751 13.96 4,04. , 578,665 46.68 18,379.69 78,115.89 pg γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 pg γ = 2.00 744,344,284,284,394 4. 53 24,637.62 306,944,062 51.48 19,499.78 87,136.31PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 pg γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,67,66,82. 9,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,11,11. 22,036.39 95,337.33 SB κ= 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,999999999999997. , 377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,034 427,127,122,1222,122,422,422,122,422,422,422,422,222,222,222,222,222. 9 17,153.94 74,449.28 SB κ = 3.001,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Tabla 2: Espacio requerido y costo de procesamiento esperado (en # Publicaciones) observados en índices coalecidos (= 0.10) [3] P. G. Anick y R. Flynn. Versión de un sistema de recuperación de información de texto completo. En Sigir, 1992. [4] R. A. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann y G. Weikum. Una máquina del tiempo para la búsqueda de texto. Informe técnico MPI-I-2007-5-002, Instituto de Informática Max-Planck, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass y M. D. Soo. Falta en bases de datos temporales. En VLDB, 1996. [7] P. Boldi, M. Santini y S. Vigna. Haga lo mejor para hacer lo mejor: efectos paradójicos en los cálculos incrementales de PageRank. En Waw, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi y E. J. Shekita. Indexación de contenido compartido en sistemas de recuperación de información. En EDBT, 2006. [9] C. Buckley y A. F. Lewit. Optimización de búsquedas vectoriales invertidas. En Sigir, 1985. [10] M. Burrows y A. L. Hisgen. Método y aparato para generar y buscar el índice de ubicaciones de palabras basadas en el rango. Patente de EE. UU. 5.915,251, 1999. [11] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índice estático en los sistemas de recuperación de texto. En Cikm, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek y A. Soffer. Poda de índice estático para sistemas de recuperación de información. En Sigir, 2001. [13] http://www.europarchive.org.[14] R. Fagin, R. Kumar y D. Sivakumar. Comparación de las listas K superiores. Siam J. Discrete Math., 17 (1): 134-160, 2003. [15] R. Fagin, A. Lotem y M. Naor. Algoritmos de agregación óptimos para el middleware. J. Comput. Syst. Sci., 66 (4): 614-656, 2003. [16] S. Guha, K. Shim y J. Cortejar. Rehist: algoritmos de construcción de histograma de error relativo. En VLDB, 2004. [17] M. Hallovici, R. Lempel y S. Yogev. Indexación eficiente de secuencias de documentos versionadas. En ECIR, 2007. [18] http://www.archive.org.[19] Y. E. Ioannidis y V. Poosala. Equilibrar la optimización del histograma y la practicidad para la estimación del tamaño del resultado de la consulta. En Sigmod, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik y T. Suel. Histogramas óptimos con garantías de calidad. En VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart y M. J. Pazzani. Un algoritmo en línea para segmentar series de tiempo. En ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr. y M. P. Vecchi. Optimización por recocido simulado. Science, 220 (4598): 671-680, 1983. [23] J. Kleinberg y E. Tardos. Diseño de algoritmo. Addison-Wesley, 2005. [24] U. Manber. Introducción a los algoritmos: un enfoque creativo. Addison-Wesley, 1989. [25] K. Nørv˚ag y A. O. N. Nybø. DIST: indexación de texto temporal dinámico y escalable. En el tiempo, 2006. [26] J. M. Ponte y W. B. Croft. Un enfoque de modelado de idiomas para la recuperación de información. En Sigir, 1998. [27] S. E. Robertson y S. Walker. Okapi/Keenbow en TREC-8. En Trec, 1999. [28] B. Salzberg y V. J. Tsotras. Comparación de métodos de acceso para datos que evolucionan en el tiempo. ACM Comput. Surv., 31 (2): 158-221, 1999. [29] M. Stack. Búsqueda de texto completo de colecciones de archivos web. En IWAW, 2006. [30] E. Terzi y P. Tsaparas. Algoritmos eficientes para la segmentación de secuencia. En Siam-DM, 2006. [31] M. Theobald, G. Weikum y R. Schenkel. Evaluación de consultas de Top-K con garantías probabilísticas. En VLDB, 2004. [32] http://www.wikipedia.org.[33] I. H. Witten, A. Moffat y T. C. Bell. Gestión de gigabytes: compresión e indexación de documentos e imágenes. Morgan Kaufmann Publishers Inc., 1999. [34] J. Zhang y T. Suel. Búsqueda eficiente en grandes colecciones textuales con redundancia. En www, 2007. [35] J. Zobel y A. Moffat. Archivos invertidos para motores de búsqueda de texto. ACM Comput. Surv., 38 (2): 6, 2006.",
    "original_sentences": [
        "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
        "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
        "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
        "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
        "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
        "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
        "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
        "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
        "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
        "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
        "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
        "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
        "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
        "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
        "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
        "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
        "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
        "Looking at their evolutionary history, we are faced with even larger data volumes.",
        "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
        "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
        "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
        "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
        "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
        "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
        "The remainder of this paper is organized as follows.",
        "The presented work is put in context with related work in Section 2.",
        "We delineate our model of a temporally versioned document collection in Section 3.",
        "We present our time-travel inverted index in Section 4.",
        "Building on it, temporal coalescing is described in Section 5.",
        "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
        "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
        "We briefly review work under these categories here.",
        "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
        "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
        "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
        "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
        "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
        "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
        "This adaptation, however, does not provide the intended time-travel text search functionality.",
        "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
        "Unlike the inverted file index, their applicability to text search is not well understood.",
        "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
        "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
        "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
        "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
        "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
        "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
        "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
        "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
        "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
        "Each version dti has an associated timestamp ti reflecting when the version was created.",
        "Each version is a vector of searchable terms or features.",
        "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
        "We employ a discrete definition of time, so that timestamps are non-negative integers.",
        "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
        "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
        "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
        "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
        "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
        "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
        "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
        "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
        "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
        "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
        "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
        "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
        "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
        "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
        "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
        "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
        "The sort-order of index lists depends on which queries are to be supported efficiently.",
        "For Boolean queries it is favorable to sort index lists in document-order.",
        "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
        "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
        "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
        "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
        "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
        "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
        "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
        "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
        "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
        "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
        "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
        "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
        "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
        "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
        "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
        "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
        "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
        "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
        "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
        "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
        "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
        "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
        "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
        "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
        "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
        "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
        "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
        "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
        "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
        "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
        "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
        "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
        "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
        "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
        "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
        "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
        "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
        "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
        "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
        "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
        "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
        "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
        "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
        "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
        "While doing so, it coalesces sequences of postings having maximal length.",
        "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
        "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
        "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
        "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
        "The time complexity of the algorithm is in O(n).",
        "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
        "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
        "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
        "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
        "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
        "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
        "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
        "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
        "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
        "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
        "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
        "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
        "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
        "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
        "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
        "Further, the two techniques, can be applied separately and are independent.",
        "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
        "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
        "To aid the presentation in the rest of the paper, we first provide some definitions.",
        "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
        "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
        "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
        "We also assume that intervals in M are disjoint.",
        "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
        "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
        "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
        "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
        "Therefore, we will refer to this approach as Popt in the remainder.",
        "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
        "This approach requires minimal space, since it keeps each posting exactly once.",
        "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
        "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
        "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
        "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
        "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
        "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
        "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
        "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
        "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
        "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
        "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
        "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
        "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
        "The technique presented next, which is named SB, tackles this very problem.",
        "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
        "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
        "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
        "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
        "X m∈M |Lv : m| ≤ κ |Lv| .",
        "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
        "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
        "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
        "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
        "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
        "In each round a random successor of the current solution is looked at.",
        "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
        "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
        "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
        "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
        "The solution space for the problem at hand can be efficiently explored.",
        "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
        "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
        "Note that b1 and bn are always set to true.",
        "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
        "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
        "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
        "Its space complexity is in O(n) - for keeping the n boolean variables.",
        "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
        "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
        "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
        "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
        "For our experiments we used two different datasets.",
        "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
        "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
        "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
        "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
        "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
        "The thus extracted queries contained a total of 422 distinct terms.",
        "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
        "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
        "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
        "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
        "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
        "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
        "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
        "In total 522 terms appear in the extracted queries.",
        "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
        "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
        "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
        "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
        "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
        "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
        "Index size continues to reduce on both datasets, as we increase the value of .",
        "How does the reduction in index size affect the query results?",
        "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
        "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
        "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
        "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
        "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
        "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
        "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
        "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
        "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
        "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
        "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
        "However, note that the postings in the materialized sublists still retain their original timestamps.",
        "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
        "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
        "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
        "We report the mean EPC, as well as the 5%- and 95%-percentile.",
        "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
        "The Sopt and Popt approaches are, by their definition, parameter-free.",
        "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
        "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
        "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
        "Table 2 lists the obtained space and performance figures.",
        "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
        "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
        "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
        "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
        "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
        "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
        "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
        "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
        "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
        "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
        "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
        "REFERENCES [1] V. N. Anh and A. Moffat.",
        "Pruned Query Evaluation Using Pre-Computed Impacts.",
        "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
        "Pruning Strategies for Mixed-Mode Querying.",
        "In CIKM, 2006.",
        "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
        "Versioning a Full-Text Information Retrieval System.",
        "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
        "Modern Information Retrieval.",
        "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
        "A Time Machine for Text search.",
        "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
        "Coalescing in Temporal Databases.",
        "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
        "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
        "In WAW, 2004. [8] A.",
        "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
        "Indexing Shared Content in Information Retrieval Systems.",
        "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
        "Optimization of Inverted Vector Searches.",
        "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
        "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
        "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
        "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
        "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
        "Static Index Pruning for Information Retrieval Systems.",
        "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
        "Comparing Top k Lists.",
        "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
        "Optimal Aggregation Algorithms for Middleware.",
        "J. Comput.",
        "Syst.",
        "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
        "Woo.",
        "REHIST: Relative Error Histogram Construction Algorithms.",
        "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
        "Efficient Indexing of Versioned Document Sequences.",
        "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
        "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
        "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
        "Optimal Histograms with Quality Guarantees.",
        "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
        "An Online Algorithm for Segmenting Time Series.",
        "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
        "Optimization by Simulated Annealing.",
        "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
        "Algorithm Design.",
        "Addison-Wesley, 2005. [24] U. Manber.",
        "Introduction to Algorithms: A Creative Approach.",
        "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
        "DyST: Dynamic and Scalable Temporal Text Indexing.",
        "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
        "A Language Modeling Approach to Information Retrieval.",
        "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
        "Okapi/Keenbow at TREC-8.",
        "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
        "Comparison of Access Methods for Time-Evolving Data.",
        "ACM Comput.",
        "Surv., 31(2):158-221, 1999. [29] M. Stack.",
        "Full Text Search of Web Archive Collections.",
        "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
        "Efficient Algorithms for Sequence Segmentation.",
        "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
        "Top-k Query Evaluation with Probabilistic Guarantees.",
        "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
        "Managing Gigabytes: Compressing and Indexing Documents and Images.",
        "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
        "Efficient Search in Large Textual Collections with Redundancy.",
        "In WWW, 2007. [35] J. Zobel and A. Moffat.",
        "Inverted Files for Text Search Engines.",
        "ACM Comput.",
        "Surv., 38(2):6, 2006."
    ],
    "error_count": 0,
    "keys": {
        "time machine": {
            "translated_key": "máquina del tiempo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A <br>time machine</br> for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient <br>time machine</br> scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A <br>time machine</br> for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Una \"máquina del tiempo\" para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto de Información de Informática Saarbr¨ucken, Alemania {Kberberi, Bedathur, Neumann, Weikum}@mpi-inf.mpg.de Abstracto Texto Búsqueda sobre Temporada versión temporalLas colecciones de documentos como los archivos web han recibido poca atención como problema de investigación.",
                "Los resultados muestran inequívocamente que nuestros métodos permiten construir una eficiente \"máquina de tiempo\" escalable a grandes colecciones de texto de versiones.",
                "Una \"máquina de tiempo\" para la búsqueda de texto."
            ],
            "translated_text": "",
            "candidates": [
                "máquina del tiempo",
                "máquina del tiempo",
                "máquina del tiempo",
                "máquina de tiempo",
                "máquina del tiempo",
                "máquina de tiempo"
            ],
            "error": []
        },
        "text search": {
            "translated_key": "búsqueda de texto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for <br>text search</br> Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT <br>text search</br> over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel <br>text search</br> by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel <br>text search</br> over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "<br>text search</br> on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel <br>text search</br>, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel <br>text search</br> fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel <br>text search</br> by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel <br>text search</br>. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel <br>text search</br> functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to <br>text search</br> is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel <br>text search</br> functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel <br>text search</br>. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel <br>text search</br> over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel <br>text search</br> functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for <br>text search</br>.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full <br>text search</br> of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for <br>text search</br> Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Una máquina del tiempo para \"Búsqueda de texto\" Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto de Información de Informática Saarbr¨ucken, Alemania {Kberberi, Bedathur, Neumann, WeikumhtyLas colecciones de documentos versionadas temporalmente, como los archivos web, han recibido poca atención como problema de investigación.",
                "Como consecuencia, no existe una solución escalable y de principios para buscar dicha colección a partir de un tiempo especificado t.En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la \"búsqueda de texto\" de viaje en el tiempo extendiendo el índice de archivos invertidos para prepararlo para la búsqueda temporal.",
                "Introducción En este trabajo, abordamos la \"búsqueda de texto\" en el tiempo sobre colecciones de documentos versionadas temporalmente.",
                "Sin embargo, la \"búsqueda de texto\" en estas colecciones es principalmente negativa del tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo la versión más reciente de un documento está indexada o las versiones se indexan de forma independiente y se tratan como documentos separados.",
                "La \"búsqueda de texto\" de viaje en el tiempo, como lo desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desarrollar su máximo potencial como lo demuestra el siguiente ejemplo.",
                "Como consecuencia, los enfoques de \"búsqueda de texto\" de viaje en el tiempo fallan, y los enfoques viables deben ampliarse bien a tales grandes volúmenes de datos.",
                "Este documento presenta una solución eficiente a la \"búsqueda de texto\" de viaje en el tiempo al realizar las siguientes contribuciones clave: 1.",
                "El popular índice de archivos invertidos bien estudiados [35] se extiende de manera transparente para habilitar la \"búsqueda de texto\".2.",
                "Sin embargo, esta adaptación no proporciona la funcionalidad prevista de \"búsqueda de texto\" de viaje en el tiempo.",
                "A diferencia del índice de archivos invertidos, su aplicabilidad a la \"búsqueda de texto\" no se entiende bien.",
                "Sin embargo, ninguno de los enfoques considera el tiempo explícitamente o proporciona la funcionalidad deseada de \"búsqueda de texto\".",
                "En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivos invertidos que lo preparan para la \"búsqueda de texto\".4.1 Índice de archivos invertidos Un índice de archivos invertidos consiste en un vocabulario, comúnmente organizado como un b+-tree, que asigna cada término a su IDFScore y una lista invertida.",
                "Conclusiones En este trabajo, hemos desarrollado una solución eficiente para la \"búsqueda de texto\" de viaje en el tiempo sobre colecciones de documentos versionadas temporalmente.",
                "¿Cómo puede la funcionalidad de \"búsqueda de texto de texto\" descrita habilitar o acelerar la minería de texto a lo largo del eje de tiempo (por ejemplo, rastrear los cambios de sentimiento en las opiniones de los clientes)?9.",
                "Una máquina del tiempo para \"búsqueda de texto\".",
                "\"Búsqueda de texto\" completa de colecciones de archivos web.",
                "Archivos invertidos para motores de \"búsqueda de texto\"."
            ],
            "translated_text": "",
            "candidates": [
                "búsqueda de texto",
                "Búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "Búsqueda de texto",
                "búsqueda de texto",
                "Búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "Búsqueda de texto",
                "búsqueda de texto de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto",
                "Búsqueda de texto",
                "búsqueda de texto",
                "búsqueda de texto"
            ],
            "error": []
        },
        "inverted file index": {
            "translated_key": "índice de archivos invertido",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the <br>inverted file index</br> to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied <br>inverted file index</br> [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the <br>inverted file index</br>, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The <br>inverted file index</br> is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the <br>inverted file index</br> that make it ready for time-travel text search. 4.1 <br>inverted file index</br> An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel <br>inverted file index</br> In order to prepare an <br>inverted file index</br> for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel <br>inverted file index</br> are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard <br>inverted file index</br> above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the <br>inverted file index</br> makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Como consecuencia, no existe una solución escalable y de principios para buscar dicha colección a partir de un tiempo especificado t.En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo al extender el \"índice de archivos invertidos\" para prepararlo para la búsqueda temporal.",
                "El popular \"índice de archivos invertidos\" bien estudiado [35] se extiende de manera transparente para habilitar la búsqueda de texto de viaje en tiempo.2.",
                "A diferencia del \"índice de archivos invertidos\", su aplicabilidad a la búsqueda de texto no se entiende bien.",
                "Time-TravelInvertedFileIndex El \"índice de archivos invertidos\" es una técnica estándar para la indexación de texto, implementada en muchos sistemas.",
                "En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al \"índice de archivos invertidos\" que lo preparan para la búsqueda de texto de viaje en el tiempo.4.1 \"Índice de archivos invertidos\" Un índice de archivos invertidos consiste en un vocabulario, comúnmente organizado como un b+-tree, que mapea cada término a su IDFScore y una lista invertida.",
                "Para una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos [35].4.2 Viajes en el tiempo \"Índice de archivos invertidos\" para preparar un \"índice de archivos invertidos\" para viajes en tiempo, ampliamos tanto las listas invertidas como la estructura del vocabulario incorporando explícitamente información temporal.",
                "Las publicaciones en nuestro \"índice de archivos invertido\" de tiempo en el tiempo son, por lo tanto, del formulario (D, P, [TB, TE)) donde D y P se definen como en el \"Índice de archivos invertido\" estándar anteriores y [TB, TE)es el intervalo de tiempo de validez.",
                "Observamos que nuestra extensión propuesta del \"índice de archivos invertidos\" no hace suposiciones sobre el orden de clasificación de las listas de índice."
            ],
            "translated_text": "",
            "candidates": [
                "índice de archivos invertido",
                "índice de archivos invertidos",
                "Índice de archivos invertidos",
                "índice de archivos invertidos",
                "índice de archivos invertido",
                "índice de archivos invertidos",
                "índice de archivos invertido",
                "índice de archivos invertidos",
                "índice de archivos invertido",
                "índice de archivos invertidos",
                "Índice de archivos invertidos",
                "índice de archivos invertido",
                "Índice de archivos invertidos",
                "índice de archivos invertidos",
                "índice de archivos invertido",
                "índice de archivos invertido",
                "Índice de archivos invertido",
                "índice de archivos invertido",
                "índice de archivos invertidos"
            ],
            "error": []
        },
        "temporal search": {
            "translated_key": "búsqueda temporal",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for <br>temporal search</br>.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Como consecuencia, no existe una solución escalable y de principios para buscar dicha colección a partir de un tiempo especificado t.En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo al extender el índice de archivos invertidos para prepararlo para la \"búsqueda temporal\"."
            ],
            "translated_text": "",
            "candidates": [
                "búsqueda temporal",
                "búsqueda temporal"
            ],
            "error": []
        },
        "approximate temporal coalescing": {
            "translated_key": "fusión temporal aproximada",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce <br>approximate temporal coalescing</br> as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: <br>approximate temporal coalescing</br> leads to extremely long index lists with very poor queryprocessing performance.",
                "The <br>approximate temporal coalescing</br> technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "<br>approximate temporal coalescing</br> reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "<br>approximate temporal coalescing</br> is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in <br>approximate temporal coalescing</br>, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the <br>approximate temporal coalescing</br> technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, <br>approximate temporal coalescing</br> is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that <br>approximate temporal coalescing</br> induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Introducimos la \"fusión temporal aproximada\" como un método sintonizable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados.",
                "Para términos frecuentes y grandes colecciones altamente dinámicas, esta puntuación de tiempo no coalimentó la Figura 1: \"Counesscing temporal aproximado\" conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultoría muy pobre.",
                "La técnica de \"aproximación temporal aproximada\" que proponemos en esta sección contrarresta esta explosión en el tamaño de la lista de índices.",
                "\"Coalescre temporal aproximado\" reduce el número de publicaciones en una lista de índices fusionando dicha secuencia de publicaciones que tienen cargas útiles casi iguales, mientras mantienen el error máximo limitado.",
                "La \"fusión temporal aproximada\" es muy efectiva dada tales cargas útiles fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo.",
                "Luego declaramos formalmente el problema tratado en \"Counsescing temporal aproximado\" y discutimos el cálculo de soluciones óptimas y aproximadas.",
                "Las estadísticas de recolección (es decir, N y AVDL) y las estadísticas de término (es decir, DF) se calcularon en granularidad mensual para ambos conjuntos de datos.7.2 Impacto de la fusión temporal Nuestro primer conjunto de experimentos tiene como objetivo evaluar la técnica de \"coalescación temporal aproximada\", descrita en la Sección 5, en términos de reducción del tamaño del índice y su efecto en la calidad de los resultados.",
                "Como demuestran estos resultados, la \"fusión temporal aproximada\" es altamente efectiva para reducir el tamaño del índice.",
                "Es tranquilizador ver a partir de estos resultados que \"aproximar la fusión temporal\" induce una interrupción mínima a los resultados de la consulta, ya que RR@K y Kt@K están dentro de los límites razonables."
            ],
            "translated_text": "",
            "candidates": [
                "coalcación temporal aproximada",
                "fusión temporal aproximada",
                "coalcación temporal aproximada",
                "Counesscing temporal aproximado",
                "coalcación temporal aproximada",
                "aproximación temporal aproximada",
                "coalcación temporal aproximada",
                "Coalescre temporal aproximado",
                "coalcación temporal aproximada",
                "fusión temporal aproximada",
                "coalcación temporal aproximada",
                "Counsescing temporal aproximado",
                "coalcación temporal aproximada",
                "coalescación temporal aproximada",
                "coalcación temporal aproximada",
                "fusión temporal aproximada",
                "coalcación temporal aproximada",
                "aproximar la fusión temporal"
            ],
            "error": []
        },
        "web archive": {
            "translated_key": "archivo web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a <br>web archive</br>, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of <br>web archive</br> Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Si la consulta pudiera enriquecerse con un punto de tiempo, digamos el 20 de agosto de 2003, ya que el día después del escándalo se reveló, y se emitió contra un \"archivo web\", solo las páginas que existían específicamente en ese momento podrían recuperarse mejor satisfaciendo a los periodistasnecesidad de información.",
                "Búsqueda de texto completo de colecciones de \"archivo web\"."
            ],
            "translated_text": "",
            "candidates": [
                "archivo web",
                "archivo web",
                "archivo web",
                "archivo web"
            ],
            "error": []
        },
        "versioned document collection": {
            "translated_key": "recopilación de documentos versionada",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally <br>versioned document collection</br> in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally <br>versioned document collection</br> D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a <br>versioned document collection</br>, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a <br>versioned document collection</br> are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Delineamos nuestro modelo de una \"recopilación de documentos versionada\" temporalmente en la Sección 3.",
                "Modelo en el presente trabajo, tratamos con una \"recopilación de documentos versionada\" temporalmente que se modela como se describe a continuación.",
                "Counsescing temporal Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, a una \"recopilación de documentos versionada\", obtenemos una publicación por término por versión de documento.",
                "Se basa en la observación de que la mayoría de los cambios en una \"recopilación de documentos versionada\" son menores, dejando no tocados grandes partes del documento."
            ],
            "translated_text": "",
            "candidates": [
                "Recopilación de documentos versiones",
                "recopilación de documentos versionada",
                "Recopilación de documentos versionados",
                "recopilación de documentos versionada",
                "Recopilación de documentos versionados",
                "recopilación de documentos versionada",
                "Recopilación de documentos versionados",
                "recopilación de documentos versionada"
            ],
            "error": []
        },
        "collaborative authoring environment": {
            "translated_key": "entorno de autoría colaborativa",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, <br>collaborative authoring environment</br>s like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Dada una consulta de palabras clave, Q y un tiempo en nuestro objetivo es identificar y clasificar los documentos relevantes como si la colección estuviera en su estado a partir del tiempo t.Un número creciente de tales colecciones de documentos versionadas está disponible hoy, incluidos los archivos web, el \"entorno de autoría colaborativa\" como wikis o alimentos para la información de tiempo de tiempo."
            ],
            "translated_text": "",
            "candidates": [
                "entorno de autor de colaboración",
                "entorno de autoría colaborativa"
            ],
            "error": []
        },
        "timestamped information feed": {
            "translated_key": "fuente de información con marca de tiempo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or <br>timestamped information feed</br>s.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Dada una consulta de palabras clave, Q y un tiempo en nuestro objetivo es identificar y clasificar los documentos relevantes como si la colección estuviera en su estado a partir del tiempo t.Un número cada vez mayor de tales colecciones de documentos versionadas está disponible hoy, incluidos archivos web, entornos de autoría colaborativa como wikis o \"Feed de información de tiempo de tiempo\" s."
            ],
            "translated_text": "",
            "candidates": [
                "Feed de información con marca de tiempo",
                "Feed de información de tiempo de tiempo"
            ],
            "error": []
        },
        "document-content overlap": {
            "translated_key": "superposición del contenido de documento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the <br>document-content overlap</br> or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Trabajo relacionado Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) Métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice explotando la \"superposición del contenido de documento\"O podando porciones del índice."
            ],
            "translated_text": "",
            "candidates": [
                "superposición de documento contenido",
                "superposición del contenido de documento"
            ],
            "error": []
        },
        "indexing range-based value": {
            "translated_key": "Valor basado en el rango de indexación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for <br>indexing range-based value</br>s and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Burrows y Hisgen [10], en una descripción de la patente, delinean un método para \"indexar el valor basado en el rango\" y mencionan su uso potencial para la búsqueda en función de las fechas asociadas con los documentos."
            ],
            "translated_text": "",
            "candidates": [
                "Valor basado en el rango de indexación",
                "indexar el valor basado en el rango"
            ],
            "error": []
        },
        "open source search-engine nutch": {
            "translated_key": "motor de búsqueda Nutch de código abierto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the <br>open source search-engine nutch</br> to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Stack [29] informa experiencias prácticas hechas al adaptar la \"Nuez de búsqueda de código abierto\" para buscar archivos web."
            ],
            "translated_text": "",
            "candidates": [
                "Search-Engine Nutch de motor abierto de código abierto",
                "Nuez de búsqueda de código abierto"
            ],
            "error": []
        },
        "static indexpruning technique": {
            "translated_key": "Técnica de impulso de índice estático",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "validity time-interval": {
            "translated_key": "intervalo de tiempo de validez",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The <br>validity time-interval</br> val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the <br>validity time-interval</br>.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose <br>validity time-interval</br> does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose <br>validity time-interval</br> spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El \"intervalo de tiempo de validez\" Val (DTI) de una versión dti es [ti, ti+1), si existe una versión más nueva con la marca de tiempo asociada Ti+1, y [ti, ahora) donde ahora apunta a lo más grande posiblevalor de una marca de tiempo (es decir, ∀t: t <ahora).",
                "Las publicaciones en nuestro índice de archivos invertido de viaje en tiempo son, por lo tanto, del formulario (D, P, [TB, TE)) donde D y P se definen como en el índice de archivos invertido estándar anterior y [TB, TE) es la \"validezintervalo de tiempo\".",
                "Para este fin, la lectura secuencial se extiende omitiendo todas las publicaciones cuya \"intervalo de tiempo de validez\" no contiene t (es decir, t ∈ [tb, te)).",
                "Tenga en cuenta que todas las publicaciones cuya \"intervalo de tiempo de validez\" abarca los límites temporales de varios sublistas se replican en cada uno de los sublistas abarcados."
            ],
            "translated_text": "",
            "candidates": [
                "Validez intervalo de tiempo",
                "intervalo de tiempo de validez",
                "Validez intervalo de tiempo",
                "validezintervalo de tiempo",
                "Validez intervalo de tiempo",
                "intervalo de tiempo de validez",
                "Validez intervalo de tiempo",
                "intervalo de tiempo de validez"
            ],
            "error": []
        },
        "sublist materialization": {
            "translated_key": "Materialización sublista",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two <br>sublist materialization</br> techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "<br>sublist materialization</br> Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: <br>sublist materialization</br> it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of <br>sublist materialization</br> using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the <br>sublist materialization</br> techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the <br>sublist materialization</br> improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 <br>sublist materialization</br> We now turn our attention towards evaluating the <br>sublist materialization</br> techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Desarrollamos dos técnicas de \"materialización sublista\" para mejorar el rendimiento del índice que permiten el comercio fuera del espacio frente al rendimiento.4.",
                "La eficiencia de \"materialización sublista\" de procesar una consulta Q t en nuestro índice invertido de viaje en el tiempo está influenciada adversamente por la E/S desperdiciada debido a las publicaciones de lectura pero omitidas.",
                "Por lo tanto, para procesar la consulta q t tiempo T1 T2 T3 T4 T5 T6 T8 T8 T9 T10 D1 D2 D3 Documento 1 2 3 4 5 6 7 8 9 10 Figura 2: \"Materialización sublista\" Es suficiente para escanear cualquier sublista materializado cuyoTimeInterval contiene t.Ilustramos la idea de \"materialización sublista\" utilizando un ejemplo que se muestra en la Figura 2.",
                "Luego, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existieron en esta t.A primera vista, puede parecer contradictorio reducir el tamaño del índice en el primer paso (usando un fase temporal), y luego aumentarlo nuevamente utilizando las técnicas de \"materialización sublista\" presentadas en esta sección.",
                "El uso de la fusión temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la \"materialización sublista\" mejora el rendimiento al replicar juiciosamente las entradas.",
                "En el conjunto de datos UKGOV relativamente menos dinámico (como se puede ver en los valores σ anteriores), los resultados fueron aún mejores, con altos valores de RR y KT observados en todo el espectro de valores para ambos valores de corte.7.3 \"Materialización sublista\" ahora dirigimos nuestra atención hacia la evaluación de las técnicas de \"materialización sublista\" introducidas en la Sección 6."
            ],
            "translated_text": "",
            "candidates": [
                "Materialización sublista",
                "materialización sublista",
                "materialización sublista",
                "materialización sublista",
                "Materialización sublista",
                "Materialización sublista",
                "materialización sublista",
                "materialización sublista",
                "materialización sublista",
                "materialización sublista",
                "materialización sublista",
                "Materialización sublista",
                "Materialización sublista",
                "materialización sublista"
            ],
            "error": []
        },
        "temporal text index": {
            "translated_key": "Índice de texto temporal",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the <br>temporal text index</br> we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Cabe señalar que las técnicas de registro de índice pueden adaptarse para funcionar junto con el \"índice de texto temporal\" que proponemos aquí.3."
            ],
            "translated_text": "",
            "candidates": [
                "Índice de texto temporal",
                "índice de texto temporal"
            ],
            "error": []
        },
        "time-travel text search": {
            "translated_key": "Búsqueda de texto de viaje en el tiempo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for <br>time-travel text search</br> by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address <br>time-travel text search</br> over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "<br>time-travel text search</br>, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to <br>time-travel text search</br> fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to <br>time-travel text search</br> by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable <br>time-travel text search</br>. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended <br>time-travel text search</br> functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired <br>time-travel text search</br> functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for <br>time-travel text search</br>. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for <br>time-travel text search</br> over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described <br>time-travel text search</br> functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Como consecuencia, no existe una solución escalable y de principios para buscar dicha colección a partir de un tiempo especificado t.En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la \"búsqueda de texto de viaje en el tiempo\" extendiendo el índice de archivos invertidos para prepararlo para la búsqueda temporal.",
                "Introducción En este trabajo abordamos la \"búsqueda de texto de viaje en el tiempo\" en colecciones de documentos versionadas temporalmente.",
                "La \"búsqueda de texto de viaje en el tiempo\", como lo desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desarrollar su máximo potencial como lo demuestra el siguiente ejemplo.",
                "Como consecuencia, los enfoques de Na¨ıve para la \"búsqueda de texto de viaje en el tiempo\", y los enfoques viables deben ampliarse bien a tales grandes volúmenes de datos.",
                "Este documento presenta una solución eficiente para la \"búsqueda de texto de viaje en el tiempo\" al hacer las siguientes contribuciones clave: 1.",
                "El popular índice de archivos invertidos bien estudiados [35] se extiende de manera transparente para habilitar la \"búsqueda de texto de viaje en el tiempo\".2.",
                "Sin embargo, esta adaptación no proporciona la funcionalidad prevista de \"Búsqueda de texto de viaje en el tiempo\".",
                "Sin embargo, ninguno de los enfoques considera el tiempo explícitamente o proporciona la funcionalidad deseada de \"Búsqueda de texto de viaje en el tiempo\".",
                "En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivos invertidos que lo preparan para la \"búsqueda de texto de viaje en el tiempo\".4.1 Índice de archivos invertidos Un índice de archivos invertidos consiste en un vocabulario, comúnmente organizado como un b+-tree, que asigna cada término a su IDFScore y una lista invertida.",
                "Conclusiones En este trabajo, hemos desarrollado una solución eficiente para la \"búsqueda de texto de viaje en el tiempo\" sobre colecciones de documentos versionadas temporalmente.",
                "¿Cómo puede la funcionalidad descrita de \"búsqueda de texto de viaje en el tiempo\" habilitar o acelerar la minería de texto a lo largo del eje de tiempo (por ejemplo, rastrear los cambios de sentimiento en las opiniones de los clientes)?9."
            ],
            "translated_text": "",
            "candidates": [
                "Búsqueda de texto de viaje en el tiempo",
                "búsqueda de texto de viaje en el tiempo",
                "Búsqueda de texto de viaje en el tiempo",
                "búsqueda de texto de viaje en el tiempo",
                "Búsqueda de texto de viaje en el tiempo",
                "búsqueda de texto de viaje en el tiempo",
                "Búsqueda de texto de viaje en el tiempo",
                "búsqueda de texto de viaje en el tiempo",
                "Búsqueda de texto de viaje en el tiempo",
                "búsqueda de texto de viaje en el tiempo",
                "Búsqueda de texto de viaje en el tiempo",
                "búsqueda de texto de viaje en el tiempo",
                "Búsqueda de texto de viaje en el tiempo",
                "Búsqueda de texto de viaje en el tiempo",
                "Búsqueda de texto de viaje en el tiempo",
                "Búsqueda de texto de viaje en el tiempo",
                "Búsqueda de texto de viaje en el tiempo",
                "búsqueda de texto de viaje en el tiempo",
                "Búsqueda de texto de viaje en el tiempo",
                "búsqueda de texto de viaje en el tiempo",
                "Búsqueda de texto de viaje en tiempo",
                "búsqueda de texto de viaje en el tiempo"
            ],
            "error": []
        }
    }
}