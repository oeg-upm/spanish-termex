{
    "id": "H-85",
    "original_text": "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance. We present a real-world study of modeling the behavior of web search users to predict web search result preferences. Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks. Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior. We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods. We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone. We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback. General Terms Algorithms, Measurement, Performance, Experimentation. 1. INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general. Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs. However, explicit human ratings are expensive and difficult to obtain. At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results. If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems. Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search. However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks. Therefore, it is not clear whether these techniques will work for general real-world web search. A significant distinction is that web search is not controlled. Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered. But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting. By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting. Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage. Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions. Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings. We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results. Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6). We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2. BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval. The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20]. A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features. Current search engines are commonly tuned on human relevance judgments. Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated. Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research. Several research groups have evaluated the relationship between implicit measures and user interest. In these studies, both reading time and explicit ratings of interest are collected. Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels. Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system. Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems. More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon. Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web. The authors hypothesized correlations between a high degree of page activity and a users interest. While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest. Claypool et al. [6] studied how several implicit measures related to the interests of the user. They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited. Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest. Fox et al. [7] explored the relationship between implicit and explicit measures in Web search. They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions. They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page. Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions. More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence. By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting. A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12]. Unfortunately, the extent to which existing research applies to real-world web search is unclear. In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3. LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences. Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces. Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately. Our general idea is to model the deviations from the expected user behavior. Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information. We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions. A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately. In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments. The data set is described in more detail in Section 5.2. For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs. For these queries we aggregate click data over more than 120,000 searches performed over a three week period. We also have explicit relevance judgments for the top 10 results for each query. Figure 3.1 shows the relative clickthrough frequency as a function of result position. The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p). These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1. The resulting distribution agrees with previous observations that users click more often on top-ranked results. This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches. First we consider the distribution of clicks for the relevant documents for these queries. Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR). While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result. For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries. Nevertheless, many users still click on the non-relevant results in position 1 for such queries. This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant). If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results. Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users. We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior. Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution. We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted. Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features. More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 . In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair. This aggregation gives additional robustness of not relying on individual noisy user interactions. In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values. We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result. Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize. This information was obtained via opt-in client-side instrumentation from users of a major web search engine. This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7]. An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior. Furthermore, we include derived, distributional features computed as described above. The features we use to represent user search interactions are summarized in Table 3.1. For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing. Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary. To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text. These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc. Browsing features: Simple aspects of the user web page interactions can be captured and quantified. These features are used to characterize interactions with pages beyond the results page. For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query. These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries). We include both the direct features and the derived features described above. Clickthrough features: Clicks are a special case of user interaction with the search engine. We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4. For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove). The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1. Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences. We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights. We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence. Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences. The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries. For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair. We can then analyze the user behavior for all the instances where these queries were submitted to the search engine. To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items. More specifically, for each judged query we check if a result link has been judged. If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result. These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set. RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4. PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences. These models range from using no implicit user feedback to using all available implicit user feedback. Ranking search results to predict user preferences is a fundamental problem in information retrieval. Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1). At the same time, user interactions with a search engine provide a wealth of information. A commonly considered type of interaction is user clicks on search results. Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results. We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2. As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3). We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker. For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine. Hence, we will call this system Current for the subsequent discussion. While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality. The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10]. By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies. We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next. Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one. Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking. Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above. For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above. We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper. These strategies are motivated and empirically tested for individual users in a laboratory setting. As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior. The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency. We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model. For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC . Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events. The choice of d selects the tradeoff between recall and precision. While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position. However, for informational queries, multiple results may be clicked, with varying frequency. Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results. We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m. As in CD, the choice of m selects the tradeoff between recall and precision. The pairs may be preferred in the original order or in reverse of it. Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other. Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui. Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior. CDiff and CD are complimentary. CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD. Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions. Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies. As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results. We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3). The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1. Relative user preferences are then estimated using the learned user behavior model described in Section 3.4. Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences. This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond. As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5. EXPERIMENTAL SETUP We now describe our experimental setup. We first describe the methodology used, including our evaluation metrics (Section 5.1). Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results. This allows us to compare to previous work [9,10]. Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9]. The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments. We discuss other applications of our models beyond web search ranking in Section 7. To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels. To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label). In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20]. While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way. Specifically, we report the average query recall and precision. For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted. The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively. A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture. We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine. For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort. In addition for these queries we also had user interaction data for more than 120,000 instances of these queries. The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine. This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31. These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market. In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs). These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings. Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison. We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1). Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page. In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions. This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1). Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation. Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method. To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%. The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets). It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results. Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above. To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6. RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results. Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2). The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively. Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence. In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset. Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08. In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels. Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies. However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence. But first, we consider the best performing strategy, UserBehavior. Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared. Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies. Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking. To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation. Figure 6.2 reports Precision vs. Recall for each feature group. Interestingly, Query-text alone has low accuracy (only marginally better than Random). Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model. Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs. Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively). Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier. For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy. Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data. We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query. Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query. Not surprisingly, CD+CDiff improves with more clicks. This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories. Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff. For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days). Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall. In contrast, the current search engine always makes a prediction for every result for a given query. As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available. Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time. We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625). As expected, Recall of both strategies improves quickly with more days of interaction data examined. We now briefly summarize our experimental results. We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences. Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results). Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N. Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone. Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7. CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting. We showed that our robust models result in higher prediction accuracy than previously published techniques. We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries. Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios. Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions. We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features. By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information. Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies. Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles. For example, the user behavior model on intranet search may be different from the web search behavior. Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings. A natural application of our preference prediction models is to improve web search ranking [1]. In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking. For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels. Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine. While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries. For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries. Hence, clustering queries and learning different predictive models for each query type is a promising research direction. Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models. Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences. As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting. Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods. The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8. REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan. HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents. In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,. In Proceedings of WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda. Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White. Evaluating implicit measures to improve the search experience. In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick. Learning users interests by unobtrusively observing their normal behavior. In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical. Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography. In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl. GroupLens: Applying collaborative filtering to usenet news. In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval. In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim. Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim. Modeling information content using observable behavior. In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web. In Working with Technology in Mind: Brunswikian. Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill. Introduction to modern information retrieval. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001",
    "original_translation": "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferencesde los resultados de búsqueda web es crucial para el desarrollo, la implementación y el mantenimiento del motor de búsqueda. Presentamos un estudio del mundo real sobre el modelado del comportamiento de los usuarios de búsqueda web para predecir las preferencias de los resultados de la búsqueda web. El modelado y la interpretación precisos del comportamiento del usuario tienen aplicaciones importantes para clasificar, hacer clic en la detección de spam, la personalización de la búsqueda web y otras tareas. Nuestra idea clave para mejorar la robustez de la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento del usuario ruidoso esperado. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de la predicción sobre los métodos de clics de última generación. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá de Clickthrough, lo que da como resultado una mayor precisión de predicción de preferencias que los modelos basados solo en la información de clics. Reportamos los resultados de una evaluación experimental a gran escala que muestran mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícitos publicados. Categorías y descriptores de sujetos H.3.3 [Búsqueda y recuperación de información]: proceso de búsqueda, comentarios de relevancia. Algoritmos de términos generales, medición, rendimiento, experimentación.1. Introducción La medición de relevancia es crucial para la búsqueda web y para la recuperación de la información en general. Tradicionalmente, la relevancia de búsqueda se mide mediante el uso de asesores humanos para juzgar la relevancia de los pares de documentos de consulta. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando comentarios valiosos implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación de relevancia automática o implícita se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda web. Sin embargo, la mayoría del trabajo de IR tradicional se realizó sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionadas. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda web no está controlada. Los usuarios individuales pueden comportarse irracionalmente o maliciosamente, o ni siquiera ser usuarios reales;Todo esto afecta los datos que se pueden recopilar. Pero la cantidad de los datos de interacción del usuario es órdenes de magnitud más grandes que cualquier cosa disponible en una configuración de búsqueda no WEB. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual), podemos corregir el ruido inherente a las interacciones individuales y generar juicios de relevancia que son más precisos que las técnicas no diseñadas específicamente para la configuración de búsqueda web. Además, las observaciones y las ideas obtenidas en entornos de laboratorio no necesariamente se traducen en uso del mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación de grandes cantidades de interacciones del usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a las condiciones cambiantes, cambiar los patrones de comportamiento del usuario y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo distributivo de comportamiento del usuario, robusto para el ruido dentro de las sesiones de usuarios individuales, que pueden recuperar las preferencias de relevancia de las interacciones del usuario (Sección 3).• Extensiones de estrategias de clic existentes para incluir características de navegación e interacción más ricas (Sección 4).• Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de última generación publicadas anteriormente, en un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y describimos las direcciones futuras y varias aplicaciones de este trabajo en la Sección 7, lo que concluye el documento.2. Antecedentes y resultados de búsqueda de clasificación de trabajo relacionados es un problema fundamental en la recuperación de la información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta al contenido de la página como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede usar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales se ajustan comúnmente a los juicios de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta de acuerdo con la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de la retroalimentación de relevancia implícita ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre las medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaron los artículos de noticias de Usenet y descubrieron que el tiempo de lectura podría predecir los niveles de interés de los usuarios. Konstan et al.[13] mostraron que el tiempo de lectura era un fuerte predictor de interés de los usuarios en su sistema Grouplens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos de los usuarios observables utilizando dos dimensiones, el propósito subyacente del comportamiento observado y el alcance del elemento que se actúa. Goecks y Shavlik [8] se aproximaron a las etiquetas humanas mediante la recolección de un conjunto de medidas de actividad de la página mientras los usuarios navegaron por la red mundial. Los autores plantearon la hipótesis de correlaciones entre un alto grado de actividad de página y un interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra fue pequeño y las medidas implícitas no se probaron contra juicios explícitos de interés del usuario. Claypool et al.[6] estudiaron cómo varias medidas implícitas se relacionaron con los intereses del usuario. Desarrollaron un navegador personalizado llamado Curious Browser para recopilar datos, en un laboratorio de computación, sobre indicadores de intereses implícitos y para investigar los juicios explícitos de las páginas web visitadas. Claypool et al.descubrió que el tiempo dedicado a una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con un interés explícito, mientras que los métodos de desplazamiento individual y los clics del mouse no estaban correlacionados con un interés explícito. Fox et al.[7] exploró la relación entre medidas implícitas y explícitas en la búsqueda web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios de relevancia explícita tanto para consultas individuales como para las sesiones de búsqueda. Descubrieron que el clic era la variable individual más importante, pero que la precisión predictiva podría mejorarse mediante el uso de variables adicionales, en particular el tiempo de permanencia en una página. Joachims [9] desarrolló información valiosa sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clic para aprender funciones de clasificación. Más recientemente, Joachims et al.[10] presentó una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con calificaciones explícitas, los autores mostraron que es posible interpretar con precisión los eventos de clic en un entorno de laboratorio controlado. Una descripción más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, la medida en que la investigación existente se aplica a la búsqueda web del mundo real no está claro. En este documento, construimos una investigación previa para desarrollar modelos de interpretación de comportamiento del usuario sólidos para la configuración de búsqueda web real.3. Aprender los modelos de comportamiento del usuario, como señalamos anteriormente, el comportamiento real del usuario de la búsqueda web real puede ser ruidoso en el sentido de que los comportamientos del usuario solo están probabilísticamente relacionados con juicios y preferencias de relevancia explícita. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchos rastros de sesión de búsqueda de usuarios poco confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de consulta influenciado por la relevancia de los resultados aparente y un componente de fondo - usuarios que hacen clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos en detalle en la Sección 3.2, calculamos las características derivadas que miden la desviación del valor de características observadas para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic de resultados, analizado a continuación y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2).3.1 Un estudio de caso en distribuciones de clics Como discutimos, agregamos estadísticas en muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró el resumen del resultado prometedor;También podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual del usuario, el clic y de otro modo, es ruidoso y no se puede confiar en juicios de relevancia precisos. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el presente, es suficiente tener en cuenta que nos centramos en una muestra aleatoria de 3.500 consultas que fueron muestreadas aleatoriamente de los registros de consulta. Para estas consultas agregamos datos de clic en más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícita para los 10 mejores resultados para cada consulta. La Figura 3.1 muestra la frecuencia de clic relativo en función de la posición de resultado. La frecuencia de clics agregada en la posición del resultado P se calcula primero calculando la frecuencia de un clic en P para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta aterrice en la posición P). Estas frecuencias se promedian a través de consultas y se normalizan para que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante está de acuerdo con observaciones anteriores en que los usuarios hacen clic más a menudo en los resultados de alto rango. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable de clasificar los resultados, así como los sesgos para hacer clic en los resultados superiores y al ruido, intentamos separar estos componentes en el análisis que sigue.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 1911 21 23 25 27 27 29 Posición de resultado Relativa Primero consideramos la distribución de clics para los documentos relevantes para estas consultas. La Figura 3.2 informa la distribución agregada de clics para consultas con una posición variable del documento relevante (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, hay claramente picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado más relevante en la posición 2, la frecuencia de clics relativo en esa posición (segunda barra) es mayor que la frecuencia de clic en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los mejores resultados: los usuarios hacen clic más a menudo en los resultados que se clasifican más alto, incluso cuando no son relevantes.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 Posición de resultado RelativeclickFrequency PTR = 1 PTR = 2 PTR = 3 PTR = 5 PTR = 10 Figura 3.2: Frecuencia de clic relativo para consultas con PTR variable (posición de TOP de arribaDocumento relevante).-0.06 -0.04 -0.0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 Posición de resultado correcciónPTR (posición de la parte superior relevante). Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante puede interpretarse como el componente de relevancia de los resultados. Tenga en cuenta que la distribución de clics corregido se correlaciona estrechamente con la relevancia del resultado real según lo explícitamente calificado por los jueces humanos.3.2 Modelo de comportamiento de usuario robusto Los clics en los resultados de búsqueda comprenden solo una pequeña fracción de las actividades posteriores a la búsqueda realizadas por los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario posterior a la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados superiores, acabamos de demostrar cómo la distribución de clics basada en relevancia se puede recuperar corriendo para la distribución anterior de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia de la página) están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directo y desviación donde el primero es los valores medidos directamente, y el último es la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características directamente observadas correspondientes. Más formalmente, postulamos que el valor observado O de una característica F para una consulta q y el resultado R puede expresarse como una mezcla de dos componentes :) ,, () () ,, (frqrelfcfrqo += (1) donde) ((FC es la distribución de fondo anterior para los valores de F agregados en todas las consultas, y Rel (Q, R, F) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra anteriormente con la función de clic, si restamosLa distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia de clic observada en una posición determinada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales del usuario en el comportamiento, promediamos los valores de características observados en todos los usuarios y sesiones de búsqueda para cada par de la consulta-URL. Esta agregación brinda robustez adicional de no confiar en las interacciones individuales de los usuarios ruidosos. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye las características observadas directamente y los valores de características derivados y corregidos. Ahora describimos las características reales que usamos para representar el comportamiento del usuario.3.3 Características para representar el comportamiento del usuario Nuestro objetivo es idear un conjunto de características suficientemente rico que nos permitan caracterizar cuando un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegar, refinar su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al.[7]. Una diferencia importante es que muchas de nuestras características son (por diseño) consultas específicas, mientras que la suya era (por diseño) un modelo de comportamiento del usuario general de consulta. Además, incluimos características de distribución derivadas calculadas como se describió anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en el texto de consulta de grupos, el clic y la navegación. Características del texto de consulta: los usuarios deciden qué resultados examinar con más detalle al observar el título de los resultados, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos las características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estas incluyen características tales como superposición entre las palabras en el título y la consulta (titleOverlap), la fracción de palabras compartidas por la consulta y el resumen de resultados (resumenverlap), etc. Características de navegación: los aspectos simples de las interacciones de la página web del usuario se pueden capturar y cuantificar. Estas características se utilizan para caracterizar las interacciones con páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios viven en una página (tiempo de tiempo de tiempo) o dominio (horario de tiempo), y la desviación del tiempo de permanencia del tiempo de permanencia de la página esperado para una consulta. Estas características nos permiten modelar la diversidad intraleria del comportamiento de navegación de páginas (por ejemplo, consultas de navegación, en promedio, es probable que tengan un tiempo de permanencia de página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Haga clic en las características: los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en clics descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consultas-URL proporcionamos el número de clics para el resultado (ClickFrequency), como 1, por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia.así como si hubo un clic en el resultado a continuación o por encima de la URL actual (ISCLICKBELOW, ISCLICKABOVE). Los valores de características derivados, como ClickRelativeFrequency y ClickDeviation, se calculan como se describe en la Ecuación 1. Las características de la consulta-Texto de la fracción del titleOverlap de las palabras compartidas entre la consulta y el título Resumen de la fracción de las palabras compartidas entre la consulta y la fracción de consulta de consulta resumida de las palabras compartidas entre la consulta y la fracción de consulta URL de las palabras compartidas entre la consulta y el dominio Número de la longitudPalabras compartidas con las próximas características de navegación de consultas Tiempo de tiempo Página de permanencia Tiempo de permanencia CumulativeTimeonPage Tiempo acumulativo para todasNormización utilizada, 1 de otra manera ISredirected 1 Si la URL inicial igual a la URL final, 0 de otra manera ispathFromsearch 1 Si solo siguió los enlaces después de la consulta, 0, de lo contrario, el número de lúpulo de la página para llegar a la página desde la consulta promedio, tiempo promedio de tiempo en página para esta consulta desviación de divulgación de la medición de la promedio general promedioTiempo de permanencia en la página Desviación de evaluación acumulada de la página Desde el tiempo acumulativo promedio en la página Desviación de dominio de la página desde el tiempo promedio del tiempo en el dominio ShorturDeviation Desviation Desde el tiempo promedio de la URL de la URL Posición de la posición de la URL en la URL en la clasificación actual Número de clics de clicde un clic para esta consulta y la desviación de la adviación de URL de la frecuencia esperada de clic en la frecuencia de clics isNexted 1 Si hay un clic en la siguiente posición, 0 de lo contrario ispreviousclicked 1 Si hay un clic en la posición anterior, 0 de lo contrario es el clickabove 1 si hay un clic arriba, 0De lo contrario, ISCLICKBELOW 1 Si hay clic a continuación, 0 de lo contrario Tabla 3.1: Características utilizadas para representar interacciones posteriores a la búsqueda para una consulta determinada URL de resultados de búsqueda 3.4 Aprendizaje Un modelo de comportamiento predictivo que ha descrito nuestras características, ahora pasamos al método real de asignaciónLas características de las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita automáticamente en lugar de depender de la heurística o las ideas. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de confiar (solo) en nuestra intuición y evidencia de laboratorio limitada. Nuestro enfoque general es capacitar a un clasificador para inducir pesos para las características del comportamiento del usuario y, en consecuencia, obtener un modelo predictivo de preferencias del usuario. La capacitación se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios de usuarios explícitos para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícita disponibles para cada consulta/resultadopar. Luego podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas se enviaron al motor de búsqueda. Para aprender el mapeo de las características a las preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta juzgada verificamos si se ha juzgado un enlace de resultado. Si es así, la etiqueta se asigna al par Consulta/URL y al vector de características correspondiente para ese resultado de la búsqueda. Estos vectores de los valores de características correspondientes a las URL juzgadas o no relevantes por los anotadores humanos se convierten en nuestro conjunto de capacitación. RankNet ha demostrado un excelente rendimiento al aprender a clasificar objetos en un entorno supervisado, por lo tanto, usamos RankNet para nuestros experimentos.4. Predecir las preferencias del usuario en nuestros experimentos, exploramos varios modelos para predecir las preferencias del usuario. Estos modelos van desde el uso de la retroalimentación implícita del usuario hasta el uso de todos los comentarios de usuarios implícitos disponibles. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de la información. La mayoría de los enfoques tradicionales de búsqueda de IR y web utilizan una combinación de características de página y enlace para rango de resultados de búsqueda, y un sistema de clasificación de estado de estado representativo se utilizará como nuestro ranker de línea de base (Sección 4.1). Al mismo tiempo, las interacciones de usuario con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es que el usuario hace clic en los resultados de búsqueda. El trabajo anterior [9], como se describió anteriormente, también examinó qué resultados se omitieron (por ejemplo, omitir arriba y omitir a continuación) y otras estrategias relacionadas para inducir juicios de preferencias de los usuarios que saltan sobre los resultados y no hacen clic en los siguientes resultados. También hemos agregado refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Como los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia al introducir un modelo de aprendizaje automático que incorpora clics y otros aspectos del comportamiento del usuario, como consultas de seguimiento y tiempo de permanencia de la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea de base, un algoritmo de clasificación de estado de arte utilizado por un motor de búsqueda web operativo.4.1 Modelo de referencia Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para capacitar a un ranker existente. Para nuestro sistema de referencia, utilizamos un sistema de clasificación de página de última generación que actualmente utiliza un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema actual para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados basados en cientos de características, como la consulta para documentar la similitud, la consulta para anclar la similitud de texto y la calidad de la página intrínseca. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y los experimentos de las siguientes dos secciones.4.2 Modelo de clics Si suponemos que cada clic del usuario fue motivado por un proceso racional que seleccionó el resumen de resultados más prometedor, podemos interpretar cada clic como se describe en Joachims et al. [10]. Al estudiar el seguimiento oyario y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que funcionan mejor en sus experimentos, saltamos arriba y saltamos a continuación. Estrategia SA (omitir arriba): para un conjunto de resultados para una consulta y un resultado de clic en la posición P, todos los resultados no reclutados clasificados anteriormente P son menos relevantes que el resultado en p.Además de la información sobre los resultados por encima del resultado hecho, también tenemos información sobre el resultado inmediatamente después del clic. Estudio de seguimiento ocular realizado por Joachims et al.[10] mostraron que los usuarios generalmente consideran el resultado inmediatamente después del resultado hecho en la clasificación actual. Su Skip Next Strategy utiliza esta observación para predecir que un resultado después del resultado hecho en P es menos relevante que el resultado hecho, con una precisión comparable a la estrategia SA anterior. Para una mejor cobertura, combinamos la estrategia de SA con esta extensión para derivar el omitir arriba + omitir la estrategia siguiente: estrategia sa + n (omitir arriba + omitir a continuación): esta estrategia predice todos los resultados no haciendo clic inmediatamente después de un resultado de clics como menos relevanteque el resultado hecho hecho, y combina estas predicciones con las de la estrategia SA anterior. Experimentamos con variaciones de estas estrategias, y descubrimos que SA+N superó a SA y la estrategia original de omitir el próximo, por lo que consideraremos las estrategias SA y SA+N en el resto del documento. Estas estrategias están motivadas y probadas empíricamente para usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en la configuración de búsqueda web real debido a la inconsistencia inherente y la ruidosa del comportamiento de los usuarios individuales. El enfoque general para usar nuestros modelos de clic directamente es filtrar los clics a aquellos que reflejan la frecuencia de clics superior a la oportunidad. Luego usamos las mismas estrategias SA y SA+N, pero solo para clics que tienen una frecuencia más alta de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia Rel (Q, R, F) de la característica de clics observada F como la desviación de la distribución esperada (fondo) de clics) (FC. Estrategia CD (desviación D): para una consulta dada, calcule la distribución de frecuencia de clic observada O (R, P) para todos los resultados r en posiciones p.La desviación de clic para un resultado R en la posición P, dev (R, P) se calcula como:) (), (), (PCPropRdev - = donde C (p) es el clic esperado en la posición p. If dev (r, r,p)> D, retenga la entrada de clics a la estrategia SA+N anterior, y aplique la estrategia SA+N sobre el conjunto filtrado de eventos de clic. La elección de D selecciona la compensación entre el retiro y la precisión. Si bien la estrategia anterior se extiende SA y SA+N, aún asume que se prefiere un resultado hecho (filtrado) sobre todos los resultados no reclutados presentados al usuario por encima de una posición haciendo clic. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con una frecuencia variable. Por lo tanto, es preferible comparar individualmente los resultados para una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de consulta correspondientes. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDIFF (Margen M): Compute Deviation Dev (R, P) para cada resultado R1 ... Rn en la posición p.Para cada par de resultados RI y RJ, predicen la preferencia de RI sobre RJ IFF Dev (Ri, Pi) -dev (RI, PJ)> m. Como en CD, la elección de M selecciona la compensación entre el retiro y la precisión. Los pares pueden preferirse en el orden original o en reversa. Dado el margen, dos resultados pueden ser efectivamente indistinguibles, pero solo uno puede preferirse sobre el otro. Intuitivamente, CDIFF generaliza la idea de omisión anterior para incluir casos en los que el usuario se saltó (es decir, hizo clic menos de lo esperado) en UJ y preferido (es decir, hizo clic más de lo esperado) en la interfaz de usuario. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados en los que se hace clic, lo que lo hace más apropiado para el ruidoso comportamiento del usuario. C Diff y CD son complementarios. C Diff es una generalización del modelo de frecuencia de clic a través de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, la combinación de las dos estrategias para mejorar la cobertura es un enfoque natural: estrategia CD+CDIFF (desviación D, margen M): unión de predicciones de CD y CDIFF. Se consideraron otras variaciones de las estrategias anteriores, pero estos cinco métodos cubren el rango de rendimiento observado.4.3 Modelo general de comportamiento del usuario Las estrategias descritas en la sección anterior generan pedidos basados únicamente en las frecuencias de clic observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones del usuario con los resultados de los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos de comportamiento predictivo de usuario derivado automáticamente (Sección 3). La Estrategia de UserBehavior: para una consulta dada, cada resultado se representa con las características de la Tabla 3.1. Las preferencias relativas del usuario se estiman utilizando el modelo de comportamiento del usuario erudito descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícita como entrada para RankNet, que aprende una ponderación óptima de las características para predecir las preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, lo que le permite beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como lo demuestran nuestros experimentos en las secciones posteriores, el modelado de un conjunto más rico de interacciones de usuario más allá de los clics da como resultado predicciones más precisas de las preferencias del usuario.5. Configuración experimental ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluidas nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3).5.1 Metodología y métricas de evaluación Nuestra evaluación se centra en el acuerdo por pares entre las preferencias para los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones, como las funciones de clasificación de ajuste, la preferencia por pares se puede usar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá de la clasificación de búsqueda web en la Sección 7. Para crear nuestro conjunto de pares de pruebas, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo las preferencias para pares de acuerdo con el orden de las etiquetas de relevancia asociadas. Para evitar la ambigüedad en la evaluación, descartamos todos los lazos (es decir, pares con igual etiqueta). Para calcular la precisión de nuestras predicciones de preferencia con respecto a las preferencias correctas, adaptamos el recuerdo estándar y las medidas de precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se usan de manera similar. Específicamente, informamos el recuerdo promedio de consultas y la precisión. Para nuestra tarea, la precisión de consulta y el recuerdo de consulta para una consulta Q se definen como: • Precisión de consulta: fracción de preferencias predichas para resultados para Q que están de acuerdo con las preferencias obtenidas de un juicio humano explícito.• Recuerdo de consulta: fracción de preferencias obtenidas de un juicio humano explícito para Q que se predijeron correctamente. El retiro general y la precisión se calculan como el promedio de recuperación de consultas y precisión de consulta, respectivamente. Un inconveniente de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, que el acuerdo por pares no captura. Discutimos este problema aún más cuando consideramos las extensiones del trabajo actual en la Sección 7. 5.2 conjuntos de datos para la evaluación utilizamos 3.500 consultas que fueron muestreadas aleatoriamente de los registros de consultas (para un motor de búsqueda web importante. Para cada consulta, los 10 mejores resultados de búsqueda devueltos fueron calificados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo de mejora de relevancia continua. Además para estas consultas, también tuvimos datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones del usuario se cosecharon de trazas anónimas de navegación que siguieron inmediatamente una consulta enviada al motor de búsqueda web. Esta recopilación de datos formó parte de los comentarios voluntarios enviados por los usuarios del 11 al 31 de octubre. Estas tres semanas (21 días) de los datos de interacción del usuario se filtraron para incluir solo a los usuarios en English-U.S.mercado. Para comprender mejor el efecto de la cantidad de datos de interacción del usuario disponibles para una consulta sobre la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: consultas con clasificación humanaCon al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-url) • Q10: consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consultas-url).• Q20: consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos se recopilaron como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a las de datos informados previamente recopilados en entornos de laboratorio. Además, el tamaño de los datos es un orden de magnitud mayor que cualquier estudio reportado en la literatura.5.3 Métodos comparados consideramos una serie de métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícitas publicadas previamente y algunas variantes de estos enfoques (Sección 4.2), y a la clasificación actual del motor de búsqueda en función de la consulta y las características de la página solo (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: la estrategia de clic por encima de la estrategia de clic (Sección 4.2) • SA+N: una extensión más completa de SA que aprovecha mejor la clasificación actual del motor de búsqueda.• CD: nuestro refinamiento de SA+N que aprovecha nuestro modelo de mezcla de distribución de clics para seleccionar clics confiables para la interpretación (Sección 4.2).• CDIFF: Nuestra generalización de la estrategia de CD que utiliza explícitamente el componente de relevancia de las probabilidades de clic para inducir preferencias entre los resultados de búsqueda (Sección 4.2).• CD+CDIFF: la estrategia que combina CD y CDIFF como la unión de preferencias predichas de ambos (Sección 4.2).• UserBehavior: ordenamos predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares, observamos que las puntuaciones de rango más altas indican una mayor confianza en las predicciones. Esta heurística nos permite hacer una elegante compensación de precisión de recuerdo utilizando el puntaje del resultado más alto en el puesto para umbral de las consultas (Sección 4.3) • Actual: Ranking actual del motor de búsqueda (Sección 4.1). Tenga en cuenta que la implementación actual de Ranker se capacitó en un superconjunto de los pares de consulta/URL nominal en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que nosotros para nuestra evaluación. Entrenamiento/división de pruebas: la única estrategia para la cual se requirió dividir los conjuntos de datos en la capacitación y la prueba fue el método UserBehavior. Para evaluar userbehavior, entrenamos y validamos en el 75% de las consultas etiquetadas, y probamos el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición en las consultas entre el entrenamiento y los conjuntos de pruebas). Vale la pena señalar que tanto el ad-hoc sa como el sa+n, así como las estrategias basadas en la distribución (CD, CDIFF y CD+CDIFF), no requieren un conjunto de entrenamiento y prueba separados, ya que se basan enHeurística para detectar frecuencias anómalas de clic para obtener resultados. Por lo tanto, todas las estrategias, excepto UserBehavior, se probaron en el conjunto completo de consultas y las preferencias de relevancia asociadas, mientras que UserBehavior se probó en un subconjunto de retención elegido al azar de las consultas como se describió anteriormente. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de pruebas de retención, lo que resulta en los mismos resultados de precisión que las pruebas sobre los conjuntos de datos completos.6. Resultados Ahora recurrimos a la evaluación experimental de predecir la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de la precisión de recuperación sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N funcionan subóptimamente en este entorno, exhibiendo precisión 0.627 y 0.638 respectivamente. Además, no existe un mecanismo para recordar la compensación de la precisión con SA y SA+N, ya que no proporcionan confianza de predicción. Por el contrario, nuestras técnicas basadas en distribución de clics CD y CD+CDIFF exhiben una precisión algo más alta que SA y SA+N (0.648 y 0.717 en el retiro de 0.08, lo máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.1 0.2 0.2 0.25 0.3 0.35 0.4 0.45 Recuerdos Precisión SA SA+N CD CDIFF CD+CDIFF Corriente de usuarios Figura 6.1: Precisión vs. Recuerdos de SA, SA+N, CD, CDIFF, CD+CDIFF, UserBehavior y los métodos de predicción de relevancia actual en el conjunto de datos Q1. Curiosamente, el CDIFF solo exhibe una precisión igual a SA (0.627) en el mismo recuerdo en 0.08. Por el contrario, al combinar estrategias CD y CDIFF (método CD+CDIFF) logramos el mejor rendimiento de todas las estrategias basadas en el clic, exhibiendo precisión de más de 0.66 para valores de recuperación de hasta 0.14 y más alto en niveles de recuperación más bajos. Claramente, la agregación y la interpretación de inteligencia de los clics resulta en una ganancia significativa para la búsqueda web realista, que las estrategias descritas anteriormente. Sin embargo, incluso la estrategia de interpretación de clics CD+CDIFF se puede mejorar al aprender automáticamente a interpretar la evidencia de clic agregado. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. La incorporación del historial de navegación posterior a la búsqueda además de los clics (características de navegación) da como resultado el mayor retiro y precisión entre todos los métodos comparados. La navegación exhibe precisión de más de 0.7 en el retiro de 0.16, superando significativamente nuestras estrategias de base y solo por clic. Además, Browse puede lograr un alto retiro (tan alto como 0.43) mientras se mantiene la precisión (0.67) significativamente mayor que la clasificación de línea de base. Para analizar más a fondo el valor de diferentes dimensiones de la retroalimentación implícita modelada por la estrategia de usuario de usuario, consideramos cada grupo de características de forma aislada. La Figura 6.2 informa precisión frente a recuperar para cada grupo de características. Curiosamente, el texto de consulta solo tiene baja precisión (solo marginalmente mejor que al azar). Además, las características de navegación por sí solas tienen una precisión más alta (con un mayor retiro de recuerdo) que considerar todas las características en nuestro modelo de Behavior de usuarios. La aplicación de diferentes métodos de aprendizaje automático para combinar predicciones del clasificador puede aumentar el rendimiento del uso de todas las características para todos los valores de recuperación.0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recarga Precisión Todas las características de la consulta de texto de consulta Figura 6.2: Precisión vs. Recuperación para predecir la relevancia con cada grupo de características individualmente.0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20Figura 6.3: Recuerdo vs. Precisión de CD+CDIFF y UserBehavior para conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el ranker entrenado sobre las características de solo clic a punto de hacer clic logra un retiro y precisión sustancialmente más alto que las estrategias de interpretación de clicking diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador capacitado en clic logra 0.67 precisión a 0.42 recordatorio frente al recuerdo máximo de 0.14 logrado por la estrategia CD+CDIFF. Nuestras estrategias de interpretación del comportamiento del usuario y de los usuarios se basan en datos extensos de interacción del usuario. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer un reanimiento de resultados para esa consulta. La Figura 6.3 informa las curvas de precisión de recuerdo para los métodos CD+CDIFF y UserBehavior para diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No es sorprendente que CD+CDIFF mejore con más clics. Esto indica que la precisión mejorará a medida que se disponga de más historias de interacción del usuario, y más consultas del conjunto Q1 tendrán historiales de interacción integrales. Del mismo modo, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDIFF. Para consultas con clics suficientes, CD+CDIFF exhibe una precisión comparable con la navegación en un retiro más bajo.0 0.05 0.1 0.15 0.2 7 12 17 21 21 días de datos de interacción del usuario Recomendado Recuerdo CD+CDIFF21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor clasificación), manteniendo en consecuencia una mayor precisión a expensas del recuerdo. En contraste, el motor de búsqueda actual siempre hace una predicción para cada resultado para una consulta dada. Como consecuencia, el retiro de la corriente es alto (0.627) a expensas de la menor precisión como otra dimensión de adquirir datos de capacitación, consideramos la curva de aprendizaje con respecto a la cantidad (días) de los datos de capacitación disponibles. La Figura 6.4 informa el retiro de CD+CDIFF y las estrategias de UserBehavior para cantidades variables de datos de capacitación recopilados a lo largo del tiempo. Fijamos la precisión mínima para ambas estrategias a 0.7 como un punto sustancialmente más alto que la línea de base (0.625). Como se esperaba, el recuerdo de ambas estrategias mejora rápidamente con más días de datos de interacción examinados. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar inteligentemente los clics de los usuarios en consultas y usuarios, podemos lograr una mayor precisión en la predicción de las preferencias de los usuarios. Debido a la distribución sesgada de los clics del usuario, nuestras estrategias solo por clic tienen una alta precisión, pero un bajo retiro (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clic CD+CDIFF supera a los resultados de última generación recientes por un gran margen (0.72 precisión para CD+CDIFF frente a 0.64 para SA+N) al nivel de recuperación más alto de SA+N. Además, al considerar las características integrales de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que considerando solo hacer clic. Nuestra estrategia de Behavior de usuarios logra el recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho más alta en niveles de recuperación más bajos), supera sustancialmente la clasificación actual de preferencias de los motores de búsqueda y todos los demás métodos de interpretación de retroalimentación implícita.7. CONCLUSIONES Y EL FUTURO OR Our Paper es el primero, que sepamos, a interpretar el comportamiento de los usuarios posteriores a la investigación para estimar las preferencias de los usuarios en una configuración de búsqueda web real. Mostramos que nuestros modelos robustos dan como resultado una mayor precisión de predicción que las técnicas publicadas anteriormente. Introducimos técnicas nuevas, robustas y probabilísticas para interpretar la evidencia de clics al agregar a los usuarios y consultas. Nuestros métodos dan como resultado una interpretación de clics sustancialmente más precisa que los resultados publicados anteriormente no diseñados específicamente para escenarios de búsqueda web. Nuestros métodos predicciones de preferencias de relevancia son sustancialmente más precisas que la clasificación actual de resultados de búsqueda de estado de última generación que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario posterior a la búsqueda que incorpora características de clic, navegación y consulta. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que excede la interpretación de la información limitada de clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario da como resultado un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a las condiciones cambiantes y cambiar los perfiles del usuario. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda web. Nuestro método general de usuario de usuario podría adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales que incluyen detección de spam de clic, detección de abuso de búsqueda, personalización y clasificación específica del dominio. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían capacitarse en ejemplos de abuso de búsqueda o hacer clic en el comportamiento de spam en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían usarse directamente para detectar anomalías en el comportamiento del usuario, ya sea debido al abuso o a los problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no seguir siendo igualmente bien para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, para consultas ambiguas con múltiples significados) pueden dar como resultado un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, la agrupación de consultas y el aprendizaje de diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este documento, el uso de la sabiduría de las multitudes puede darnos una interpretación precisa de las interacciones del usuario incluso en la configuración inherentemente de búsqueda web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda web con una precisión mayor que los métodos publicados anteriormente. Las preferencias de relevancia previstas se pueden utilizar para la evaluación y ajuste de relevancia automática, para implementar la búsqueda en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda web.8. Referencias [1] E. Agichtein, E. Brill y S. Dumais, Mejora de la clasificación de búsqueda web incorporando el comportamiento del usuario, en Actas de la Conferencia de la ACM sobre investigación y desarrollo sobre recuperación de información (Sigir), 2006 [2] J. Allan. Descripción general de la pista dura en TREC 2003: recuperación de alta precisión de documentos. En Proceedings of Trec 2003, 24-37, 2004. [3] S. Brin y L. Page, la anatomía de un motor de búsqueda web hipertextual a gran escala,. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendiendo a clasificarse utilizando la descendencia de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Autor (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing.2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluar las medidas implícitas para mejorar la experiencia de búsqueda. En Transacciones ACM en Sistemas de Información, 2005 [8] J. Goecks y J. Shavlick. Aprender los intereses de los usuarios observando discretamente su comportamiento normal. En Actas del Taller IJCAI sobre aprendizaje automático para el filtrado de información.1999. [9] T. Joachims, Optimización de los motores de búsqueda utilizando datos de clics, en los procedimientos de la conferencia de ACM sobre descubrimiento de conocimientos y datamining (Sigkdd), 2002 [10] T. Joachims, L. granka, B. Pang, H. Hembrookey G. Gay, interpretando con precisión los datos de clics como retroalimentación implícita, en los procedimientos de la Conferencia de ACM sobre investigación y desarrollo sobre recuperación de información (Sigir), 2005 [11] T. Joachims, haciendo práctico el aprendizaje SVM a gran escala. Avances en los métodos del núcleo, en el aprendizaje de vectores de soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Comentarios implícitos para inferir la preferencia del usuario: una bibliografía. En Sigir Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. Grouplens: aplicando filtrado colaborativo a Usenet News. En Communications of ACM, 1997. [14] M. morita e Y. shinoda, filtrado de información basado en el análisis de comportamiento del usuario y la mejor recuperación de texto de coincidencia. En Actas de la Conferencia de ACM sobre investigación y desarrollo sobre recuperación de información (Sigir), 1994 [15] D. Oard y J. Kim. Comentarios implícitos para los sistemas de recomendación.En Actas del Taller AAAAI sobre Sistemas de Recomendación.1998 [16] D. Oard y J. Kim. Modelado de contenido de información utilizando comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencias y Tecnología de la Información.2001 [17] P. Pirolli, El uso del aroma de información proximal para buscar contenido distal en la red mundial. Al trabajar con la tecnología en mente: Brunswikian. Recursos para la ciencia e ingeniería cognitiva, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de consulta: Aprender a clasificarse a partir de comentarios implícitos, en procedimientos de la Conferencia ACM sobre Discovery y Minería de datos (KDD), ACM, ACM, 2005 [19] F. Radlinski y T. Joachims, evaluando la robustez del aprendizaje de los comentarios implícitos, en el taller ICML sobre el aprendizaje en la búsqueda web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Descripción general de TREC, 2001",
    "original_sentences": [
        "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
        "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
        "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
        "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
        "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
        "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
        "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
        "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
        "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
        "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
        "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
        "However, explicit human ratings are expensive and difficult to obtain.",
        "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
        "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
        "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
        "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
        "Therefore, it is not clear whether these techniques will work for general real-world web search.",
        "A significant distinction is that web search is not controlled.",
        "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
        "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
        "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
        "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
        "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
        "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
        "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
        "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
        "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
        "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
        "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
        "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
        "Current search engines are commonly tuned on human relevance judgments.",
        "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
        "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
        "Several research groups have evaluated the relationship between implicit measures and user interest.",
        "In these studies, both reading time and explicit ratings of interest are collected.",
        "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
        "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
        "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
        "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
        "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
        "The authors hypothesized correlations between a high degree of page activity and a users interest.",
        "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
        "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
        "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
        "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
        "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
        "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
        "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
        "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
        "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
        "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
        "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
        "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
        "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
        "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
        "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
        "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
        "Our general idea is to model the deviations from the expected user behavior.",
        "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
        "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
        "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
        "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
        "The data set is described in more detail in Section 5.2.",
        "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
        "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
        "We also have explicit relevance judgments for the top 10 results for each query.",
        "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
        "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
        "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
        "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
        "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
        "First we consider the distribution of clicks for the relevant documents for these queries.",
        "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
        "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
        "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
        "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
        "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
        "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
        "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
        "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
        "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
        "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
        "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
        "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
        "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
        "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
        "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
        "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
        "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
        "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
        "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
        "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
        "Furthermore, we include derived, distributional features computed as described above.",
        "The features we use to represent user search interactions are summarized in Table 3.1.",
        "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
        "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
        "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
        "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
        "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
        "These features are used to characterize interactions with pages beyond the results page.",
        "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
        "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
        "We include both the direct features and the derived features described above.",
        "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
        "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
        "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
        "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
        "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
        "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
        "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
        "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
        "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
        "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
        "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
        "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
        "More specifically, for each judged query we check if a result link has been judged.",
        "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
        "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
        "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
        "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
        "These models range from using no implicit user feedback to using all available implicit user feedback.",
        "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
        "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
        "At the same time, user interactions with a search engine provide a wealth of information.",
        "A commonly considered type of interaction is user clicks on search results.",
        "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
        "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
        "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
        "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
        "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
        "Hence, we will call this system Current for the subsequent discussion.",
        "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
        "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
        "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
        "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
        "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
        "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
        "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
        "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
        "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
        "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
        "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
        "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
        "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
        "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
        "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
        "The choice of d selects the tradeoff between recall and precision.",
        "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
        "However, for informational queries, multiple results may be clicked, with varying frequency.",
        "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
        "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
        "As in CD, the choice of m selects the tradeoff between recall and precision.",
        "The pairs may be preferred in the original order or in reverse of it.",
        "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
        "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
        "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
        "CDiff and CD are complimentary.",
        "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
        "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
        "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
        "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
        "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
        "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
        "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
        "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
        "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
        "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
        "EXPERIMENTAL SETUP We now describe our experimental setup.",
        "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
        "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
        "This allows us to compare to previous work [9,10].",
        "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
        "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
        "We discuss other applications of our models beyond web search ranking in Section 7.",
        "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
        "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
        "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
        "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
        "Specifically, we report the average query recall and precision.",
        "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
        "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
        "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
        "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
        "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
        "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
        "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
        "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
        "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
        "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
        "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
        "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
        "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
        "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
        "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
        "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
        "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
        "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
        "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
        "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
        "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
        "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
        "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
        "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
        "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
        "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
        "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
        "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
        "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
        "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
        "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
        "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
        "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
        "But first, we consider the best performing strategy, UserBehavior.",
        "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
        "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
        "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
        "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
        "Figure 6.2 reports Precision vs. Recall for each feature group.",
        "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
        "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
        "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
        "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
        "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
        "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
        "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
        "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
        "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
        "Not surprisingly, CD+CDiff improves with more clicks.",
        "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
        "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
        "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
        "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
        "In contrast, the current search engine always makes a prediction for every result for a given query.",
        "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
        "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
        "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
        "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
        "We now briefly summarize our experimental results.",
        "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
        "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
        "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
        "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
        "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
        "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
        "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
        "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
        "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
        "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
        "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
        "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
        "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
        "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
        "For example, the user behavior model on intranet search may be different from the web search behavior.",
        "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
        "A natural application of our preference prediction models is to improve web search ranking [1].",
        "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
        "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
        "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
        "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
        "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
        "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
        "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
        "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
        "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
        "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
        "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
        "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
        "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
        "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
        "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
        "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
        "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
        "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
        "Evaluating implicit measures to improve the search experience.",
        "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
        "Learning users interests by unobtrusively observing their normal behavior.",
        "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
        "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
        "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
        "GroupLens: Applying collaborative filtering to usenet news.",
        "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
        "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
        "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
        "Modeling information content using observable behavior.",
        "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
        "In Working with Technology in Mind: Brunswikian.",
        "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
        "Introduction to modern information retrieval.",
        "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
    ],
    "error_count": 0,
    "keys": {
        "relevance measurement": {
            "translated_key": "Medición de relevancia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION <br>relevance measurement</br> is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Introducción \"Medición de relevancia\" es crucial para la búsqueda web y para la recuperación de la información en general."
            ],
            "translated_text": "",
            "candidates": [
                "Medición de relevancia",
                "Medición de relevancia"
            ],
            "error": []
        },
        "implicit feedback": {
            "translated_key": "retroalimentación implícita",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting <br>implicit feedback</br> is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published <br>implicit feedback</br> interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable <br>implicit feedback</br> through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether <br>implicit feedback</br> could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general <br>implicit feedback</br> interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published <br>implicit feedback</br> interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of <br>implicit feedback</br> modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other <br>implicit feedback</br> interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as <br>implicit feedback</br>, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, <br>implicit feedback</br> for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "<br>implicit feedback</br> for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from <br>implicit feedback</br>, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from <br>implicit feedback</br>, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Nuestra idea clave para mejorar la robustez de la interpretación de la \"retroalimentación implícita\" es modelar desviaciones dependientes de la consulta del comportamiento del usuario esperado.",
                "Reportamos los resultados de una evaluación experimental a gran escala que muestran mejoras sustanciales sobre los métodos de interpretación publicados de \"retroalimentación implícita\".",
                "Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos \"comentarios implícitos\" a través de sus interacciones con los resultados de búsqueda.",
                "OARD y KIM [15] estudiaron si la \"retroalimentación implícita\" podría sustituir las calificaciones explícitas en los sistemas de recomendación.",
                "Intentamos aprender una estrategia general de interpretación de \"retroalimentación implícita\" automáticamente en lugar de depender de la heurística o las ideas.",
                "Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de \"retroalimentación implícita\" publicadas previamente y algunas variantes de estos enfoques (Sección 4.2), y a la clasificación actual del motor de búsqueda en función de las características de consulta y página sola (Sección 4.1).",
                "Para analizar más a fondo el valor de diferentes dimensiones de \"retroalimentación implícita\" modeladas por la estrategia de Behavior de usuarios, consideramos cada grupo de características de forma aislada.",
                "Nuestra estrategia de Behavior de usuarios logra el recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho más alta en niveles de recuperación más bajos), supera sustancialmente la clasificación actual de preferencias de motores de búsqueda y todos los demás métodos de interpretación de \"retroalimentación implícita\".7.",
                "En Actas del Taller IJCAI sobre aprendizaje automático para el filtrado de información.1999. [9] T. Joachims, Optimización de los motores de búsqueda utilizando datos de clics, en los procedimientos de la conferencia de ACM sobre descubrimiento de conocimientos y datamining (Sigkdd), 2002 [10] T. Joachims, L. granka, B. Pang, H. Hembrookey G. Gay, interpretando con precisión los datos de clics como \"retroalimentación implícita\", en los procedimientos de la Conferencia de ACM sobre investigación y desarrollo sobre recuperación de información (SIGIR), 2005 [11] T. Joachims, haciendo práctico el aprendizaje SVM a gran escala.",
                "Avances en los métodos del núcleo, en el aprendizaje de vectores de soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, \"Comentarios implícitos\" para inferir la preferencia del usuario: una bibliografía.",
                "\"Comentarios implícitos\" para los sistemas de recomendación.En Actas del Taller AAAAI sobre Sistemas de Recomendación.1998 [16] D. Oard y J. Kim.",
                "Recursos para la ciencia e ingeniería cognitiva, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de consulta: Aprender a clasificarse desde \"retroalimentación implícita\", en los procedimientos de la conferencia de ACM sobre descubrimiento de conocimiento y minería de datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluación de la robustez del aprendizaje de los \"comentarios implícitos\", en el taller ICML sobre el aprendizaje en la búsqueda web, 2005 [20] G. Salton y M. McGill."
            ],
            "translated_text": "",
            "candidates": [
                "retroalimentación implícita",
                "retroalimentación implícita",
                "retroalimentación implícita",
                "retroalimentación implícita",
                "retroalimentación implícita",
                "comentarios implícitos",
                "retroalimentación implícita",
                "retroalimentación implícita",
                "retroalimentación implícita",
                "retroalimentación implícita",
                "retroalimentación implícita",
                "retroalimentación implícita",
                "retroalimentación implícita",
                "retroalimentación implícita",
                "Comentarios implícitos",
                "retroalimentación implícita",
                "retroalimentación implícita",
                "retroalimentación implícita",
                "retroalimentación implícita",
                "Comentarios implícitos",
                "retroalimentación implícita",
                "Comentarios implícitos",
                "retroalimentación implícita",
                "retroalimentación implícita",
                "comentarios implícitos"
            ],
            "error": []
        },
        "information retrieval": {
            "translated_key": "recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to <br>information retrieval</br> in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving <br>information retrieval</br> systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the <br>information retrieval</br> community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in <br>information retrieval</br>.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in <br>information retrieval</br>.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on <br>information retrieval</br> (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on <br>information retrieval</br> (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on <br>information retrieval</br> (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern <br>information retrieval</br>.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Introducción La medición de relevancia es crucial para la búsqueda web y para la \"recuperación de la información\" en general.",
                "Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de \"recuperación de información\".",
                "Recientemente, la retroalimentación de relevancia automática o implícita se ha convertido en un área activa de investigación en la comunidad de \"recuperación de información\", al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda web.",
                "Antecedentes y resultados de búsqueda de clasificación de trabajo relacionados es un problema fundamental en la \"recuperación de la información\".",
                "La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la \"recuperación de la información\".",
                "Referencias [1] E. Agichtein, E. Brill y S. Dumais, Mejora de la clasificación de búsqueda web incorporando el comportamiento del usuario, en las actas de la conferencia de ACM sobre investigación y desarrollo sobre \"recuperación de información\" (Sigir), 2006 [2] J. Allan.",
                "En Actas del Taller IJCAI sobre aprendizaje automático para el filtrado de información.1999. [9] T. Joachims, Optimización de los motores de búsqueda utilizando datos de clics, en los procedimientos de la conferencia de ACM sobre descubrimiento de conocimientos y datamining (Sigkdd), 2002 [10] T. Joachims, L. granka, B. Pang, H. Hembrookey G. Gay, interpretando con precisión los datos de clics como retroalimentación implícita, en los procedimientos de la Conferencia de ACM sobre investigación y desarrollo sobre \"recuperación de información\" (Sigir), 2005 [11] T. Joachims, haciendo práctico el aprendizaje SVM a gran escala.",
                "En Actas de la Conferencia ACM sobre investigación y desarrollo sobre \"recuperación de información\" (Sigir), 1994 [15] D. Oard y J. Kim.",
                "Introducción a la \"recuperación de información\" moderna."
            ],
            "translated_text": "",
            "candidates": [
                "recuperación de información",
                "recuperación de la información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de la información",
                "recuperación de información",
                "recuperación de la información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información"
            ],
            "error": []
        },
        "clickthrough": {
            "translated_key": "clickthrough",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of <br>clickthrough</br> interpretation improves prediction accuracy over state-of-the-art <br>clickthrough</br> methods.",
                "We generalize our approach to model user behavior beyond <br>clickthrough</br>, which results in higher preference prediction accuracy than models based on <br>clickthrough</br> information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing <br>clickthrough</br> strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that <br>clickthrough</br> was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on <br>clickthrough</br> data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting <br>clickthrough</br> evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret <br>clickthrough</br> events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result <br>clickthrough</br>, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, <br>clickthrough</br> and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative <br>clickthrough</br> frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the <br>clickthrough</br> statistics and explicitly modeling post-search user behavior.",
                "Although <br>clickthrough</br> distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the <br>clickthrough</br> feature, if we subtract the background distribution (i.e., the expected <br>clickthrough</br> for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, <br>clickthrough</br>, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "<br>clickthrough</br> features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the <br>clickthrough</br>-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL <br>clickthrough</br> features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 <br>clickthrough</br> Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our <br>clickthrough</br> models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed <br>clickthrough</br> feature f as the deviation from the expected (background) <br>clickthrough</br> distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected <br>clickthrough</br> at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous <br>clickthrough</br> interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the <br>clickthrough</br> frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed <br>clickthrough</br> frequencies.",
                "As we discussed, <br>clickthrough</br> is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above <br>clickthrough</br> strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of <br>clickthrough</br> distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our <br>clickthrough</br> distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all <br>clickthrough</br>-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff <br>clickthrough</br> interpretation strategy can be improved upon by automatically learning to interpret the aggregated <br>clickthrough</br> evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and <br>clickthrough</br>-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features <br>clickthrough</br> Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over <br>clickthrough</br>-only features achieves substantially higher recall and precision than human-designed <br>clickthrough</br>-interpretation strategies described earlier.",
                "For example, the <br>clickthrough</br>-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our <br>clickthrough</br> and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our <br>clickthrough</br>-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff <br>clickthrough</br> strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering <br>clickthrough</br> alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting <br>clickthrough</br> evidence by aggregating across users and queries.",
                "Our methods result in <br>clickthrough</br> interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates <br>clickthrough</br>, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited <br>clickthrough</br> information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc <br>clickthrough</br> interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about <br>clickthrough</br> distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using <br>clickthrough</br> Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting <br>clickthrough</br> Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Mostramos que nuestro modelo de interpretación de \"clickthrough\" mejora la precisión de la predicción sobre los métodos de \"clics\" de última generación.",
                "Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del \"clic\", lo que da como resultado una mayor precisión de predicción de preferencias que los modelos basados solo en información de \"clichrough\".",
                "Nuestras contribuciones incluyen: • Un modelo distributivo de comportamiento del usuario, robusto para el ruido dentro de las sesiones de usuarios individuales, que pueden recuperar las preferencias de relevancia de las interacciones del usuario (Sección 3).• Extensiones de estrategias existentes de \"clics\" para incluir características de navegación e interacción más ricas (Sección 4).• Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de última generación publicadas anteriormente, en un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6).",
                "Descubrieron que \"clichrough\" era la variable individual más importante, pero que la precisión predictiva podría mejorarse mediante el uso de variables adicionales, en particular el tiempo de permanencia en una página.",
                "Joachims [9] desarrolló información valiosa sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de \"clics\" para aprender funciones de clasificación.",
                "Más recientemente, Joachims et al.[10] presentó una evaluación empírica de la interpretación de la evidencia de \"clic\".",
                "Al realizar estudios de seguimiento ocular y correlacionar predicciones de sus estrategias con calificaciones explícitas, los autores mostraron que es posible interpretar con precisión los eventos de \"clics\" en un entorno de laboratorio controlado.",
                "Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el resultado de \"clic\", analizamos a continuación y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2).3.1 Un estudio de caso en distribuciones de clics Como discutimos, agregamos estadísticas en muchas sesiones de usuario.",
                "En general, el comportamiento individual del usuario, \"clic\" y de lo contrario, es ruidoso y no se puede confiar en juicios de relevancia precisos.",
                "La Figura 3.1 muestra la frecuencia relativa de \"clichrough\" en función de la posición de resultado.",
                "Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de \"clics\" y modelar explícitamente el comportamiento del usuario posterior a la búsqueda.",
                "Aunque las distribuciones de \"clichrough\" están fuertemente sesgadas hacia los resultados superiores, acabamos de demostrar cómo la distribución de clics basada en relevancia se puede recuperar corriendo para la distribución de fondo anterior.",
                "",
                "Para mayor claridad, organizamos las características en el texto de consulta de grupos, \"Clickthrough\" y la navegación.",
                "Características de \"Clickthrough\": los clics son un caso especial de interacción del usuario con el motor de búsqueda.",
                "Incluimos todas las características necesarias para aprender las estrategias basadas en \"clics\" descritas en las Secciones 4.1 y 4.4.",
                "Las características de la consulta-Texto de la fracción del titleOverlap de las palabras compartidas entre la consulta y el título Resumen de la fracción de las palabras compartidas entre la consulta y la fracción de consulta de consulta resumida de las palabras compartidas entre la consulta y la fracción de consulta URL de las palabras compartidas entre la consulta y el dominio Número de la longitudPalabras compartidas con las próximas características de navegación de consultas Tiempo de tiempo Página de permanencia Tiempo de permanencia CumulativeTimeonPage Tiempo acumulativo para todasNormización utilizada, 1 de otra manera ISredirected 1 Si la URL inicial igual a la URL final, 0 de otra manera ispathFromsearch 1 Si solo siguió los enlaces después de la consulta, 0, de lo contrario, el número de lúpulo de la página para llegar a la página desde la consulta promedio, tiempo promedio de tiempo en página para esta consulta desviación de divulgación de la medición de la promedio general promedioTiempo de permanencia en la página Desviación de evaluación acumulada de la página Desde el tiempo acumulativo promedio en la página Desviación de dominio de la página desde el tiempo promedio del tiempo en el dominio ShorturDeviation Desviation Desde el tiempo promedio en el tiempo de \"URL corta\" Clickthrough \"Posición de posición de la URL en la clasificación actual NumerFrecuencia relativa de un clic para esta consulta y desviación de cheques de consulta de URL de la frecuencia de clic esperada ISNEXTCLICINE 1 Si hay un clic en la siguiente posición, 0 de lo contrario es Previous Clicked 1 Si hay un clic en la posición anterior, 0 de otra manera es CLICKABOVE 1 si hay un clic arriba, 0 de lo contrario IsClickBelow 1 Si hay clic a continuación, 0 de lo contrario Tabla 3.1: Características utilizadas para representar interacciones posteriores a la búsqueda para una consulta dada y el resultado de la búsqueda URL 3.4 Aprendizaje Un modelo de comportamiento predictivo que ha descrito nuestras características, ahora pasamos al método realde mapeo de las características a las preferencias del usuario.",
                "Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y los experimentos de las siguientes dos secciones.4.2 Modelo de \"clichrough\" Si suponemos que cada clic del usuario fue motivado por un proceso racional que seleccionó el resumen de resultados más prometedor, podemos interpretar cada clic como se describe en Joachims et al. [10].",
                "El enfoque general para usar nuestros modelos \"Clickthrough\" directamente es filtrar los clics a aquellos que reflejan la frecuencia de clics superior a la oportunidad.",
                "Para esto, estimamos el componente de relevancia Rel (Q, R, F) de la característica de \"Clickthrough\" observada como la desviación de la distribución esperada (Clickthrough \"esperada) (FC.",
                "Estrategia CD (desviación D): para una consulta dada, calcule la distribución de frecuencia de clic observada O (R, P) para todos los resultados r en posiciones p.La desviación de clic para un resultado R en la posición P, Dev (R, P) se calcula como:) (), (), (PCPropRdev - = donde C (P) es el \"clic\" esperado en la posición p. If dev (r, p)> d, retenga la entrada de clics en la estrategia SA+N anterior, y aplique la estrategia SA+N sobre el conjunto filtrado de eventos de clic.",
                "Ahora definimos una generalización de la estrategia de interpretación \"Clickthrough\" anterior: Estrategia CDIFF (Margen M): Compute Deviation Dev (R, P) para cada resultado R1 ... Rn en la posición p.Para cada par de resultados RI y RJ, predicen la preferencia de RI sobre RJ IFF Dev (Ri, Pi) -dev (RI, PJ)> m.",
                "CDIFF es una generalización del modelo de frecuencia \"Clickthrough\" de CD, pero ignora la información posicional utilizada en CD.",
                "Se consideraron otras variaciones de las estrategias anteriores, pero estos cinco métodos cubren el rango de rendimiento observado.4.3 Modelo general de comportamiento del usuario Las estrategias descritas en la sección anterior generan pedidos basados únicamente en las frecuencias observadas de \"clics\".",
                "Como discutimos, \"Clickthrough\" es solo un aspecto, aunque importante, de las interacciones del usuario con los resultados del motor de búsqueda web.",
                "Específicamente, comparamos las siguientes estrategias: • SA: la estrategia de \"Clickthrough\" de omitir arriba (Sección 4.2) • SA+N: una extensión más completa de SA que aprovecha mejor la clasificación actual del motor de búsqueda.• CD: nuestro refinamiento de SA+N que aprovecha nuestro modelo de mezcla de distribución de \"clics\" para seleccionar clics confiables para la interpretación (Sección 4.2).• CDIFF: Nuestra generalización de la estrategia de CD que utiliza explícitamente el componente de relevancia de las probabilidades de clic para inducir preferencias entre los resultados de búsqueda (Sección 4.2).• CD+CDIFF: la estrategia que combina CD y CDIFF como la unión de preferencias predichas de ambos (Sección 4.2).• UserBehavior: ordenamos predicciones basadas en la disminución de la puntuación más alta de cualquier página.",
                "Por el contrario, nuestras técnicas basadas en distribución \"Clickthrough\" CD y CD+CDIFF exhiben una precisión algo más alta que SA y SA+N (0.648 y 0.717 en el retiro de 0.08, lo máximo logrado por SA o SA+N).",
                "Por el contrario, al combinar estrategias CD y CDIFF (método CD+CDIFF) logramos el mejor rendimiento de todas las estrategias basadas en \"clic\", que exhiben una precisión de más de 0.66 para valores de recuperación de hasta 0.14 y más alto en niveles de recuperación más bajos.",
                "Sin embargo, incluso la estrategia de interpretación de \"Clickthrough\" CD+CDIFF se puede mejorar al aprender automáticamente a interpretar la evidencia agregada de \"clicshrough\".",
                "La navegación exhibe precisión de más de 0.7 en el recuerdo de 0.16, superando significativamente nuestras estrategias de base y \"clickthrough\".",
                "La aplicación de diferentes métodos de aprendizaje automático para combinar predicciones del clasificador puede aumentar el rendimiento del uso de todas las características para todos los valores de recuperación.0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuerda Precisión Todas las características \"Clickhrough\" Browsing de texto de consulta Figura 6.2: Precisión vs. Recuerdos para predecir la relevancia con cada grupo de características individualmente.0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20Figura 6.3: Recuerdo vs.",
                "Curiosamente, el ranker entrenado sobre las características de \"clima\", solo las características de recuerdo y precisión sustancialmente más altos que las estrategias de interpretación \"Clickthrough\" diseñadas por humanos se describen anteriormente.",
                "Por ejemplo, el clasificador de entrenamiento \"Clickthrough\" logra una precisión de 0.67 a 0.42 recordatorio frente al recuerdo máximo de 0.14 logrado por la estrategia CD+CDIFF.",
                "Nuestras estrategias de interpretación del comportamiento de \"clic\" y comportamiento del usuario se basan en datos extensos de interacción del usuario.",
                "Debido a la distribución sesgada del usuario hace clic en nuestras estrategias \"Clickthrough\", solo que las estrategias solo tienen una alta precisión, pero un bajo retiro (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda).",
                "Sin embargo, nuestra estrategia de \"clickthrough\" de CD+CDIFF supera a los resultados más recientes de última generación por un gran margen (0.72 precisión para CD+CDIFF frente a 0.64 para SA+N) al nivel de recuperación más alto de SA+N.",
                "Además, al considerar las características integrales de UserBehavior que modelan las interacciones de los usuarios después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que considerando \"clic\" solo.",
                "Introdujimos técnicas nuevas, robustas y probabilísticas para interpretar evidencia de \"clic\" al agregar a todos los usuarios y consultas.",
                "Nuestros métodos dan como resultado una interpretación de \"clic\" sustancialmente más precisa que los resultados publicados anteriormente no diseñados específicamente para escenarios de búsqueda web.",
                "También presentamos un modelo general para interpretar el comportamiento del usuario posterior a la búsqueda que incorpora características de \"clic\", navegación y consulta.",
                "Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que excede la interpretación de la información limitada de \"clics\".",
                "Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario da como resultado un rendimiento sustancialmente mejor que las estrategias de interpretación ad-hoc de \"clic\" ad-hoc diseñadas por humanos.",
                "Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de \"clic\" (y el aprendizaje de los modelos de comportamiento del usuario) pueden no seguir siendo igualmente bien para todas las consultas.",
                "En Actas del Taller IJCAI sobre aprendizaje automático para el filtrado de información.1999. [9] T. Joachims, Optimización de los motores de búsqueda utilizando datos de \"clics\", en Actas de la Conferencia ACM sobre Discovery y Datamining (Sigkdd), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, interpretando con precisión los datos de \"clic\" como retroalimentación implícita, en los procedimientos de la conferencia de ACM sobre investigación y desarrollo sobre recuperación de información (SIGIR), 2005 [11] T. Joachims, que hace práctico el aprendizaje de SVM a gran escala."
            ],
            "translated_text": "",
            "candidates": [
                "hacer clic a través",
                "clickthrough",
                "clics",
                "hacer clic a través",
                "clic",
                "clichrough",
                "hacer clic a través",
                "clics",
                "hacer clic a través",
                "clichrough",
                "hacer clic a través",
                "clics",
                "hacer clic a través",
                "clic",
                "hacer clic a través",
                "clics",
                "hacer clic a través",
                "clic",
                "hacer clic a través",
                "clic",
                "hacer clic a través",
                "clichrough",
                "hacer clic a través",
                "clics",
                "hacer clic a través",
                "clichrough",
                "",
                "clic",
                "clic",
                "hacer clic a través",
                "Clickthrough",
                "hacer clic a través",
                "Clickthrough",
                "hacer clic a través",
                "clics",
                "hacer clic a través",
                "URL corta",
                "hacer clic a través",
                "clichrough",
                "hacer clic a través",
                "Clickthrough",
                "Clickthrough",
                "Clickthrough",
                "Haga clic",
                "clic",
                "hacer clic a través",
                "Clickthrough",
                "hacer clic a través",
                "Clickthrough",
                "hacer clic a través",
                "clics",
                "hacer clic a través",
                "Clickthrough",
                "hacer clic a través",
                "Clickthrough",
                "clics",
                "hacer clic a través",
                "Clickthrough",
                "hacer clic a través",
                "clic",
                "hacer clic a través",
                "Clickthrough",
                "clicshrough",
                "hacer clic a través",
                "clickthrough",
                "Haga clic",
                "Clickhrough",
                "hacer clic a través",
                "clima",
                "Clickthrough",
                "hacer clic a través",
                "Clickthrough",
                "hacer clic a través",
                "clic",
                "hacer clic a través",
                "Clickthrough",
                "hacer clic a través",
                "clickthrough",
                "hacer clic a través",
                "clic",
                "hacer clic a través",
                "clic",
                "hacer clic a través",
                "clic",
                "hacer clic a través",
                "clic",
                "hacer clic a través",
                "clics",
                "hacer clic a través",
                "clic",
                "hacer clic a través",
                "clic",
                "hacer clic a través",
                "clics",
                "clic"
            ],
            "error": []
        },
        "position of top relevant document": {
            "translated_key": "Posición del documento relevante superior",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying <br>position of top relevant document</br> (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (<br>position of top relevant document</br>). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La Figura 3.2 informa la distribución agregada de clics para consultas con una \"posición del documento relevante\" (PTR) variable.",
                "Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los mejores resultados: los usuarios hacen clic más a menudo en los resultados que se clasifican más alto, incluso cuando no son relevantes.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 Posición de resultado RelativeclickFrequency PTR = 1 PTR = 2 PTR = 3 PTR = 5 PTR = 10 Figura 3.2: Frecuencia de clic relativo para consultas con PTR variable (\"de\" Posición de deDocumento relevante superior \").-0.06 -0.04 -0.0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 Posición de resultado correcciónPTR (posición de la parte superior relevante)."
            ],
            "translated_text": "",
            "candidates": [
                "Posición del documento relevante superior",
                "posición del documento relevante",
                "Posición del documento relevante superior",
                "de"
            ],
            "error": []
        },
        "top relevant document position": {
            "translated_key": "Posición de documento más relevante relevante",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "induce weight": {
            "translated_key": "inducir peso",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to <br>induce weight</br>s for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Nuestro enfoque general es capacitar a un clasificador para \"inducir peso\" S para las características del comportamiento del usuario y, en consecuencia, obtener un modelo predictivo de preferencias del usuario."
            ],
            "translated_text": "",
            "candidates": [
                "inducir peso",
                "inducir peso"
            ],
            "error": []
        },
        "predictive model": {
            "translated_key": "modelo predictivo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a <br>predictive model</br> of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Nuestro enfoque general es capacitar a un clasificador para inducir pesos para las características del comportamiento del usuario y, en consecuencia, obtener un \"modelo predictivo\" de las preferencias del usuario."
            ],
            "translated_text": "",
            "candidates": [
                "modelo predictivo",
                "modelo predictivo"
            ],
            "error": []
        },
        "user preference": {
            "translated_key": "preferencia de usuario",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring <br>user preference</br>: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Avances en los métodos del núcleo, en el aprendizaje de vectores de soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Comentarios implícitos para inferir \"preferencia del usuario\": una bibliografía."
            ],
            "translated_text": "",
            "candidates": [
                "preferencia de usuario",
                "preferencia del usuario"
            ],
            "error": []
        },
        "page dwell time": {
            "translated_key": "tiempo de permanencia en la página",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., <br>page dwell time</br>) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected <br>page dwell time</br> for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter <br>page dwell time</br> than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage <br>page dwell time</br> CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and <br>page dwell time</br> (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, \"tiempo de permanencia de la página\") están distorsionados de manera similar.",
                "Por ejemplo, calculamos cuánto tiempo los usuarios viven en una página (tiempo de tiempo de tiempo) o dominio (horario de tiempo), y la desviación del tiempo de permanencia del \"tiempo de permanencia\" esperado para una consulta.",
                "Estas características nos permiten modelar la diversidad intraleria del comportamiento de navegación de páginas (por ejemplo, consultas de navegación, en promedio, es probable que tengan un \"tiempo de permanencia de la página\" más corto que las consultas transaccionales o informativas).",
                "Las características de la consulta-Texto de la fracción del titleOverlap de las palabras compartidas entre la consulta y el título Resumen de la fracción de las palabras compartidas entre la consulta y la fracción de consulta de consulta resumida de las palabras compartidas entre la consulta y la fracción de consulta URL de las palabras compartidas entre la consulta y el dominio Número de la longitudPalabras compartidas con las próximas características de navegación de consultas TimeOnpage \"Tiempo de permanencia de la página\" CumulativeTimeOnPage Tiempo acumulativo para todas las páginas posteriores después de la búsqueda Tiempo de espera Tiempo acumulativo Tiempo de permanencia para este Dominio TimeShorturl Tiempo acumulativo en el prefijo de URL, soltar los parámetros ISFollowlink 1 si se le siguió el enlace para el resultado, 0 lo contrario es un tiempo que se expresa 0 0Chmatchmatchmatchmatchmatchmatchmatchmatchmatchmatchmatchmatchmatchmatchmatchmatchmatchmatchmatchmatch Matchmatchmatchmatch Matchmatchmatchmatchmatchmatch Matchmatchmatchmatchmatchmatch Matchmatch, 0Si se usa la normalización agresiva, 1 de lo contrario se redirigió 1 si la URL inicial es igual a la URL final, 0 de lo contrario ISPATHFROMSEARCH 1 Si solo siguió los enlaces después de la consulta, 0 hace clic de lo contrario el número de lúpulos para llegar a la página desde la consulta promedio de tiempo promedio de tiempo en la página para esta consulta Viviation Deviation desde la desviación desde la desviación desdeTiempo de permanencia promedio general en la página Desviación de evaluación acumulada de la página Desde el tiempo acumulativo promedio en la página Desviación de dominio de la página desde el tiempo promedio del tiempo en el dominio ShorturDeviation Desviation Desde el tiempo promedio en el tiempo promedio en la URL corta características de la posición de la URL en la clasificación actual Número de clics de clicFrecuencia relativa de un clic para esta consulta y desviación de cheques de consulta de URL de la frecuencia de clic esperada ISNEXTCLICINE 1 Si hay un clic en la siguiente posición, 0 de lo contrario es Previous Clicked 1 Si hay un clic en la posición anterior, 0 de otra manera es CLICKABOVE 1 si hay un clic arriba, 0 de lo contrario IsClickBelow 1 Si hay clic a continuación, 0 de lo contrario Tabla 3.1: Características utilizadas para representar interacciones posteriores a la búsqueda para una consulta dada y el resultado de la búsqueda URL 3.4 Aprendizaje Un modelo de comportamiento predictivo que ha descrito nuestras características, ahora pasamos al método realde mapeo de las características a las preferencias del usuario.",
                "Como los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia al introducir un modelo de aprendizaje automático que incorpora clics y otros aspectos del comportamiento del usuario, como consultas de seguimiento y \"tiempo de permanencia de la página\" (Sección 4.3)."
            ],
            "translated_text": "",
            "candidates": [
                "Tiempo de permanencia de la página",
                "tiempo de permanencia de la página",
                "Tiempo de permanencia de la página",
                "tiempo de permanencia",
                "Tiempo de permanencia de la página",
                "tiempo de permanencia de la página",
                "Tiempo de permanencia de la página",
                "Tiempo de permanencia de la página",
                "Tiempo de permanencia de la página",
                "tiempo de permanencia de la página"
            ],
            "error": []
        },
        "follow-up queries": {
            "translated_key": "consultas de seguimiento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as <br>follow-up queries</br> and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Como los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia al introducir un modelo de aprendizaje automático que incorpora clics y otros aspectos del comportamiento del usuario, como \"consultas de seguimiento\" y tiempo de permanencia de la página (Sección 4.3)."
            ],
            "translated_text": "",
            "candidates": [
                "consultas de seguimiento",
                "consultas de seguimiento"
            ],
            "error": []
        },
        "explicit relevance judgment": {
            "translated_key": "juicio de relevancia explícita",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and <br>explicit relevance judgment</br>s for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to <br>explicit relevance judgment</br>s and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have <br>explicit relevance judgment</br>s for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any <br>explicit relevance judgment</br>s available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with <br>explicit relevance judgment</br>s as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y \"juicio de relevancia explícita\" para consultas individuales y sesiones de búsqueda.",
                "Aprender los modelos de comportamiento del usuario, como señalamos anteriormente, el comportamiento del usuario de la búsqueda web real puede ser ruidoso en el sentido de que los comportamientos del usuario solo están probabilísticamente relacionados con el \"juicio de relevancia explícita\" y las preferencias.",
                "También tenemos un \"juicio de relevancia explícita\" para los 10 mejores resultados para cada consulta.",
                "Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas y cualquier \"juicio de relevancia explícita\" está disponible para cada unopary/par de resultados.",
                "Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con el \"juicio de relevancia explícita\" como entrada a RankNet que aprende una ponderación óptima de las características para predecir las preferencias."
            ],
            "translated_text": "",
            "candidates": [
                "juicio de relevancia explícita",
                "juicio de relevancia explícita",
                "juicio de relevancia explícita",
                "juicio de relevancia explícita",
                "juicio de relevancia explícita",
                "juicio de relevancia explícita",
                "juicio de relevancia explícita",
                "juicio de relevancia explícita",
                "juicio de relevancia explícita",
                "juicio de relevancia explícita"
            ],
            "error": []
        },
        "predictive behavior model": {
            "translated_key": "modelo de comportamiento predictivo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a <br>predictive behavior model</br> Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a <br>predictive behavior model</br> we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Las características de la consulta-Texto de la fracción del titleOverlap de las palabras compartidas entre la consulta y el título Resumen de la fracción de las palabras compartidas entre la consulta y la fracción de consulta de consulta resumida de las palabras compartidas entre la consulta y la fracción de consulta URL de las palabras compartidas entre la consulta y el dominio Número de la longitudPalabras compartidas con las próximas características de navegación de consultas Tiempo de tiempo Página de permanencia Tiempo de permanencia CumulativeTimeonPage Tiempo acumulativo para todasNormización utilizada, 1 de otra manera ISredirected 1 Si la URL inicial igual a la URL final, 0 de otra manera ispathFromsearch 1 Si solo siguió los enlaces después de la consulta, 0, de lo contrario, el número de lúpulo de la página para llegar a la página desde la consulta promedio, tiempo promedio de tiempo en página para esta consulta desviación de divulgación de la medición de la promedio general promedioTiempo de permanencia en la página Desviación de evaluación acumulada de la página Desde el tiempo acumulativo promedio en la página Desviación de dominio de la página desde el tiempo promedio del tiempo en el dominio ShorturDeviation Desviation Desde el tiempo promedio de la URL de la URL Posición de la posición de la URL en la URL en la clasificación actual Número de clics de clicde un clic para esta consulta y la desviación de la adviación de URL de la frecuencia esperada de clic en la frecuencia de clics isNexted 1 Si hay un clic en la siguiente posición, 0 de lo contrario ispreviousclicked 1 Si hay un clic en la posición anterior, 0 de lo contrario es el clickabove 1 si hay un clic arriba, 0de lo contrario, ISCLICKBELOW 1 Si hay clic a continuación, 0 de lo contrario Tabla 3.1: Características utilizadas para representar interacciones posteriores a la búsqueda para una consulta dada y resultado de la búsqueda URL 3.4 Aprendizaje Un \"modelo de comportamiento predictivo\" Habiendo descrito nuestras características, ahora pasamos al método realde mapeo de las características a las preferencias del usuario.",
                "Recuerde que para aprender un \"modelo de comportamiento predictivo\" utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícita como entrada a RankNet que aprende una ponderación óptima de las características para predecir las preferencias."
            ],
            "translated_text": "",
            "candidates": [
                "modelo de comportamiento predictivo",
                "modelo de comportamiento predictivo",
                "modelo de comportamiento predictivo",
                "modelo de comportamiento predictivo"
            ],
            "error": []
        },
        "recall measure": {
            "translated_key": "Medida de recuerdo",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "precision measure": {
            "translated_key": "medida de precisión",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "low recall": {
            "translated_key": "bajo retiro",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but <br>low recall</br> (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Debido a la distribución sesgada de los clics del usuario, nuestras estrategias solo por clic tienen una alta precisión, pero \"bajo retiro\" (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda)."
            ],
            "translated_text": "",
            "candidates": [
                "bajo retiro",
                "bajo retiro"
            ],
            "error": []
        },
        "web search ranking": {
            "translated_key": "clasificación de búsqueda web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond <br>web search ranking</br> in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve <br>web search ranking</br> [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving <br>web search ranking</br> by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Discutimos otras aplicaciones de nuestros modelos más allá de la \"clasificación de búsqueda web\" en la Sección 7.",
                "Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la \"clasificación de búsqueda web\" [1].",
                "Referencias [1] E. Agichtein, E. Brill y S. Dumais, Mejora de la \"clasificación de búsqueda web\" al incorporar el comportamiento del usuario, en los procedimientos de la Conferencia ACM sobre investigación y desarrollo sobre recuperación de información (SIGIR), 2006 [2] J. Allan."
            ],
            "translated_text": "",
            "candidates": [
                "Ranking de búsqueda web",
                "clasificación de búsqueda web",
                "Ranking de búsqueda web",
                "clasificación de búsqueda web",
                "Ranking de búsqueda web",
                "clasificación de búsqueda web"
            ],
            "error": []
        },
        "click spam detection": {
            "translated_key": "hacer click en la detección de spam",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, <br>click spam detection</br>, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including <br>click spam detection</br>, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El modelado y la interpretación precisos del comportamiento del usuario tienen aplicaciones importantes para la clasificación, \"Haga clic en la detección de spam\", la personalización de la búsqueda web y otras tareas.",
                "Además, nuestro trabajo tiene muchas aplicaciones potenciales que incluyen \"detección de spam\", detección de abuso de búsqueda, personalización y clasificación específica del dominio."
            ],
            "translated_text": "",
            "candidates": [
                "Haga clic en la detección de spam",
                "Haga clic en la detección de spam",
                "Haga clic en la detección de spam",
                "detección de spam"
            ],
            "error": []
        },
        "search abuse detection": {
            "translated_key": "Detección de abuso de búsqueda",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, <br>search abuse detection</br>, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluida la detección de spam de clic, \"detección de abuso de búsqueda\", personalización y clasificación específica de dominio."
            ],
            "translated_text": "",
            "candidates": [
                "Detección de abuso de búsqueda",
                "detección de abuso de búsqueda"
            ],
            "error": []
        },
        "personalization": {
            "translated_key": "personalización",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search <br>personalization</br>, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, <br>personalization</br>, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El modelado y la interpretación precisos del comportamiento del usuario tienen aplicaciones importantes para la clasificación, haga clic en la detección de spam, la \"personalización\" de búsqueda web y otras tareas.",
                "Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluida la detección de spam de clic, la detección de abuso de búsqueda, la \"personalización\" y la clasificación específica del dominio."
            ],
            "translated_text": "",
            "candidates": [
                "personalización",
                "personalización",
                "personalización",
                "personalización"
            ],
            "error": []
        },
        "domain-specific ranking": {
            "translated_key": "clasificación específica del dominio",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and <br>domain-specific ranking</br>.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Además, nuestro trabajo tiene muchas aplicaciones potenciales que incluyen detección de spam de clic, detección de abuso de búsqueda, personalización y \"clasificación específica del dominio\"."
            ],
            "translated_text": "",
            "candidates": [
                "clasificación específica del dominio",
                "clasificación específica del dominio"
            ],
            "error": []
        },
        "interpret implicit relevance feedback": {
            "translated_key": "Interpretar la retroalimentación de relevancia implícita",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "user behavior model": {
            "translated_key": "modelo de comportamiento del usuario",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust <br>user behavior model</br> Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General <br>user behavior model</br> The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned <br>user behavior model</br> described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the <br>user behavior model</br> on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Tenga en cuenta que la distribución de clics corregido se correlaciona estrechamente con la relevancia del resultado real según lo explícitamente calificado por los jueces humanos.3.2 Los clics robustos \"Modelo de comportamiento del usuario\" en los resultados de búsqueda comprenden solo una pequeña fracción de las actividades posteriores a la búsqueda realizadas por los usuarios.",
                "Se consideraron otras variaciones de las estrategias anteriores, pero estos cinco métodos cubren el rango de rendimiento observado.4.3 \"Modelo de comportamiento del usuario\" general Las estrategias descritas en la sección anterior generan pedidos basados únicamente en las frecuencias de clic observadas.",
                "Las preferencias relativas del usuario se estiman utilizando el \"Modelo de comportamiento del usuario\" aprendido descrito en la Sección 3.4.",
                "Por ejemplo, el \"modelo de comportamiento del usuario\" en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda web."
            ],
            "translated_text": "",
            "candidates": [
                "modelo de comportamiento del usuario",
                "Modelo de comportamiento del usuario",
                "modelo de comportamiento del usuario",
                "Modelo de comportamiento del usuario",
                "modelo de comportamiento del usuario",
                "Modelo de comportamiento del usuario",
                "modelo de comportamiento del usuario",
                "modelo de comportamiento del usuario"
            ],
            "error": []
        },
        "predict relevance preference": {
            "translated_key": "Predecir la preferencia de relevancia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically <br>predict relevance preference</br>s for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Nuestras técnicas nos permiten \"predecir la preferencia de relevancia\" automáticamente para los resultados de búsqueda web con una precisión mayor que los métodos publicados anteriormente."
            ],
            "translated_text": "",
            "candidates": [
                "Predecir la preferencia de relevancia",
                "predecir la preferencia de relevancia"
            ],
            "error": []
        }
    }
}