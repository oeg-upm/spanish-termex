{
    "id": "H-21",
    "original_text": "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo! Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine. We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query. Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic. Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported. We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1. INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising. One thing, however, has remained constant: people use very short queries. Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information. Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient. Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience. At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results. For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries. In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes. Given such classifications, one can directly use them to provide better search results as well as more focused ads. The problem of query classification is extremely difficult owing to the brevity of queries. Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it. Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world. For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear. Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge. Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation. To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query. Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query. For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs. We crawl the Web pages pointed by these URLs, and classify these pages. Finally, we use these result-page classifications to classify the original query. Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification. Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline. Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification. This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time. Another important aspect of our work lies in the choice of queries. The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times. While individual queries in this long tail are rare, together they account for a considerable mass of all searches. Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on. However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis. Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters. A natural choice for such aggregation is to classify the queries into a topical taxonomy. Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries. Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial. Early studies in query interpretation focused on query augmentation through external dictionaries [22]. More recent studies [18, 21] also attempted to gather some additional knowledge from the Web. However, these studies had a number of shortcomings, which we overcome in this paper. Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11]. They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18]. The main contributions of this paper are as follows. First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development. The taxonomy used in this work is two orders of magnitude larger than that used in prior studies. The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported. Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable. We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages). We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries. This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2. METHODOLOGY Our methodology has two main phases. In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries. However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified. In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1). Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1. Given a taxonomy of this size, the computational efficiency of classification is a major issue. Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples. Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers. A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields. Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine. The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document. Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q). Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query. We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q). We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query. This is the case for the majority of queries that are unambiguous. Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words. In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds. Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q). The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1). While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query. This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance. Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q. This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q. In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula. This relevance function is an adaptation of the traditional word-based retrieval rules. For example, we may let categories be the words in the vocabulary. We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj. With such choices, the method given by (1) becomes the standard TFIDF retrieval rule. If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). That is, the ads are ranked according to P(q|a). This relevance model has been employed in various statistical language modeling techniques for information retrieval. The intuition can be described as follows. We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj. For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad. It should be mentioned that in our case, each query and ad can have multiple categories. For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj. We use P(Cj|q) to denote the probability of q belonging to category Cj. Here the sum Cj ∈C P(Cj|q) may not equal to one. We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known). Thus, we only need to obtain estimates of P(Cj|q) for each query q. Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q. In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search. That is, top results ranked by search engines should also be ranked high by this formula. Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking. Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)). Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads. Nor does it affect query classification with appropriately chosen thresholds. In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter. The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)). Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale. In the experiment, we will simply take uniform weights wi. A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x. In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q. For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine. Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data. That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear. Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q). The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method. In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3. EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application. Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity. For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node. The ads appropriate for these two queries are, however, very different. To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics. Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9. Figure 1 shows the distribution of categories by taxonomy levels. Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising. Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query. All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency. These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query. A discussion of bidding and placement mechanisms is beyond the scope of this paper [13]. However, many searches do not explicitly use phrases that someone bids on. Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase. In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc. These transformations are based on rules and dictionaries. As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries. Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries). The first set of queries can be matched to at least one ad using broad match as described above. Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them. In a sense, these are even more rare queries and further away from common queries. As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2. The queries in the two sets differ in their classification difficulty. In fact, queries in Set 2 are difficult to interpret even for human evaluators. Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words. Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets. As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words. Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2. Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators. These evaluators were trained editorial staff who possessed knowledge about the taxonomy. The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query. About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation. To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect. We used standard evaluation metrics: precision, recall and F1. In what follows, we plot precision-recall graphs for all the experiments. For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1). Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge. Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach. This baseline classifier is actually a production version of the query classifier running in a major US search engine. In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results. In what follows, we start with the general assessment of the effect of using Web search results. We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs. We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result. For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify. We use top search engine results for collecting background knowledge for queries. We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages. Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy. Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used. Engine Context Prec. F1 Prec. F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge. First, individual results can be classified separately, with subsequent voting among individual classifications. Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier. Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin. However, in the case of summaries, bundling together is found to be consistently better than individual classification. This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results. Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together. The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification. This observation differs from findings by Shen et al. [20], who found summaries to be more useful. We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes. Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings. As can be readily seen, all three variants produce very similar results. However, the precision-recall curve for the 1-class experiment has higher fluctuations. Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth. Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query. Figure 5 and Table 2 present the results of this experiment. In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50). This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise. Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline. We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model. As we have seen, the voting method works quite well. In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine. We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries. The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d). Method B requires a training/testing split. Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods. For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation. The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG. The decaying choice of log2(i + 1) is conventional, which does not have particular importance. The overall DCG of a system is the averaged DCG over queries. We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output. Therefore as a single metric, it is convenient for comparing the methods. Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers. Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3. The oracle method is the best ranking of categories for each query after seeing human judgments. It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance. The simple voting method performs very well in our experiments. The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5). However, both methods are computationally more costly, and the potential gain is minor enough to be neglected. This means that as a simple method, voting is quite effective. We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement. This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results. It may be possible to improve this method by including other page-features that can differentiate top-ranked search results. However, the effectiveness will require further investigation which we did not test. We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries). One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4. RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words. Consequently, many researchers studied possible ways to enhance queries with additional information. One important direction in enhancing queries is through query expansion. This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results. Early work in information retrieval concentrated on manually reviewing the returned results [16, 15]. However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14]. More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation. Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval. Studies in the field pursue different approaches for obtaining additional information about the queries. Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2]. Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global. The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21]. The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier. Several teams used the Web to enrich the queries and provide more context for classification. The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications. The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines. To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier. The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes. A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node. Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification. Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2. This simplifies the process and removes the need for mapping between taxonomies. This also streamlines taxonomy maintenance and development. Using this approach, we were able to achieve good performance in a very large scale taxonomy. We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query. In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies. In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy. For this, an intermediate taxonomy with a training set (ODP) is used. Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy. As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy. While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5. CONCLUSIONS Query classification is an important information retrieval task. Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching. Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy. To address this problem, we proposed a methodology for using search results as a source of external knowledge. To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query. Classifying these results then allows us to classify the original query with substantially higher accuracy. The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification. Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy. Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works. We also experimented with different values of parameters that characterize our method. When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge. Overall, query classification performance was the best when using the full crawled pages (Table 1). These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries. Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results. We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries. In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output. Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results. This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages. We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications. For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency. On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy. Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous. When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole. We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme. Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages. The best results were obtained when using 40 top search hits. In this work, we first classify search results, and then use their classifications directly to classify the original query. Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier. In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages. We plan to further investigate this direction in our future work. It is also essential to note that implementing our methodology incurs little overhead. If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting. To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries. This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web. We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements. In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6. REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz. Automatic web query classification using labeled and unlabeled training data. In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz. Improving automatic query classification via semi-supervised learning. In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart. Pattern Classification and Scene Analysis. John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron. UCLA-Okapi at TREC-2: Query expansion experiments. In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch. Feature generation for text categorization using world knowledge. In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein. Categorizing web queries according to geographical locality. In CIKM03, 2003. [7] E. Han and G. Karypis. Centroid-based document classification: Analysis and experimental results. In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen. IR evaluation methods for retrieving highly relevant documents. In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi. The ferrety algorithm for the KDD Cup 2005 problem. In SIGKDD Explorations, volume 7. ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann. Analyzing the effect of query class on document retrieval performance. In Proc. Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai. KDD CUP-2005 report: Facing a great challenge. In SIGKDD Explorations, volume 7, pages 91-99. ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley. Improving automatic query expansion. In SIGIR98, pages 206-214, 1998. [13] M. Moran and B. Hunt. Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site. Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. In TREC-3, 1995. [15] J. Rocchio. Relevance feedback in information retrieval. In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323. Prentice Hall, 1971. [16] G. Salton and C. Buckley. Improving retrieval performance by relevance feedback. JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy. The Statistical Analysis of Discrete Data. Springer-Verlag, 1989. [18] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin, and Q. Yang. Q2C@UST: Our winning solution to query classification in KDDCUP 2005. In SIGKDD Explorations, volume 7, pages 100-110. ACM, 2005. [19] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin, and Q. Yang. Query enrichment for web-query classification. ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J. Sun, Q. Yang, and Z. Chen. Building bridges for web query classification. In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer. Classifying search engine queries using the web as background knowledge. In SIGKDD Explorations, volume 7. ACM, 2005. [22] E. Voorhees. Query expansion using lexical-semantic relations. In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft. Improving the effectiveness of information retrieval with local context analysis. ACM TOIS, 18(1):79-112, 2000.",
    "original_translation": "Clasificación robusta de consultas raras utilizando el conocimiento web Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo! Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {Broder |Marcusf |GABR |Amrutaj |VANJAJ |tzhang}@yahoo-inc.com Resumen Proponemos una metodología para construir un sistema de clasificación de consultas sólido práctico que pueda identificar miles de clases de consultas con precisión razonable, al tiempo que tratamos en tiempo real con el volumen de consultas de un motor de búsqueda web comercial. Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de búsqueda web recuperados por la consulta. Motivado por las necesidades de la publicidad de búsqueda, nos centramos principalmente en consultas raras, que son los más difíciles desde el punto de vista del aprendizaje automático, pero en la agregación representan una fracción considerable del tráfico de motores de búsqueda. La evaluación empírica confirma que nuestra metodología produce una precisión de clasificación considerablemente mayor que la informada anteriormente. Creemos que la metodología propuesta conducirá a una mejor coincidencia de anuncios en línea con consultas raras y, en general, a una mejor experiencia de usuario. Categorías y descriptores de sujetos H.3.3 [Algorización y recuperación de información de información]: Búsqueda de información y recuperación de retroalimentación de relevancia, proceso de búsqueda de términos generales Algoritmos, medición, rendimiento, experimentación 1. Introducción En su vida útil de 12 años, Web Search había crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo una industria de ocho mil millones de dólares alimentada por la publicidad web. Una cosa, sin embargo, se ha mantenido constante: las personas usan consultas muy cortas. Varios estudios estiman la longitud promedio de una consulta de búsqueda entre 2.4 y 2.7 palabras, que, según todas las cuentas, pueden transportar solo una pequeña cantidad de información. Los motores de búsqueda comercial hacen un trabajo notablemente bueno al interpretar estas cuerdas cortas, pero no son (¡todavía!) Omniscient. Por lo tanto, el uso de un conocimiento externo adicional para aumentar las consultas puede contribuir en gran medida a mejorar los resultados de búsqueda y la experiencia del usuario. Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar la base económica de la búsqueda web, a saber, la publicidad en línea, a través del mecanismo de búsqueda patrocinado que coloca anuncios relevantes junto con los resultados de búsqueda. Por ejemplo, saber que la consulta SD450 se trata de cámaras, mientras que NC4200 trata sobre las computadoras portátiles, obviamente, puede conducir a anuncios más enfocados, incluso si ningún anunciante ha presentado específicamente estas consultas particulares. En este estudio presentamos una metodología para la clasificación de consultas, donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos. Dadas tales clasificaciones, uno puede usarlas directamente para proporcionar mejores resultados de búsqueda, así como anuncios más enfocados. El problema de la clasificación de consultas es extremadamente difícil debido a la brevedad de las consultas. Observe, sin embargo, que en muchos casos un humano que mira una consulta de búsqueda y los resultados de la consulta de búsqueda funcionan notablemente bien para darle sentido. Por supuesto, el gran volumen de consultas de búsqueda no se presta a la supervisión humana y, por lo tanto, necesitamos fuentes alternativas de conocimiento sobre el mundo. Por ejemplo, en el ejemplo anterior, SD450 trae páginas sobre las cámaras canónicas, mientras que NC4200 trae páginas sobre las computadoras portátiles de Compaq, por lo tanto, a un humano, la intención es bastante clara. Los motores de búsqueda indexan cantidades colosales de información, y como tales pueden verse como repositorios de conocimiento muy completos. Después de la heurística descrita anteriormente, proponemos utilizar los resultados de búsqueda en sí mismos para obtener ideas adicionales para la interpretación de la consulta. Con este fin, empleamos el paradigma de retroalimentación de pseudo relevancia y asumimos que los principales resultados de búsqueda son relevantes para la consulta. Ciertamente, no todos los resultados son igualmente relevantes y, por lo tanto, utilizamos esquemas de votación elaborados para obtener un conocimiento confiable sobre la consulta. Para el propósito de este estudio, primero enviamos la consulta dada a un motor de búsqueda web general y recopilamos una serie de las URL de mayor puntuación. Rastizamos las páginas web señaladas por estas URL y clasificamos estas páginas. Finalmente, utilizamos estas clasificaciones de la página de resultados para clasificar la consulta original. Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda web de esta manera produce mejoras sustanciales en la precisión de la clasificación de consultas. Tenga en cuenta que en una implementación práctica de nuestra metodología dentro de un motor de búsqueda comercial, todas las páginas indexadas se pueden clasificar previamente utilizando la tubería normal de procesamiento e indexación de texto. Por lo tanto, en el tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin hacer ningún rastreo o clasificación. Esta sobrecarga adicional es mínima y, por lo tanto, el uso de resultados de búsqueda para mejorar la clasificación de consultas es completamente factible en tiempo de ejecución. Otro aspecto importante de nuestro trabajo radica en la elección de consultas. El volumen de consultas en los motores de búsqueda de hoy sigue la ley de poder familiar, donde algunas consultas aparecen muy a menudo, mientras que la mayoría de las consultas aparecen solo unas pocas veces. Si bien las consultas individuales en esta larga cola son raras, juntas representan una masa considerable de todas las búsquedas. Además, el volumen agregado de tales consultas proporciona una oportunidad sustancial para los ingresos a través de la publicidad en línea.1 Las plataformas de búsqueda y publicidad pueden estar capacitadas para obtener buenos resultados para consultas frecuentes, incluidos datos auxiliares como mapas, atajos a información estructurada relacionada, exitosaanuncios, y así sucesivamente. Sin embargo, las consultas de la cola simplemente no tienen suficientes ocurrencias para permitir el aprendizaje estadístico de manera por cuarta. Por lo tanto, necesitamos agregar tales consultas de alguna manera y razonar a nivel de grupos de consultas agregados. Una elección natural para dicha agregación es clasificar las consultas en una taxonomía tópica. Saber qué nodos de taxonomía son más relevantes para la consulta dada nos ayudará a proporcionar el mismo tipo de soporte para consultas raras que para consultas frecuentes. En consecuencia, en este trabajo nos centramos en la clasificación de consultas raras, cuya clasificación correcta es probable que sea particularmente beneficiosa. Los primeros estudios en la interpretación de la consulta se centraron en el aumento de consultas a través de diccionarios externos [22]. Estudios más recientes [18, 21] también intentaron reunir algunos conocimientos adicionales de la Web. Sin embargo, estos estudios tuvieron una serie de deficiencias, que superamos en este documento. Específicamente, los trabajos anteriores en el campo utilizaron taxonomías de clasificación de consultas muy pequeñas de solo unas pocas docenas de nodos, que no permiten una amplia especificidad para la publicidad en línea [11]. También utilizaron una taxonomía auxiliar separada para documentos web, de modo que se tuviera que emplear un nivel adicional de indirección para establecer la correspondencia entre las taxonomías auxiliares y principales [18]. Las principales contribuciones de este documento son las siguientes. Primero, construimos el clasificador de consulta directamente para la taxonomía objetivo, en lugar de usar una estructura auxiliar secundaria;Esto simplifica enormemente el mantenimiento y el desarrollo de la taxonomía. La taxonomía utilizada en este trabajo es dos órdenes de magnitud más grandes que la utilizada en estudios anteriores. La evaluación empírica demuestra que nuestra metodología para usar el conocimiento externo logra mayores mejoras que las reportadas anteriormente. Dado que nuestra taxonomía es considerablemente mayor, el problema de clasificación que enfrentamos es mucho más difícil, lo que hace que las mejoras que logramos sean particularmente notables. También informamos los resultados de un estudio empírico exhaustivo de diferentes esquemas de votación y diferentes profundidades de conocimiento (por ejemplo, utilizando resúmenes de búsqueda frente a páginas gastadas completas). Descubrimos que rastrear los resultados de búsqueda produce un conocimiento más profundo y conduce a mayores mejoras que los meros resúmenes. Este resultado contrasta con los hallazgos anteriores en la clasificación de consultas [20], pero está respaldado por la investigación en la clasificación de texto convencional [5].2. Metodología Nuestra metodología tiene dos fases principales. En la primera fase, 1 en los ejemplos anteriores, SD450 y NC4200 representan modelos de dispositivos bastante antiguos, y por lo tanto, hay anunciantes que colocan anuncios en estas consultas. Sin embargo, en este documento tratamos principalmente con consultas raras que son extremadamente difíciles de igualar con los anuncios relevantes.Construimos un clasificador de documentos para clasificar los resultados de búsqueda en la misma taxonomía en la que se clasificarán consultas. En la segunda fase, desarrollamos un clasificador de consulta que invoca el clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas.2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizados en un importante motor de búsqueda de EE. UU. (Ver Sección 3.1). Los editores humanos poblaron los nodos de taxonomía con ejemplos etiquetados que utilizamos como instancias de capacitación para aprender un clasificador de documentos en la fase 1. Dada una taxonomía de este tamaño, la eficiencia computacional de la clasificación es un problema importante. Pocos algoritmos de aprendizaje automático pueden manejar de manera eficiente tantas clases diferentes, cada una con cientos de ejemplos de capacitación. Los candidatos adecuados incluyen el vecino más cercano y el clasificador Naive Bayes [3], así como los métodos de formación de prototipos como los clasificadores Rocchio [15] o basados en centroides [7]. Un estudio reciente [5] mostró que los clasificadores basados en centroides son efectivos y eficientes para las taxonomías a gran escala y, en consecuencia, utilizamos un clasificador centralide en este trabajo.2.2 Clasificación de consultas mediante la búsqueda Al haber desarrollado un clasificador de documentos para la taxonomía de la consulta, ahora recurrimos al problema de obtener una clasificación para una consulta dada basada en los resultados de búsqueda iniciales que produce. Supongamos que hay un conjunto de documentos d = d1...DM indexado por un motor de búsqueda. El motor de búsqueda se puede representar por una función f = similitud (q, d) que cuantifica la afinidad entre una consulta q y un documento d.Ejemplos de tales puntajes de afinidad utilizados en este documento son el rango: el rango del documento en la lista ordenada de resultados de búsqueda;puntaje estático: el puntaje de la bondad de la página, independientemente de la consulta (por ejemplo, PageRank);y puntaje dinámico: la cercanía de la consulta y el documento. La clasificación de consultas se determina mediante la evaluación primero de las probabilidades condicionales de todas las clases posibles P (CJ | Q), y luego seleccionando la alternativa con la mayor probabilidad Cmax = Arg Maxcj ∈C P (CJ | Q). Nuestro objetivo es estimar la probabilidad condicional de cada clase posible utilizando los resultados de búsqueda inicialmente devueltos por la consulta. Utilizamos la siguiente fórmula que incorpora clasificaciones de resultados de búsqueda individuales: P (CJ | Q) = D∈D P (CJ | Q, D) · P (D | Q) = D∈D P (Q | CJ, D) P (Q | D) · P (CJ | D) · P (D | Q). Suponemos que P (Q | CJ, D) ≈ P (Q | D), es decir, una probabilidad de una consulta dada un documento se puede determinar sin conocer la clase de la consulta. Este es el caso de la mayoría de las consultas que son inequívocas. Los ejemplos de mostrador son consultas como Jaguar (marca de animales y automóviles) o manzana (fabricante de frutas e informáticos), pero tales consultas ambiguas no pueden clasificarse por definición, y generalmente consiste en palabras comunes. En este trabajo nos concentramos en consultas raras, que tienden a contener palabras raras, ser más largas y coincidir con menos documentos;En consecuencia, en nuestro entorno, esta suposición se mantiene principalmente. Usando esta suposición, podemos escribir P (CJ | Q) = D∈D P (CJ | D) · P (D | Q). La probabilidad condicional de una clasificación para un documento P (CJ | D) se estima utilizando la salida del clasificador de documento (Sección 2.1). Si bien P (D | Q) es más difícil de calcular, consideramos el modelo de relevancia subyacente para clasificar documentos dados una consulta. Este problema se explora más a fondo en la siguiente sección.2.3 Modelo de relevancia basado en la clasificación Para describir una relación formal de clasificación y colocación de anuncios (o búsqueda), consideramos un modelo para usar la clasificación para determinar la relevancia de anuncios (o búsqueda). Que A sea un anuncio y Q sea una consulta, denotamos por r (a, q) la relevancia de a a q. Este número indica cuán relevante es el anuncio A para consultar q, y se puede usar para clasificar los anuncios A para una consulta dada q. En este artículo, consideramos la siguiente aproximación de la función de relevancia: R (A, Q) ≈ RC (A, Q) = CJ ∈C W (CJ) S (CJ, A) S (CJ, Q).(1) El lado derecho expresa cómo usamos el esquema de clasificación C para clasificar los anuncios, donde S (C, a) es una función de puntuación que especifica qué probable A está en la Clase C, y S (C, Q) es Afunción de puntuación que especifica qué probable es Q en la clase C.El valor W (c) es un término de ponderación para la categoría C, lo que indica la importancia de la categoría C en la fórmula de relevancia. Esta función de relevancia es una adaptación de las reglas de recuperación tradicionales basadas en palabras. Por ejemplo, podemos dejar que las categorías sean las palabras en el vocabulario. Tomamos S (CJ, a) como la palabra cuenta de CJ en A, S (CJ, Q) como la palabra cuenta de CJ en Q, y W (CJ) como el término IDF ponderación para la palabra CJ. Con tales opciones, el método dado por (1) se convierte en la regla de recuperación estándar de TFIDF. Si tomamos S (CJ, A) = P (CJ | A), S (CJ, Q) = P (CJ | Q) y W (CJ) = 1/P (CJ), y suponga que Q y Ase generan independientemente dado un concepto oculto C, entonces tenemos rc (a, q) = cj ∈C p (cj | a) p (cj | q)/p (cj) = cj ∈C p (cj | a) p (q| Cj)/p (q) = p (q | a)/p (q). Es decir, los anuncios se clasifican según P (Q | A). Este modelo de relevancia se ha empleado en varias técnicas de modelado de lenguaje estadístico para la recuperación de la información. La intuición se puede describir de la siguiente manera. Suponemos que una persona busca un anuncio A construyendo una consulta Q: La persona primero elige un concepto CJ de acuerdo con los pesos P (CJ | A), y luego construye una consulta Q con probabilidad P (Q | CJ) basada en elconcepto cj. Para este proceso de generación de consultas, los anuncios se pueden clasificar en función de la probabilidad de que se genere la consulta observada a partir de cada anuncio. Debe mencionarse que en nuestro caso, cada consulta y anuncio pueden tener múltiples categorías. Para simplificar, denotamos por CJ una variable aleatoria que indica si Q pertenece a la categoría CJ. Usamos P (CJ | Q) para denotar la probabilidad de Q perteneciente a la categoría CJ. Aquí la suma cj ∈C p (cj | q) puede no igual a uno. Luego consideramos la siguiente fórmula de clasificación: rc (a, q) = cj ∈C p (cj | a) p (cj | q).(2) Asumimos que la estimación de P (CJ | A) se basa en un sistema de categorización de texto existente (que se conoce). Por lo tanto, solo necesitamos obtener estimaciones de P (CJ | Q) para cada consulta q. La ecuación (2) es el modelo de relevancia AD que consideramos en este documento, con parámetros desconocidos P (CJ | Q) para cada consulta q. Para obtener sus estimaciones, utilizamos los resultados de búsqueda de los principales motores de búsqueda de EE. UU., Donde suponemos que la fórmula de clasificación en (2) ofrece una buena clasificación para la búsqueda. Es decir, los resultados principales clasificados por los motores de búsqueda también deben clasificarse altos por esta fórmula. Por lo tanto, dada una consulta Q, y las páginas de resultados K Top K D1 (Q) ,..., DK (Q) De un motor de búsqueda importante, ajustamos los parámetros P (CJ | Q) para que RC (DI (Q), Q) tenga puntajes altos para i = 1 ,..., K. Vale la pena mencionar que utilizando este método solo podemos calcular la fuerza relativa de P (CJ | Q), pero no la escala, porque la escala no afecta la clasificación. Además, es posible que los parámetros estimados puedan ser de la forma G (P (CJ | Q)) para alguna función monótona G (·) de la probabilidad realmente condicional G (P (CJ | Q)). Aunque esto puede cambiar el significado de los parámetros desconocidos que estimamos, no afecta la calidad del uso de la fórmula para clasificar los anuncios. Tampoco afecta la clasificación de consultas con umbrales elegidos adecuadamente. En lo que sigue, consideramos dos métodos para calcular la información de clasificación P (CJ | Q).2.4 El método de votación nos gustaría calcular P (CJ | Q) para que RC (DI (Q), Q) sean altos para i = 1 ,..., K y RC (D, Q) son bajos para un documento aleatorio d.Suponga que el vector [P (CJ | D)] CJ ∈C es aleatorio para un documento promedio, entonces la condición de que CJ ∈C P (CJ | Q) 2 es pequeño implica que RC (D, Q) también es pequeño promediadod.Por lo tanto, un método natural es maximizar k i = 1 wirc (di (q), q) sujeto a cj ∈C p (cj | q) 2 es pequeño, donde wi son pesos asociados con cada rango I: max [p (·| q)]   1 k k i = 1 wi cj ∈C p (cj | di (q)) p (cj | q) - λ cj ∈C p (cj | q) 2 , donde asumimos k i = 1Wi = 1, y λ> 0 es un parámetro de regularización de ajuste. La solución óptima es p (cj | q) = 1 2λ k i = 1 wip (cj | di (q)). Dado que tanto p (cj | di (q)) como p (cj | q) pertenecen a [0, 1], podemos tomar λ = 0.5 para alinear la escala. En el experimento, simplemente tomaremos pesos uniformes WI. Una estrategia más compleja es dejar que W también dependa de D: P (CJ | Q) = D W (D, Q) G (P (CJ | D)), donde G (x) es una cierta transformación de x. En esta formulación general, W (D, Q) puede depender de factores distintos del rango de D en los resultados del motor de búsqueda para q. Por ejemplo, puede ser una función de R (D, Q) donde R (D, Q) es el puntaje de relevancia devuelto por el motor de búsqueda subyacente. Además, si se nos da un conjunto de pares de categoría/consulta de entrenamiento marcado a mano (C, Q), tanto los pesos W (D, Q) como la transformación G (·) se pueden aprender utilizando técnicas de clasificación estándar.2.5 Clasificación discriminativa Podemos tratar el problema de estimar P (CJ | Q) como un problema de clasificación, donde para cada Q, etiquetamos Di (Q) para i = 1 ,..., K como datos positivos, y los documentos restantes como datos negativos. Es decir, asignamos la etiqueta yi (q) = 1 para di (q) cuando i ≤ k, y etiqueta yi (q) = −1 para di (q) cuando i> k. en esta configuración, la regla de puntuación de clasificación paraUn documento di (q) es lineal. Sea xi (q) = [p (cj | di (q))], y w = [p (cj | q)], luego cj ∈C p (cj | q) p (cj | di (q)) = w· Xi (Q). Los valores P (CJ | D) son las características para el clasificador lineal, y [P (CJ | D)] es el vector de peso, que se puede calcular utilizando cualquier método de clasificación lineal. En este artículo, consideramos estimar W usando regresión logística [17] de la siguiente manera: P (· | Q) = arg minw i ln (1 + e - w · xi (q) yi (q))).0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 NumberOfcategories Taxonomy Nivel Figura 1: Número de categorías por nivel 3. Evaluación En esta sección, evaluamos nuestra metodología que utiliza resultados de búsqueda web para mejorar la clasificación de consultas.3.1 Taxonomía Nuestra elección de taxonomía fue guiada por una aplicación de publicidad web. Dado que queremos que las clases sean útiles para hacer coincidir anuncios con consultas, la taxonomía debe ser lo suficientemente elaborada como para facilitar una amplia especificidad de clasificación. Por ejemplo, clasificar todas las consultas médicas en un nodo probablemente dará como resultado una mala coincidencia de anuncios, ya que tanto las consultas doloridas como las de gripe terminarán en el mismo nodo. Sin embargo, los anuncios apropiados para estas dos consultas son muy diferentes. Para evitar tales situaciones, la taxonomía debe proporcionar una discriminación suficiente entre los temas comerciales comunes. Por lo tanto, en este documento empleamos una taxonomía elaborada de aproximadamente 6000 nodos, dispuestos en una jerarquía con profundidad media 5 y profundidad máxima 9. La Figura 1 muestra la distribución de categorías por niveles de taxonomía. Los editores humanos poblaron la taxonomía con consultas etiquetadas (aproximadamente 150 consultas por nodo), que se utilizaron como un conjunto de capacitación;Se ha asignado una pequeña fracción de consultas a más de una categoría.3.2 Digresión: los conceptos básicos de la búsqueda patrocinada para discutir nuestro conjunto de consultas de evaluación, necesitamos una breve introducción a algunos conceptos básicos de publicidad web. La publicidad patrocinada de búsqueda (o búsqueda pagada) está colocando anuncios textuales en las páginas de resultados de los motores de búsqueda web, con anuncios impulsados por la consulta de origen. Todos los principales motores de búsqueda (Google, Yahoo!, Y MSN) admiten tales anuncios y actúan simultáneamente como un motor de búsqueda y una agencia de publicidad. Estos anuncios textuales se caracterizan por una o más frases de oferta que representan esas consultas donde los anunciantes desean que se muestre su anuncio.(La frase de oferta de nombre proviene del hecho de que los anunciantes ofrecen varias cantidades para asegurar su posición en la torre de anuncios asociados a una consulta. Una discusión sobre los mecanismos de licitación y colocación está más allá del alcance de este documento [13]. Sin embargo, muchas búsquedas no utilizan explícitamente frases en las que alguien ofrece. En consecuencia, los anunciantes también compran coincidencias amplias, es decir, pagan para colocar sus anuncios en consultas que constituyen alguna modificación de la frase de oferta deseada. En Broad Match, se pueden aplicar varias modificaciones sintácticas a la consulta para que coincida con la frase de oferta, por ejemplo, que cae o agregue palabras, sustitución de sinónimos, etc. Estas transformaciones se basan en reglas y diccionarios. A medida que los anunciantes tienden a cubrir consultas de alto volumen y de alto ingreso, las consultas de amplio partido caen en la cola de la distribución con respecto tanto al volumen como a los ingresos.3.3 Conjuntos de datos utilizamos dos conjuntos representativos de 1000 consultas. Ambos conjuntos contienen consultas que no se pueden igualar directamente con los anuncios, es decir, ninguna de las consultas contiene una frase de oferta (esto significa que eliminamos prácticamente todas las consultas populares). El primer conjunto de consultas se puede coincidir con al menos un anuncio usando una coincidencia amplia como se describió anteriormente. Las consultas en el segundo set no pueden coincidir incluso con una coincidencia amplia y, por lo tanto, el motor de búsqueda utilizado en nuestro estudio actualmente no muestra ninguna publicidad para ellos. En cierto sentido, estas son consultas aún más raras y más lejos de consultas comunes. Como medida de rareza de consulta, estimamos su frecuencia en un mes de registros de consultas para un importante motor de búsqueda de EE. UU.;La frecuencia media fue 1 para consultas en el conjunto 1 y 0 para consultas en el conjunto 2. Las consultas en los dos conjuntos difieren en su dificultad de clasificación. De hecho, las consultas en el Set 2 son difíciles de interpretar incluso para los evaluadores humanos. Las consultas en el conjunto 1 tienen en promedio 3.50 palabras, con la más larga que tiene 11 palabras;Las consultas en el conjunto 2 tienen en promedio 4.39 palabras, con la consulta más larga de 81 palabras. Estudios recientes estiman que la duración promedio de las consultas web es de poco menos de 3 palabras2, que es más baja que en nuestros conjuntos de pruebas. Como otra medida de dificultad de consulta, medimos la fracción de consultas que contienen comillas, ya que esta última ayuda a la interpretación de la consulta al agrupar significativamente las palabras. Solo el 8% consultas en el conjunto 1 y 14% en el conjunto 2 contenían comillas.3.4 Metodología y métricas de evaluación Los dos conjuntos de consultas se clasificaron en la taxonomía objetivo utilizando las técnicas presentadas en la Sección 2. Según los valores de confianza asignados, las 3 clases principales para cada consulta se presentaron a los evaluadores humanos. Estos evaluadores fueron personal editorial capacitado que poseía conocimiento sobre la taxonomía. Los editores consideraron todos los pares de consulta y los calificaron en la escala 1 a 4, con 1, lo que significa que la clasificación es muy relevante y 4 significa que es irrelevante para la consulta. Alrededor de 2.4% consultas en el conjunto 1 y 5.4% consultas en el conjunto 2 se consideraron que no era clasificable (por ejemplo, cadenas aleatorias de caracteres), y en consecuencia se excluyeron de la evaluación. Para calcular las métricas de evaluación, tratamos las clasificaciones con las calificaciones 1 y 2 para ser correctas, y aquellos con calificaciones 3 y 4 para ser incorrectas. Utilizamos métricas de evaluación estándar: precisión, recuperación y F1. En lo que sigue, trazamos gráficos de precisión de precisión para todos los experimentos. Para comparación con otros estudios publicados, también informamos valores de precisión y F1 correspondientes al recuerdo completo (r = 1). Debido a la falta de espacio, solo mostramos gráficos para el conjunto de consultas 1;Sin embargo, mostramos los resultados numéricos para ambos conjuntos en las tablas.3.5 Resultados Comparamos nuestro método con un clasificador de consulta de referencia que no utiliza ningún conocimiento externo. Nuestro clasificador de línea de base amplió consultas utilizando técnicas de expansión de consultas estándar, agrupó sus términos utilizando un reconocimiento de frases, aumentó ciertas frases en la consulta en función de sus propiedades estadísticas y realizó clasificación utilizando los 2 http://www.rankstat.com/html/en/Seo-News1-Mother-Peopleuse-2-Word-Phrases-In-Search-Engines.html 0.4 0.5 0.6 0.7 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Motor de base de recuperación de precisión Un motor de página completa A A un motor de página completa A A A un motor A de página completa.Resumen Motor B Motor de página completa B Resumen Figura 2: El efecto del conocimiento externo más cercano-vecino. Este clasificador de línea de base es en realidad una versión de producción del clasificador de consultas que se ejecuta en un importante motor de búsqueda de EE. UU. En nuestros experimentos, variamos valores de parámetros pertinentes que caracterizan la forma exacta de usar los resultados de búsqueda. En lo que sigue, comenzamos con la evaluación general del efecto del uso de resultados de búsqueda web. Luego procedemos a explorar técnicas más refinadas, como usar solo resúmenes de búsqueda en lugar de rastrear las URL devueltas. También experimentamos con el uso de diferentes números de resultados de búsqueda por consulta, así como con variando el número de clasificaciones consideradas para cada resultado de la búsqueda. Por falta de espacio, solo mostramos gráficos para consultas del conjunto 1 y omitimos los gráficos para consultas del conjunto 2, que exhiben fenómenos similares.3.5.1 El efecto de las consultas de conocimiento externo por sí mismas es muy corto y difícil de clasificar. Utilizamos los mejores resultados de los motores de búsqueda para recopilar conocimiento de fondo para consultas. Empleamos dos principales motores de búsqueda de EE. UU., Y utilizamos sus resultados de dos maneras, ya sea solo resúmenes o el texto completo de las páginas de resultados rastreados. La Figura 2 y la Tabla 1 muestran que dicho conocimiento adicional mejora considerablemente la precisión de la clasificación. Curiosamente, descubrimos que el motor de búsqueda A funciona constantemente mejor con el texto de la página completa, mientras que el motor de búsqueda B funciona mejor cuando se utilizan resúmenes. Contexto del motor prec. F1 prec. F1 establecido 1 conjunto 1 conjunto 2 conjunto 2 una página completa 0.72 0.84 0.509 0.721 b Página completa 0.706 0.827 0.497 0.665 un resumen 0.586 0.744 0.396 0.572 b Resumen 0.645 0.788 0.467 0.638 base 0.534 0.696 0.365 0.536 Tabla 1: el efecto de la tasa 1: el efecto de usar el efecto externo de la línea de base de usar 0.534 0.365 0.536.Conocimiento 3.5.2 Técnicas de agregación Hay dos formas principales de utilizar los resultados de búsqueda como conocimiento adicional. Primero, los resultados individuales se pueden clasificar por separado, con la votación posterior entre clasificaciones individuales. Alternativamente, los resultados de búsqueda individuales se pueden agrupar como un metadocumento y clasificarse como tal utilizando el clasificador de documento. La Figura 3 presenta los resultados de estos dos enfoques cuando se utilizan páginas de texto completo, la técnica que utiliza clasificaciones individuales de resultados de búsqueda, evidentemente, supera el enfoque de agrupación por un amplio margen. Sin embargo, en el caso de los resúmenes, se encuentra que agruparse es consistentemente mejor que la clasificación individual. Esto se debe a que los resúmenes por sí mismos son demasiado cortos para clasificarse correctamente individualmente, pero cuando se agrupan, son mucho más estables.0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Recuerdos de precisión Línea de base Bundled Votación de página completa Página completa Resumen de votación Bundled Votación Resumen 3: Votación versus Bundling 3.5.3 Texto de página completa Vs. Resumen a Summary toResumir las dos secciones anteriores, el conocimiento de los antecedentes para cada consulta se obtiene utilizando el texto de la página completa o solo los resúmenes de los resultados de búsqueda principales. Se encontró que el texto de la página completa era más en conjunto con la clasificación votada, mientras que los resúmenes eran útiles cuando se agruparon juntos. Los mejores resultados en general se obtuvieron con resultados de página completa clasificadas individualmente, con la votación posterior utilizada para determinar la clasificación final de consulta. Esta observación difiere de los hallazgos de Shen et al.[20], quien encontró que los resúmenes son más útiles. Atribuimos esta distinción al hecho de que las consultas que utilizamos en este estudio son las colas, que son raras y difíciles de clasificar.3.5.4 Variando el número de clases por resultado de búsqueda también variamos el número de clasificaciones por resultado de la búsqueda, es decir, cada resultado se permitió tener clases 1, 3 o 5. La Figura 4 muestra los gráficos de recuperación de precisión correspondientes tanto para la página completa como para la configuración de solo suma. Como se puede ver fácilmente, las tres variantes producen resultados muy similares. Sin embargo, la curva de recuperación de precisión para el experimento de 1 clase tiene fluctuaciones más altas. El uso de 3 clases por resultado de búsqueda produce una curva más estable, mientras que con 5 clases por resultado, la curva de recolección de precisión es muy suave. Por lo tanto, a medida que aumentamos el número de clases por resultado, observamos una mayor estabilidad en la clasificación de consultas.3.5.5 Variando el número de resultados de búsqueda obtenidos también experimentamos con diferentes números de resultados de búsqueda por consulta. La Figura 5 y la Tabla 2 presentan los resultados de este experimento. En línea con nuestra intuición, observamos que la precisión de clasificación aumenta constantemente a medida que aumentamos el número de resultados de búsqueda utilizados de 10 a 40, con una ligera caída a medida que continuamos utilizando aún más resultados (50). Esto se debe a que el uso de muy pocos resultados de búsqueda proporciona muy poco conocimiento externo, mientras que el uso de demasiados resultados introduce ruido adicional. Usando la prueba t emparejada, evaluamos la significación estadística 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Recuerda de precisión Línea base 1 Clase de clase completa 3 Clases Full Page 5 Clases Full-Page 1 Class Summary de clase 1 Class Summary3 Clases Resumen 5 Clases Resumen Figura 4: Variando el número de clases por página 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 RECUERDO DE PRECISIÓN 10 20 30 40 50 Base Figura 5: Varicando el número de resultados de resultados.por consulta de las mejoras debido a nuestra metodología versus la línea de base. Encontramos que los resultados son muy significativos (p <0.0005), confirmando así el valor del conocimiento externo para la clasificación de consultas.3.6 Votación versus métodos alternativos Como se explica en la Sección 2.2, uno puede usar varios métodos para clasificar las consultas de los resultados del motor de búsqueda en función de nuestro modelo de relevancia. Como hemos visto, el método de votación funciona bastante bien. En esta sección, comparamos el rendimiento de los resultados de búsqueda de los diez mejores con los siguientes dos métodos: • A: Aprendizaje discriminativo de la clasificación de consulta basada en la regresión logística, descrita en la Sección 2.5.• B: Pesos de aprendizaje basados en la puntuación de calidad devuelta por un motor de búsqueda. Discretizamos el puntaje de calidad S (D, Q) de un par de consultas/documentos en {alto, mediano, bajo}, y aprendemos los tres pesos W en un conjunto de consultas de entrenamiento y probamos las consultas de rendimiento en retención. La fórmula de clasificación, como se explica al final de la Sección 2.4, es P (CJ | Q) = D W (S (D, Q)) P (CJ | D). El método B requiere una división de entrenamiento/prueba. Ni la votación ni el método A requieren tal división;Sin embargo, para consistencia, dibujamos aleatoriamente las divisiones de entrenamiento/prueba de 50-50 durante diez veces e informamos el rendimiento medio ± desviación estándar en la división de prueba para los tres métodos. Para este experimento, en lugar de precisión y retiro, utilizamos DCG-K (k = 1, 5), popular en la evaluación del motor de búsqueda. La métrica DCG (ganancia acumulada con descuento), descrita en [8], es una medida de clasificación donde se le pide al sistema que clasifique un conjunto de candidatos (en número de resultados de precisión F1 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 400.807 0.893 50 0.798 0.887 Tabla 2: variando el número de resultados de búsqueda en nuestro caso, categorías juzgadas para cada consulta) y calcula para cada consulta Q: DCGK (Q) = K i = 1 g (CI (Q))/ Log2 (I+ 1), donde CI (Q) es la categoría I-Th para la consulta Q clasificada por el sistema, y G (CI) es el grado de CI: asignamos el grado de 10, 5, 1, 0 al 4 puntosEscala de juicio descrita anteriormente para calcular DCG. La elección en descomposición de LOG2 (i + 1) es convencional, que no tiene particular importancia. El DCG general de un sistema es el DCG promedio sobre consultas. Utilizamos esta métrica en lugar de precisión/retiro en este experimento porque puede manejar directamente la salida de múltiples accesorios. Por lo tanto, como una sola métrica, es conveniente para comparar los métodos. Tenga en cuenta que las curvas de precisión/retiro utilizadas en las secciones anteriores producen algunas ideas adicionales que no son aparentes inmediatamente de los números de DCG. Conjunto 1 Método DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Votación 5.28 ± 0.15 11.80 ± 0.31 Método A 5.48 ± 0.16 12.22 ± 0.34 Método B 5.36 ± 0.18 12.15 ± 0.35 Conjunto 2 Método DCG-1 DCG-5 ORACREG.± 0.18 9.94 ± 0.32 Votación 3.50 ± 0.17 7.80 ± 0.28 Método A 3.63 ± 0.23 8.11 ± 0.33 Método B 3.55 ± 0.18 7.99 ± 0.31 Tabla 3: Votación y métodos alternativos Los resultados de nuestros experimentos se dan en la Tabla 3. El método Oracle es la mejor clasificación de categorías para cada consulta después de ver juicios humanos. No se puede lograr mediante ningún algoritmo realista, pero se incluye aquí como un límite superior absoluto en el rendimiento de DCG. El método de votación simple funciona muy bien en nuestros experimentos. Los métodos más complicados pueden conducir a una ganancia de rendimiento moderada (especialmente el método A, que utiliza capacitación discriminativa en la Sección 2.5). Sin embargo, ambos métodos son computacionalmente más costosos, y la ganancia potencial es lo suficientemente menor como para ser descuidado. Esto significa que, como método simple, la votación es bastante efectiva. Podemos observar que el método B, que utiliza la puntuación de calidad devuelta por un motor de búsqueda para ajustar los pesos de importancia de las páginas devueltas para una consulta, no produce una mejora apreciable. Esto implica que poner en el mismo peso (votación) funciona de manera similar, ya que poner pesos más altos a documentos de mayor calidad y pesos más bajos a documentos de calidad más bajos (Método B), al menos para los resultados de búsqueda principales. Puede ser posible mejorar este método al incluir otras características de página que pueden diferenciar los resultados de búsqueda de mejor clasificación. Sin embargo, la efectividad requerirá una mayor investigación que no probamos. También podemos observar que el rendimiento en el conjunto 2 es más bajo que el del conjunto 1, lo que significa que las consultas en el conjunto 2 son más difíciles que las del conjunto 1. 3.7 Análisis de falla, examinamos los casos cuando el conocimiento externo no mejoró la clasificación de consultas e identificamosTres causas principales de tal falta de mejora.(1) Consultas que contienen cadenas aleatorias, como los números de teléfono: estas consultas no producen resultados de búsqueda coherentes, por lo que estas últimas no pueden ayudar a la clasificación (alrededor del 5% de las consultas fueron de este tipo).(2) consultas que no producen resultados de búsqueda en absoluto;Hubo un 8% de tales consultas en el conjunto 1 y 15% en el conjunto 2. (3) consultas correspondientes a eventos recientes, para las cuales el motor de búsqueda aún no tenía una amplia cobertura (alrededor del 5% de las consultas). Un ejemplo notable de tales consultas son los nombres completos de artículos de noticias, si el artículo exacto aún no ha sido indexado por el motor de búsqueda, es probable que los resultados de búsqueda sean de poca utilidad.4. Trabajo relacionado a pesar de que la duración promedio de las consultas de búsqueda aumenta constantemente con el tiempo, una consulta típica sigue siendo más corta que 3 palabras. En consecuencia, muchos investigadores estudiaron posibles formas de mejorar las consultas con información adicional. Una dirección importante para mejorar las consultas es a través de la expansión de la consulta. Esto se puede hacer utilizando diccionarios electrónicos y tesauros [22], o mediante técnicas de retroalimentación de relevancia que utilizan algunos resultados de búsqueda de alta puntuación. El trabajo temprano en la recuperación de la información se concentró en revisar manualmente los resultados devueltos [16, 15]. Sin embargo, el gran volumen de consultas hoy en día no se presta a la supervisión manual y, por lo tanto, los trabajos posteriores se centran en la retroalimentación de relevancia ciega, que básicamente asume que los resultados devueltos son relevantes [23, 12, 4, 14]. Más recientemente, los estudios en el aumento de consultas se centraron en la clasificación de consultas, suponiendo que tales clasificaciones sean beneficiosas para una interpretación de consultas más enfocada. De hecho, Kowalczyk et al.[10] encontraron que el uso de clases de consulta mejoró el rendimiento de la recuperación de documentos. Los estudios en el campo persiguen diferentes enfoques para obtener información adicional sobre las consultas. Beitzel et al.[1] utilizó el aprendizaje semi-supervisado, así como datos no etiquetados [2]. Gravano et al.[6] Consultas clasificadas con respecto a la localidad geográfica para determinar si su intención es local o global. La Copa KDD 2005 en la clasificación de consultas web inspiró otra línea de investigación, que se centró en enriquecer consultas utilizando motores y directorios de búsqueda web [11, 18, 20, 9, 21]. La especificación de la tarea KDD proporcionó una pequeña taxonomía (67 nodos) junto con un conjunto de consultas etiquetadas, y planteó un desafío para usar estos datos de capacitación para construir un clasificador de consulta. Varios equipos utilizaron la web para enriquecer las consultas y proporcionar más contexto para la clasificación. Las principales preguntas de investigación de este enfoque son (1) cómo construir un clasificador de documentos, (2) cómo traducir sus clasificaciones en la taxonomía objetivo y (3) cómo determinar la clase de consulta basada en clasificaciones de documentos. La solución ganadora de la Copa KDD [18] propuso usar un conjunto de clasificadores junto con la búsqueda de múltiples motores de búsqueda. Para abordar el problema (1) anterior, su solución utilizó el Proyecto Open Directory (ODP) para producir un clasificador de documentos basado en ODP. La jerarquía ODP se asignó a la taxonomía objetivo utilizando coincidencias de palabras en nodos individuales. Se creó un clasificador de documentos para la taxonomía objetivo utilizando las páginas en la taxonomía de ODP que aparecen en los nodos asignados al nodo objetivo particular. Por lo tanto, los documentos web se clasificaron primero con respecto a la jerarquía de ODP, y sus clasificaciones se asignaron posteriormente a la taxonomía objetivo para la clasificación de consultas. En comparación con este enfoque, resolvimos el problema de la clasificación de documentos directamente en la taxonomía objetivo mediante el uso de las consultas para producir el clasificador de documentos como se describe en la Sección 2. Esto simplifica el proceso y elimina la necesidad de mapeo entre taxonomías. Esto también optimiza el mantenimiento y el desarrollo de la taxonomía. Usando este enfoque, pudimos lograr un buen rendimiento en una taxonomía a gran escala. También evaluamos algunas alternativas sobre cómo combinar clasificaciones de documentos individuales al clasificar la consulta. En un artículo de seguimiento [19], Shen et al.propuso un marco para la clasificación de consultas basado en puentes entre dos taxonomías. En este enfoque, el problema de no tener un clasificador de documentos para resultados web se resuelve mediante el uso de un conjunto de capacitación disponible para documentos con una taxonomía diferente. Para esto, se utiliza una taxonomía intermedia con un conjunto de capacitación (ODP). Luego se juzgan varios esquemas que establecen una correspondencia entre las taxonomías o permiten el mapeo del conjunto de capacitación desde la taxonomía intermedia hasta la taxonomía objetivo. A diferencia de esto, creamos un clasificador de documentos para la taxonomía objetivo directamente, sin usar documentos de una taxonomía intermedia. Si bien no pudimos comparar directamente los resultados debido al uso de diferentes taxonomías (utilizamos una taxonomía mucho mayor), nuestros resultados de precisión y recuerdo son consistentemente más altos incluso sobre el conjunto de consultas más difícil.5. Conclusiones La clasificación de consultas es una tarea importante de recuperación de información. Es probable que la clasificación precisa de las consultas de búsqueda beneficie una serie de tareas de nivel superior, como la búsqueda en la web y la coincidencia de anuncios. Dado que las consultas de búsqueda suelen ser cortas, por sí mismas generalmente llevan información insuficiente para una precisión de clasificación adecuada. Para abordar este problema, propusimos una metodología para usar los resultados de búsqueda como fuente de conocimiento externo. Con este fin, enviamos la consulta a un motor de búsqueda y asumimos que una pluralidad de los resultados de búsqueda de alquileres son relevantes para la consulta. La clasificación de estos resultados nos permite clasificar la consulta original con una precisión sustancialmente mayor. Los resultados de nuestra evaluación empírica confirmaron definitivamente que el uso de la Web como un repositorio del conocimiento mundial contribuye con información valiosa sobre la consulta y ayuda en su clasificación correcta. En particular, nuestro método exhibe una precisión significativamente mayor que los métodos descritos en estudios anteriores3 en comparación con estudios anteriores, nuestro enfoque no requiere ninguna taxonomía auxiliar, y producimos un clasificador de consulta directamente para la taxonomía objetivo. Además, la taxonomía utilizada en este estudio es aproximadamente 2 órdenes de magnitud más grandes que la utilizada en trabajos anteriores. También experimentamos con diferentes valores de parámetros que caracterizan nuestro método. Al usar los resultados de búsqueda, uno puede usar solo resúmenes de los resultados proporcionados por 3 ya que el campo de la clasificación de consultas aún no ha establecido y acordado con puntos de referencia, la comparación directa de los resultados es ciertamente complicado.El motor de búsqueda, o realmente rastreó las páginas de resultados para obtener un conocimiento aún más profundo. En general, el rendimiento de la clasificación de consultas fue el mejor cuando se usa las páginas gastadas completas (Tabla 1). Estos resultados son consistentes con estudios previos [5], que encontraron que el uso de páginas gastadas completas es superior para la clasificación de documentos que el uso solo de resúmenes breves. Nuestros hallazgos, sin embargo, son diferentes de los reportados por Shen et al.[19], quien encontró resúmenes para producir mejores resultados. Atribuimos nuestras observaciones al uso de un esquema de votación más elaborado entre las clasificaciones de los resultados de búsqueda individuales, así como al uso de un conjunto más difícil de consultas raras. En este estudio utilizamos dos principales motores de búsqueda, A y B. Curiosamente, encontramos distinciones notables en la calidad de su producción. En particular, para el motor A, los resultados generales fueron mejores cuando se usan las páginas gastadas completas de los resultados de búsqueda, mientras que para el motor B parece ser más beneficioso utilizar los resúmenes de los resultados. Esto implica que si bien la calidad de los resultados de búsqueda devueltos por el motor A es aparentemente mejor, el motor B hace un mejor trabajo para resumir las páginas. También descubrimos que los mejores resultados se obtuvieron mediante el uso de páginas gastadas completas y realizando votación entre sus clasificaciones individuales. Para un clasificador externo al motor de búsqueda, recuperar páginas completas puede ser prohibitivamente costoso, en cuyo caso uno podría preferir usar resúmenes para obtener eficiencia computacional. Por otro lado, para los propietarios de un motor de búsqueda, la clasificación completa de la página es mucho más eficiente, ya que es fácil preprocesar todas las páginas indexadas clasificándolas una vez en la taxonomía (fija). Luego, las clasificaciones de la página se obtienen como parte de los metadatos asociados con cada resultado de la búsqueda, y la clasificación de consultas puede ser casi instantánea. Al usar resúmenes, parece que se obtienen mejores resultados concatenando primero los resúmenes individuales en un meta-documento, y luego utilizando su clasificación en su conjunto. Creemos que la razón de esta observación es que los resúmenes son cortos e inherentemente más ruidosos, y por lo tanto, su agregación ayuda a identificar correctamente el tema principal. De acuerdo con nuestra intuición, el uso de muy pocos resultados de búsqueda produce un conocimiento útil pero insuficiente, y el uso de demasiados resultados de búsqueda conduce a la inclusión de páginas web marginalmente relevantes. Los mejores resultados se obtuvieron al usar 40 golpes de búsqueda superiores. En este trabajo, primero clasificamos los resultados de búsqueda y luego usamos sus clasificaciones directamente para clasificar la consulta original. Alternativamente, uno puede usar las clasificaciones de los resultados de búsqueda como características para aprender un clasificador de segundo nivel. En la Sección 3.6, hicimos algunos experimentos preliminares en esta dirección, y descubrimos que aprender tal clasificador secundario no produjo ventajas considerablemente. Planeamos investigar más a fondo esta dirección en nuestro trabajo futuro. También es esencial tener en cuenta que la implementación de nuestra metodología incurre en poca sobrecarga. Si el motor de búsqueda clasifica las páginas rastreadas durante la indexación, entonces en la hora de consulta solo necesitamos obtener estas clasificaciones y hacer la votación. Para concluir, creemos que nuestra metodología para el uso de resultados de búsqueda web tiene una promesa considerable para mejorar sustancialmente la precisión de las consultas de búsqueda web. Esto es particularmente importante para las consultas raras, para la cual se puede hacer poco aprendizaje de perQrey, y en este estudio demostramos que tal escasez de información podría abordarse aprovechando el conocimiento encontrado en la Web. Creemos que nuestros hallazgos tendrán aplicaciones inmediatas para mejorar el manejo de consultas raras, tanto para mejorar los resultados de búsqueda como para producir anuncios mejor coincidentes. En nuestra investigación adicional, también planeamos utilizar la información de la sesión para aprovechar el conocimiento sobre consultas anteriores para clasificar mejor las posteriores.6. Referencias [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury y A. Kolcz. Clasificación automática de consultas web utilizando datos de capacitación etiquetados y no etiquetados. En Actas de Sigir05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury y A. Kolcz. Mejora de la clasificación automática de consultas a través del aprendizaje semi-supervisado. En Actas de ICDM05, 2005. [3] R. Duda y P. Hart. Clasificación de patrones y análisis de la escena. John Wiley and Sons, 1973. [4] E. Efthimiadis y P. Biron. UCLA-OKAPI en TREC-2: Experimentos de expansión de consultas. En TREC-2, 1994. [5] E. Gabrilovich y S. Markovitch. Generación de características para la categorización de texto utilizando el conocimiento mundial. En IJCAI05, páginas 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou y R. Lichtenstein. Categorizar consultas web de acuerdo con la localidad geográfica. En CIKM03, 2003. [7] E. Han y G. Karypis. Clasificación de documentos basada en centroides: análisis y resultados experimentales. En PKDD00, septiembre de 2000. [8] K. Jarvelin y J. Kekalainen. IR Métodos de evaluación para recuperar documentos altamente relevantes. En Sigir00, 2000. [9] Z. Kardkovacs, D. Tikk y Z. Bansaghi. El algoritmo de ferretía para el problema de la Copa KDD 2005. En Sigkdd Explorations, Volumen 7. ACM, 2005. [10] P. Kowalczyk, I. Zukerman y M. Niemann. Analizar el efecto de la clase de consulta en el rendimiento de la recuperación de documentos. En Proc. Conf. Australiano.en AI, páginas 550-561, 2004. [11] Y. Li, Z. Zheng y H. Dai. Informe KDD CUP-2005: enfrentar un gran desafío. En Sigkdd Explorations, Volumen 7, páginas 91-99. ACM, diciembre de 2005. [12] M. Mitra, A. Singhal y C. Buckley. Mejora de la expansión automática de consultas. En Sigir98, páginas 206-214, 1998. [13] M. Moran y B. Caza. Search Engine Marketing, Inc.: Conducir el tráfico de búsqueda al sitio web de su empresa. Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu y M. Gatford. Okapi en TREC-3. En Trec-3, 1995. [15] J. Rocchio. Comentarios de relevancia en la recuperación de información. En el sistema de recuperación inteligente: experimentos en el procesamiento automático de documentos, páginas 313-323. Prentice Hall, 1971. [16] G. Salton y C. Buckley. Mejora del rendimiento de la recuperación por retroalimentación relevante. Jasis, 41 (4): 288-297, 1990. [17] T. Santner y D. Duffy. El análisis estadístico de datos discretos. Springer-Verlag, 1989. [18] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Q2C@UST: Nuestra clasificación de solución ganadora para consultar en KDDCUP 2005. En Sigkdd Explorations, Volumen 7, páginas 100-110. ACM, 2005. [19] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin y Q. Yang. Enriquecimiento de consulta para la clasificación de cuidias web. ACM TOIS, 24: 320-352, julio de 2006. [20] D. Shen, J. Sun, Q. Yang y Z. Chen. Construyendo puentes para la clasificación de consultas web. En Sigir06, páginas 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges y T. Scheffer. Clasificación de consultas del motor de búsqueda utilizando la web como conocimiento de fondo. En Sigkdd Explorations, Volumen 7. ACM, 2005. [22] E. Voorhees. Expansión de consulta utilizando relaciones léxicas semánticas. En Sigir94, 1994. [23] J. Xu y W. Bruce Croft. Mejora de la efectividad de la recuperación de información con el análisis de contexto local. ACM TOIS, 18 (1): 79-112, 2000.",
    "original_sentences": [
        "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
        "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
        "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
        "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
        "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
        "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
        "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
        "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
        "One thing, however, has remained constant: people use very short queries.",
        "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
        "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
        "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
        "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
        "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
        "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
        "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
        "The problem of query classification is extremely difficult owing to the brevity of queries.",
        "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
        "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
        "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
        "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
        "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
        "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
        "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
        "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
        "We crawl the Web pages pointed by these URLs, and classify these pages.",
        "Finally, we use these result-page classifications to classify the original query.",
        "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
        "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
        "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
        "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
        "Another important aspect of our work lies in the choice of queries.",
        "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
        "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
        "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
        "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
        "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
        "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
        "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
        "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
        "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
        "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
        "However, these studies had a number of shortcomings, which we overcome in this paper.",
        "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
        "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
        "The main contributions of this paper are as follows.",
        "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
        "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
        "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
        "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
        "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
        "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
        "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
        "METHODOLOGY Our methodology has two main phases.",
        "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
        "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
        "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
        "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
        "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
        "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
        "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
        "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
        "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
        "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
        "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
        "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
        "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
        "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
        "This is the case for the majority of queries that are unambiguous.",
        "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
        "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
        "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
        "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
        "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
        "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
        "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
        "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
        "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
        "This relevance function is an adaptation of the traditional word-based retrieval rules.",
        "For example, we may let categories be the words in the vocabulary.",
        "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
        "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
        "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
        "That is, the ads are ranked according to P(q|a).",
        "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
        "The intuition can be described as follows.",
        "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
        "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
        "It should be mentioned that in our case, each query and ad can have multiple categories.",
        "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
        "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
        "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
        "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
        "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
        "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
        "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
        "That is, top results ranked by search engines should also be ranked high by this formula.",
        "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
        "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
        "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
        "Nor does it affect query classification with appropriately chosen thresholds.",
        "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
        "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
        "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
        "In the experiment, we will simply take uniform weights wi.",
        "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
        "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
        "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
        "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
        "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
        "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
        "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
        "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
        "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
        "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
        "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
        "The ads appropriate for these two queries are, however, very different.",
        "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
        "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
        "Figure 1 shows the distribution of categories by taxonomy levels.",
        "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
        "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
        "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
        "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
        "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
        "However, many searches do not explicitly use phrases that someone bids on.",
        "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
        "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
        "These transformations are based on rules and dictionaries.",
        "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
        "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
        "The first set of queries can be matched to at least one ad using broad match as described above.",
        "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
        "In a sense, these are even more rare queries and further away from common queries.",
        "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
        "The queries in the two sets differ in their classification difficulty.",
        "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
        "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
        "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
        "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
        "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
        "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
        "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
        "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
        "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
        "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
        "We used standard evaluation metrics: precision, recall and F1.",
        "In what follows, we plot precision-recall graphs for all the experiments.",
        "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
        "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
        "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
        "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
        "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
        "In what follows, we start with the general assessment of the effect of using Web search results.",
        "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
        "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
        "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
        "We use top search engine results for collecting background knowledge for queries.",
        "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
        "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
        "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
        "Engine Context Prec.",
        "F1 Prec.",
        "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
        "First, individual results can be classified separately, with subsequent voting among individual classifications.",
        "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
        "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
        "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
        "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
        "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
        "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
        "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
        "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
        "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
        "As can be readily seen, all three variants produce very similar results.",
        "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
        "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
        "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
        "Figure 5 and Table 2 present the results of this experiment.",
        "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
        "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
        "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
        "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
        "As we have seen, the voting method works quite well.",
        "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
        "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
        "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
        "Method B requires a training/testing split.",
        "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
        "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
        "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
        "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
        "The overall DCG of a system is the averaged DCG over queries.",
        "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
        "Therefore as a single metric, it is convenient for comparing the methods.",
        "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
        "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
        "The oracle method is the best ranking of categories for each query after seeing human judgments.",
        "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
        "The simple voting method performs very well in our experiments.",
        "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
        "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
        "This means that as a simple method, voting is quite effective.",
        "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
        "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
        "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
        "However, the effectiveness will require further investigation which we did not test.",
        "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
        "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
        "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
        "Consequently, many researchers studied possible ways to enhance queries with additional information.",
        "One important direction in enhancing queries is through query expansion.",
        "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
        "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
        "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
        "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
        "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
        "Studies in the field pursue different approaches for obtaining additional information about the queries.",
        "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
        "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
        "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
        "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
        "Several teams used the Web to enrich the queries and provide more context for classification.",
        "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
        "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
        "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
        "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
        "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
        "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
        "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
        "This simplifies the process and removes the need for mapping between taxonomies.",
        "This also streamlines taxonomy maintenance and development.",
        "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
        "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
        "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
        "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
        "For this, an intermediate taxonomy with a training set (ODP) is used.",
        "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
        "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
        "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
        "CONCLUSIONS Query classification is an important information retrieval task.",
        "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
        "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
        "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
        "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
        "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
        "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
        "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
        "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
        "We also experimented with different values of parameters that characterize our method.",
        "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
        "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
        "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
        "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
        "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
        "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
        "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
        "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
        "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
        "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
        "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
        "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
        "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
        "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
        "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
        "The best results were obtained when using 40 top search hits.",
        "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
        "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
        "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
        "We plan to further investigate this direction in our future work.",
        "It is also essential to note that implementing our methodology incurs little overhead.",
        "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
        "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
        "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
        "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
        "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
        "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
        "Automatic web query classification using labeled and unlabeled training data.",
        "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
        "Improving automatic query classification via semi-supervised learning.",
        "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
        "Pattern Classification and Scene Analysis.",
        "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
        "UCLA-Okapi at TREC-2: Query expansion experiments.",
        "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
        "Feature generation for text categorization using world knowledge.",
        "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
        "Categorizing web queries according to geographical locality.",
        "In CIKM03, 2003. [7] E. Han and G. Karypis.",
        "Centroid-based document classification: Analysis and experimental results.",
        "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
        "IR evaluation methods for retrieving highly relevant documents.",
        "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
        "The ferrety algorithm for the KDD Cup 2005 problem.",
        "In SIGKDD Explorations, volume 7.",
        "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
        "Analyzing the effect of query class on document retrieval performance.",
        "In Proc.",
        "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
        "KDD CUP-2005 report: Facing a great challenge.",
        "In SIGKDD Explorations, volume 7, pages 91-99.",
        "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
        "Improving automatic query expansion.",
        "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
        "Hunt.",
        "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
        "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
        "Okapi at TREC-3.",
        "In TREC-3, 1995. [15] J. Rocchio.",
        "Relevance feedback in information retrieval.",
        "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
        "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
        "Improving retrieval performance by relevance feedback.",
        "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
        "The Statistical Analysis of Discrete Data.",
        "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
        "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
        "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
        "In SIGKDD Explorations, volume 7, pages 100-110.",
        "ACM, 2005. [19] D. Shen, R. Pan, J.",
        "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
        "Query enrichment for web-query classification.",
        "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
        "Sun, Q. Yang, and Z. Chen.",
        "Building bridges for web query classification.",
        "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
        "Classifying search engine queries using the web as background knowledge.",
        "In SIGKDD Explorations, volume 7.",
        "ACM, 2005. [22] E. Voorhees.",
        "Query expansion using lexical-semantic relations.",
        "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
        "Improving the effectiveness of information retrieval with local context analysis.",
        "ACM TOIS, 18(1):79-112, 2000."
    ],
    "error_count": 0,
    "keys": {
        "query classification": {
            "translated_key": "clasificación de consultas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust <br>query classification</br> system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for <br>query classification</br>, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of <br>query classification</br> is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of <br>query classification</br>.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve <br>query classification</br> is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small <br>query classification</br> taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in <br>query classification</br> [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform <br>query classification</br>. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 <br>query classification</br> by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "<br>query classification</br> is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect <br>query classification</br> with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving <br>query classification</br>. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final <br>query classification</br>.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in <br>query classification</br>. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for <br>query classification</br>. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve <br>query classification</br>, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web <br>query classification</br> inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for <br>query classification</br>.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for <br>query classification</br> based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS <br>query classification</br> is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of <br>query classification</br> does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, <br>query classification</br> performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and <br>query classification</br> can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web <br>query classification</br> using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic <br>query classification</br> via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to <br>query classification</br> in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-<br>query classification</br>.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web <br>query classification</br>.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {Broder |Marcusf |GABR |Amrutaj |VANJAJ |tzhangth@yahoo-inc.com Resumen Proponemos una metodología para construir un sistema práctico de \"clasificación de consultas\" que pueda identificar miles de clases de consultas con precisión razonable, mientras se trata en tiempo real con el volumen de consultas de un motor de búsqueda web comercial.",
                "En este estudio presentamos una metodología para la \"clasificación de consultas\", donde nuestro objetivo es clasificar las consultas en una taxonomía comercial de consultas web con aproximadamente 6000 nodos.",
                "El problema de la \"clasificación de consultas\" es extremadamente difícil debido a la brevedad de las consultas.",
                "Nuestra evaluación empírica confirma que el uso de los resultados de búsqueda web de esta manera produce mejoras sustanciales en la precisión de la \"clasificación de consultas\".",
                "Esta sobrecarga adicional es mínima y, por lo tanto, el uso de resultados de búsqueda para mejorar la \"clasificación de consultas\" es completamente factible en tiempo de ejecución.",
                "Específicamente, los trabajos anteriores en el campo utilizaron taxonomías muy pequeñas de \"clasificación de consultas\" de solo unas pocas docenas de nodos, que no permiten una amplia especificidad para la publicidad en línea [11].",
                "Este resultado contrasta con los hallazgos anteriores en la \"clasificación de consultas\" [20], pero está respaldado por la investigación en la clasificación de texto convencional [5].2.",
                "En la segunda fase, desarrollamos un clasificador de consulta que invoca el clasificador de documentos en los resultados de búsqueda, y usa este último para realizar \"clasificación de consultas\".2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizados en un importante motor de búsqueda de EE. UU. (Ver Sección 3.1).",
                "Un estudio reciente [5] mostró que los clasificadores basados en centroides son efectivos y eficientes para las taxonomías a gran escala y, en consecuencia, utilizamos un clasificador centralide en este trabajo.2.2 \"Clasificación de consultas\" mediante la búsqueda después de haber desarrollado un clasificador de documentos para la taxonomía de la consulta, ahora recurrimos al problema de obtener una clasificación para una consulta dada basada en los resultados de búsqueda iniciales que produce.",
                "La \"clasificación de la consulta\" se determina mediante la evaluación primero de las probabilidades condicionales de todas las clases posibles P (CJ | Q), y luego seleccionando la alternativa con la probabilidad más alta Cmax = Arg Maxcj ∈C P (CJ | Q)."
            ],
            "translated_text": "",
            "candidates": [
                "clasificación de consultas",
                "clasificación de consultas",
                "clasificación de consultas",
                "clasificación de consultas",
                "clasificación de consultas",
                "clasificación de consultas",
                "clasificación de consultas",
                "clasificación de consultas",
                "clasificación de consultas",
                "clasificación de consultas",
                "clasificación de consultas",
                "clasificación de consultas",
                "Clasificación de consultas",
                "clasificación de consultas",
                "clasificación de consultas",
                "clasificación de consultas",
                "clasificación de consultas",
                "Clasificación de consultas",
                "clasificación de consultas",
                "clasificación de la consulta"
            ],
            "error": []
        },
        "search engine": {
            "translated_key": "motor de búsqueda",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web <br>search engine</br>.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of <br>search engine</br> traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web <br>search engine</br>, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial <br>search engine</br>, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US <br>search engine</br> (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a <br>search engine</br>.",
                "The <br>search engine</br> can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major <br>search engine</br>, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the <br>search engine</br> results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying <br>search engine</br>.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a <br>search engine</br> and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the <br>search engine</br> used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US <br>search engine</br>; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US <br>search engine</br>.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top <br>search engine</br> results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that <br>search engine</br> A performs consistently better with full-page text, while <br>search engine</br> B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from <br>search engine</br> results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a <br>search engine</br>.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in <br>search engine</br> evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a <br>search engine</br> to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the <br>search engine</br> did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the <br>search engine</br>, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a <br>search engine</br>, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the <br>search engine</br>, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the <br>search engine</br>, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a <br>search engine</br>, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the <br>search engine</br> classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "<br>search engine</br> Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying <br>search engine</br> queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {Broder |Marcusf |GABR |Amrutaj |VANJAJ |tzhang}@yahoo-inc.com Resumen Proponemos una metodología para construir un sistema de clasificación de consultas sólido práctico que pueda identificar miles de clases de consultas con precisión razonable, mientras se trata en tiempo real con el volumen de consultas de un \"motor de búsqueda\" comercial.",
                "Motivado por las necesidades de la publicidad de búsqueda, nos centramos principalmente en consultas raras, que son los más difíciles desde el punto de vista del aprendizaje automático, pero en la agregación representan una fracción considerable del tráfico de \"motor de búsqueda\".",
                "A los efectos de este estudio, primero enviamos la consulta dada a un \"motor de búsqueda\" de la web general, y recolectamos una serie de las URL de mayor puntuación.",
                "Tenga en cuenta que en una implementación práctica de nuestra metodología dentro de un \"motor de búsqueda\" comercial, todas las páginas indexadas se pueden clasificar previamente utilizando la tubería normal de procesamiento e indexación de texto.",
                "En la segunda fase, desarrollamos un clasificador de consulta que invoca el clasificador de documentos en los resultados de búsqueda, y utiliza este último para realizar la clasificación de consultas.2.1 Construcción del clasificador de documentos En este trabajo utilizamos una taxonomía de clasificación comercial de aproximadamente 6000 nodos utilizados en un importante \"motor de búsqueda\" de los EE. UU. (Ver Sección 3.1).",
                "Supongamos que hay un conjunto de documentos d = d1...DM indexado por un \"motor de búsqueda\".",
                "El \"motor de búsqueda\" puede representarse por una función F = similitud (q, d) que cuantifica la afinidad entre una consulta q y un documento d.Ejemplos de tales puntajes de afinidad utilizados en este documento son el rango: el rango del documento en la lista ordenada de resultados de búsqueda;puntaje estático: el puntaje de la bondad de la página, independientemente de la consulta (por ejemplo, PageRank);y puntaje dinámico: la cercanía de la consulta y el documento.",
                "Por lo tanto, dada una consulta Q, y las páginas de resultados K Top K D1 (Q) ,..., DK (Q) De un \"motor de búsqueda\" importante, ajustamos los parámetros P (CJ | Q) para que RC (DI (Q), Q) tenga puntajes altos para i = 1 ,..., K. Vale la pena mencionar que utilizando este método solo podemos calcular la fuerza relativa de P (CJ | Q), pero no la escala, porque la escala no afecta la clasificación.",
                "En esta formulación general, W (D, Q) puede depender de factores distintos del rango de D en los resultados del \"motor de búsqueda\" para q.",
                "Por ejemplo, puede ser una función de R (D, Q) donde R (D, Q) es el puntaje de relevancia devuelto por el \"motor de búsqueda\" subyacente."
            ],
            "translated_text": "",
            "candidates": [
                "buscador",
                "motor de búsqueda",
                "buscador",
                "motor de búsqueda",
                "buscador",
                "motor de búsqueda",
                "buscador",
                "motor de búsqueda",
                "buscador",
                "motor de búsqueda",
                "buscador",
                "motor de búsqueda",
                "buscador",
                "motor de búsqueda",
                "buscador",
                "motor de búsqueda",
                "buscador",
                "motor de búsqueda",
                "buscador",
                "motor de búsqueda"
            ],
            "error": []
        },
        "search advertising": {
            "translated_key": "búsqueda de publicidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of <br>search advertising</br>, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Motivado por las necesidades de \"publicidad de búsqueda\", nos centramos principalmente en consultas raras, que son los más difíciles desde el punto de vista del aprendizaje automático, pero en la agregación representan una fracción considerable del tráfico de motores de búsqueda."
            ],
            "translated_text": "",
            "candidates": [
                "búsqueda de publicidad",
                "publicidad de búsqueda"
            ],
            "error": []
        },
        "machine learning": {
            "translated_key": "aprendizaje automático",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of <br>machine learning</br>, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few <br>machine learning</br> algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Motivado por las necesidades de la publicidad de búsqueda, nos centramos principalmente en consultas raras, que son los más difíciles desde el punto de vista del \"aprendizaje automático\", pero en la agregación representan una fracción considerable del tráfico de motores de búsqueda.",
                "Pocos algoritmos de \"aprendizaje automático\" pueden manejar de manera eficiente tantas clases diferentes, cada una con cientos de ejemplos de capacitación."
            ],
            "translated_text": "",
            "candidates": [
                "aprendizaje automático",
                "aprendizaje automático",
                "aprendizaje automático",
                "aprendizaje automático"
            ],
            "error": []
        },
        "relevance feedback": {
            "translated_key": "Comentarios de relevancia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- <br>relevance feedback</br>, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo <br>relevance feedback</br> paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via <br>relevance feedback</br> techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind <br>relevance feedback</br>, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "<br>relevance feedback</br> in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by <br>relevance feedback</br>.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Categorías y descriptores de asignaturas H.3.3 [Algoritmo de almacenamiento y recuperación de información]: Búsqueda y recuperación de información: \"Comentarios de relevancia\", Algoritmos de Términos generales del proceso de búsqueda, medición, rendimiento, experimentación 1.",
                "Con este fin, empleamos el pseudo paradigma de \"retroalimentación de relevancia\" y asumimos que los principales resultados de búsqueda son relevantes para la consulta.",
                "Esto se puede hacer utilizando diccionarios electrónicos y tesauros [22], o mediante técnicas de \"retroalimentación de relevancia\" que utilizan algunos resultados de búsqueda de alta puntuación.",
                "Sin embargo, el gran volumen de consultas hoy en día no se presta a la supervisión manual y, por lo tanto, los trabajos posteriores se centran en la \"retroalimentación de relevancia\" ciega, que básicamente asume que los resultados devueltos son relevantes [23, 12, 4, 14].",
                "\"Comentarios de relevancia\" en la recuperación de la información.",
                "Mejora del rendimiento de la recuperación por \"retroalimentación de relevancia\"."
            ],
            "translated_text": "",
            "candidates": [
                "Comentarios de relevancia",
                "Comentarios de relevancia",
                "Comentarios de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "Comentarios de relevancia",
                "Comentarios de relevancia",
                "retroalimentación de relevancia"
            ],
            "error": []
        },
        "voting scheme": {
            "translated_key": "esquema de votación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate <br>voting scheme</br> among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Atribuimos nuestras observaciones al uso de un \"esquema de votación\" más elaborado entre las clasificaciones de los resultados de búsqueda individuales, así como al uso de un conjunto más difícil de consultas raras."
            ],
            "translated_text": "",
            "candidates": [
                "esquema de votación",
                "esquema de votación"
            ],
            "error": []
        },
        "crawling": {
            "translated_key": "rastreo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any <br>crawling</br> or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that <br>crawling</br> the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually <br>crawling</br> the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Por lo tanto, en tiempo de ejecución solo necesitamos ejecutar el procedimiento de votación, sin hacer ningún \"rastreo\" o clasificación.",
                "Descubrimos que \"rastrear\" los resultados de la búsqueda produce un conocimiento más profundo y conduce a mayores mejoras que los meros resúmenes.",
                "Luego procedemos a explorar técnicas más refinadas, como usar solo resúmenes de búsqueda versus \"arrastrar\" las URL devueltas."
            ],
            "translated_text": "",
            "candidates": [
                "rastreo",
                "rastreo",
                "rastreo",
                "rastrear",
                "rastreo",
                "arrastrar"
            ],
            "error": []
        },
        "topical taxonomy": {
            "translated_key": "taxonomía tópica",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a <br>topical taxonomy</br>.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Una elección natural para dicha agregación es clasificar las consultas en una \"taxonomía tópica\"."
            ],
            "translated_text": "",
            "candidates": [
                "taxonomía tópica",
                "taxonomía tópica"
            ],
            "error": []
        },
        "affinity score": {
            "translated_key": "puntaje de afinidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such <br>affinity score</br>s used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El motor de búsqueda se puede representar por una función f = similitud (q, d) que cuantifica la afinidad entre una consulta q y un documento d.Los ejemplos de tal \"puntaje de afinidad\" utilizado en este documento son el rango, el rango del documento en la lista ordenada de resultados de búsqueda;puntaje estático: el puntaje de la bondad de la página, independientemente de la consulta (por ejemplo, PageRank);y puntaje dinámico: la cercanía de la consulta y el documento."
            ],
            "translated_text": "",
            "candidates": [
                "puntaje de afinidad",
                "puntaje de afinidad"
            ],
            "error": []
        },
        "conditional probability": {
            "translated_key": "probabilidad condicional",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the <br>conditional probability</br> of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The <br>conditional probability</br> of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually <br>conditional probability</br> g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Nuestro objetivo es estimar la \"probabilidad condicional\" de cada clase posible utilizando los resultados de búsqueda inicialmente devueltos por la consulta.",
                "La \"probabilidad condicional\" de una clasificación para un documento P (CJ | D) se estima utilizando la salida del clasificador de documento (Sección 2.1).",
                "Además, es posible que los parámetros estimados puedan ser de la forma G (P (CJ | Q)) para alguna función monótona G (·) de la \"probabilidad condicional\" realmente \"G (P (CJ | Q))."
            ],
            "translated_text": "",
            "candidates": [
                "la probabilidad condicional",
                "probabilidad condicional",
                "la probabilidad condicional",
                "probabilidad condicional",
                "la probabilidad condicional",
                "probabilidad condicional"
            ],
            "error": []
        },
        "adaptation": {
            "translated_key": "adaptación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an <br>adaptation</br> of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Esta función de relevancia es una \"adaptación\" de las reglas de recuperación tradicionales basadas en palabras."
            ],
            "translated_text": "",
            "candidates": [
                "adaptación",
                "adaptación"
            ],
            "error": []
        },
        "information retrieval": {
            "translated_key": "recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for <br>information retrieval</br>.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in <br>information retrieval</br> concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important <br>information retrieval</br> task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in <br>information retrieval</br>.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of <br>information retrieval</br> with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Este modelo de relevancia se ha empleado en varias técnicas de modelado de lenguaje estadístico para la \"recuperación de la información\".",
                "El trabajo temprano en la \"recuperación de la información\" se concentró en revisar manualmente los resultados devueltos [16, 15].",
                "Conclusiones La clasificación de consultas es una tarea importante de \"recuperación de información\".",
                "Comentarios de relevancia en \"Recuperación de información\".",
                "Mejora de la efectividad de la \"recuperación de la información\" con el análisis de contexto local."
            ],
            "translated_text": "",
            "candidates": [
                "recuperación de información",
                "recuperación de la información",
                "recuperación de información",
                "recuperación de la información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "Recuperación de información",
                "recuperación de información",
                "recuperación de la información"
            ],
            "error": []
        },
        "web search": {
            "translated_key": "búsqueda Web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial <br>web search</br> engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the <br>web search</br> results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, <br>web search</br> had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of <br>web search</br>, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general <br>web search</br> engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using <br>web search</br> results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses <br>web search</br> results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of <br>web search</br> engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using <br>web search</br> results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using <br>web search</br> engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as <br>web search</br> and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using <br>web search</br> results holds considerable promise for substantially improving the accuracy of <br>web search</br> queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {Broder |Marcusf |GABR |Amrutaj |VANJAJ |tzhangth@yahoo-inc.com Resumen Proponemos una metodología para construir un sistema de clasificación de consultas sólido práctico que pueda identificar miles de clases de consultas con precisión razonable, mientras se trata en tiempo real con el volumen de consultas de un motor comercial de \"búsqueda web\".",
                "Utilizamos una técnica de retroalimentación ciega: dada una consulta, determinamos su tema clasificando los resultados de \"búsqueda web\" recuperados por la consulta.",
                "Introducción En su vida útil de 12 años, la \"búsqueda web\" había crecido enormemente: se ha convertido simultáneamente en un factor en la vida diaria de quizás mil millones de personas y al mismo tiempo una industria de ocho mil millones de dólares alimentada por la publicidad web.",
                "Al mismo tiempo, una mejor comprensión del significado de la consulta tiene el potencial de impulsar la base económica de la \"búsqueda web\", a saber, la publicidad en línea, a través del mecanismo de búsqueda patrocinado que coloca anuncios relevantes junto con los resultados de búsqueda.",
                "A los efectos de este estudio, primero enviamos la consulta dada a un motor general de \"búsqueda web\" y recolectamos una serie de las URL de mayor puntuación.",
                "Nuestra evaluación empírica confirma que el uso de la \"búsqueda web\" da como resultado de esta manera produce mejoras sustanciales en la precisión de la clasificación de consultas.",
                "Evaluación En esta sección, evaluamos nuestra metodología que utiliza resultados de \"búsqueda web\" para mejorar la clasificación de consultas.3.1 Taxonomía Nuestra elección de taxonomía fue guiada por una aplicación de publicidad web.",
                "La publicidad patrocinada de búsqueda (o búsqueda pagada) está colocando anuncios textuales en las páginas de resultados de los motores de \"búsqueda web\", con anuncios impulsados por la consulta de origen.",
                "En lo que sigue, comenzamos con la evaluación general del efecto del uso de resultados de \"búsqueda web\".",
                "La Copa KDD 2005 en la clasificación de consultas web inspiró otra línea de investigación, que se centró en enriquecer consultas utilizando motores y directorios de \"búsqueda web\" [11, 18, 20, 9, 21]."
            ],
            "translated_text": "",
            "candidates": [
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web"
            ],
            "error": []
        },
        "blind relevance feedback": {
            "translated_key": "retroalimentación de relevancia ciega",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Classification of Rare Queries Using Web Knowledge Andrei Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, Tong Zhang Yahoo!",
                "Research, 2821 Mission College Blvd, Santa Clara, CA 95054 {broder | marcusf | gabr | amrutaj | vanjaj | tzhang}@yahoo-inc.com ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine.",
                "We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query.",
                "Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic.",
                "Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported.",
                "We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval- relevance feedback, search process General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising.",
                "One thing, however, has remained constant: people use very short queries.",
                "Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information.",
                "Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient.",
                "Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience.",
                "At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results.",
                "For instance, knowing that the query SD450 is about cameras while nc4200 is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.",
                "In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes.",
                "Given such classifications, one can directly use them to provide better search results as well as more focused ads.",
                "The problem of query classification is extremely difficult owing to the brevity of queries.",
                "Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it.",
                "Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world.",
                "For instance, in the example above, SD450 brings pages about Canon cameras, while nc4200 brings pages about Compaq laptops, hence to a human the intent is quite clear.",
                "Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge.",
                "Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation.",
                "To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query.",
                "Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query.",
                "For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs.",
                "We crawl the Web pages pointed by these URLs, and classify these pages.",
                "Finally, we use these result-page classifications to classify the original query.",
                "Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.",
                "Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline.",
                "Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification.",
                "This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.",
                "Another important aspect of our work lies in the choice of queries.",
                "The volume of queries in todays search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times.",
                "While individual queries in this long tail are rare, together they account for a considerable mass of all searches.",
                "Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on.",
                "However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis.",
                "Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters.",
                "A natural choice for such aggregation is to classify the queries into a topical taxonomy.",
                "Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries.",
                "Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.",
                "Early studies in query interpretation focused on query augmentation through external dictionaries [22].",
                "More recent studies [18, 21] also attempted to gather some additional knowledge from the Web.",
                "However, these studies had a number of shortcomings, which we overcome in this paper.",
                "Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising [11].",
                "They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].",
                "The main contributions of this paper are as follows.",
                "First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development.",
                "The taxonomy used in this work is two orders of magnitude larger than that used in prior studies.",
                "The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported.",
                "Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable.",
                "We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages).",
                "We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries.",
                "This result is in contrast with prior findings in query classification [20], but is supported by research in mainstream text classification [5]. 2.",
                "METHODOLOGY Our methodology has two main phases.",
                "In the first phase, 1 In the above examples, SD450 and nc4200 represent fairly old gadget models, and hence there are advertisers placing ads on these queries.",
                "However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified.",
                "In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2.1 Building the document classifier In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search engine (see Section 3.1).",
                "Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.",
                "Given a taxonomy of this size, the computational efficiency of classification is a major issue.",
                "Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples.",
                "Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers.",
                "A recent study [5] showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields.",
                "Lets assume that there is a set of documents D = d1 . . . dm indexed by a search engine.",
                "The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document.",
                "Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj ∈C P(Cj|q).",
                "Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query.",
                "We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d∈D P(Cj|q, d)· P(d|q) = d∈D P(q|Cj, d) P(q|d) · P(Cj|d)· P(d|q).",
                "We assume that P(q|Cj, d) ≈ P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query.",
                "This is the case for the majority of queries that are unambiguous.",
                "Counter examples are queries like jaguar (animal and car brand) or apple (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words.",
                "In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds.",
                "Using this assumption, we can write P(Cj|q) = d∈D P(Cj|d)· P(d|q).",
                "The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1).",
                "While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query.",
                "This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance.",
                "Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q.",
                "This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q.",
                "In this paper, we consider the following approximation of relevance function: R(a, q) ≈ RC (a, q) = Cj ∈C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula.",
                "This relevance function is an adaptation of the traditional word-based retrieval rules.",
                "For example, we may let categories be the words in the vocabulary.",
                "We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj.",
                "With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.",
                "If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q)/P(Cj) = Cj ∈C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q).",
                "That is, the ads are ranked according to P(q|a).",
                "This relevance model has been employed in various statistical language modeling techniques for information retrieval.",
                "The intuition can be described as follows.",
                "We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj.",
                "For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.",
                "It should be mentioned that in our case, each query and ad can have multiple categories.",
                "For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj.",
                "We use P(Cj|q) to denote the probability of q belonging to category Cj.",
                "Here the sum Cj ∈C P(Cj|q) may not equal to one.",
                "We then consider the following ranking formula: RC (a, q) = Cj ∈C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known).",
                "Thus, we only need to obtain estimates of P(Cj|q) for each query q.",
                "Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q.",
                "In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search.",
                "That is, top results ranked by search engines should also be ranked high by this formula.",
                "Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking.",
                "Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(·) of the actually conditional probability g(P(Cj|q)).",
                "Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads.",
                "Nor does it affect query classification with appropriately chosen thresholds.",
                "In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj ∈C is random for an average document, then the condition that Cj ∈C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize K i=1 wiRC (di(q), q) subject to Cj ∈C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (·|q)]   1 K K i=1 wi Cj ∈C P(Cj|di(q))P(Cj|q) − λ Cj ∈C P(Cj|q)2   , where we assume K i=1 wi = 1, and λ > 0 is a tuning regularization parameter.",
                "The optimal solution is P(Cj|q) = 1 2λ K i=1 wiP(Cj|di(q)).",
                "Since both P(Cj|di(q)) and P(Cj|q) belong to [0, 1], we may just take λ = 0.5 to align the scale.",
                "In the experiment, we will simply take uniform weights wi.",
                "A more complex strategy is to let w depend on d as well: P(Cj|q) = d w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x.",
                "In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q.",
                "For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine.",
                "Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(·) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data.",
                "That is, we assign label yi(q) = 1 for di(q) when i ≤ K, and label yi(q) = −1 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear.",
                "Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj ∈C P(Cj|q)P(Cj|di(q)) = w·xi(q).",
                "The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method.",
                "In this paper, we consider estimating w using logistic regression [17] as follows: P(·|q) = arg minw i ln(1 + e−w·xi(q)yi(q) ). 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3.",
                "EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application.",
                "Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity.",
                "For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node.",
                "The ads appropriate for these two queries are, however, very different.",
                "To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics.",
                "Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9.",
                "Figure 1 shows the distribution of categories by taxonomy levels.",
                "Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising.",
                "Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query.",
                "All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency.",
                "These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query.",
                "A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].",
                "However, many searches do not explicitly use phrases that someone bids on.",
                "Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase.",
                "In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc.",
                "These transformations are based on rules and dictionaries.",
                "As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3.3 Data sets We used two representative sets of 1000 queries.",
                "Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).",
                "The first set of queries can be matched to at least one ad using broad match as described above.",
                "Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them.",
                "In a sense, these are even more rare queries and further away from common queries.",
                "As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.",
                "The queries in the two sets differ in their classification difficulty.",
                "In fact, queries in Set 2 are difficult to interpret even for human evaluators.",
                "Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words.",
                "Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets.",
                "As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words.",
                "Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2.",
                "Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators.",
                "These evaluators were trained editorial staff who possessed knowledge about the taxonomy.",
                "The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query.",
                "About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation.",
                "To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.",
                "We used standard evaluation metrics: precision, recall and F1.",
                "In what follows, we plot precision-recall graphs for all the experiments.",
                "For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1).",
                "Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge.",
                "Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 2 http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach.",
                "This baseline classifier is actually a production version of the query classifier running in a major US search engine.",
                "In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results.",
                "In what follows, we start with the general assessment of the effect of using Web search results.",
                "We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs.",
                "We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result.",
                "For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify.",
                "We use top search engine results for collecting background knowledge for queries.",
                "We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages.",
                "Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy.",
                "Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.",
                "Engine Context Prec.",
                "F1 Prec.",
                "F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge.",
                "First, individual results can be classified separately, with subsequent voting among individual classifications.",
                "Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier.",
                "Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin.",
                "However, in the case of summaries, bundling together is found to be consistently better than individual classification.",
                "This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results.",
                "Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together.",
                "The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification.",
                "This observation differs from findings by Shen et al. [20], who found summaries to be more useful.",
                "We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes.",
                "Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings.",
                "As can be readily seen, all three variants produce very similar results.",
                "However, the precision-recall curve for the 1-class experiment has higher fluctuations.",
                "Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth.",
                "Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query.",
                "Figure 5 and Table 2 present the results of this experiment.",
                "In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50).",
                "This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.",
                "Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline.",
                "We found the results to be highly significant (p < 0.0005), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model.",
                "As we have seen, the voting method works quite well.",
                "In this section, we compare the performance of voting top-ten search results to the following two methods: • A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. • B: Learning weights based on quality score returned by a search engine.",
                "We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries.",
                "The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).",
                "Method B requires a training/testing split.",
                "Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance ± standard deviation on the test-split for all three methods.",
                "For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation.",
                "The DCG (discounted cumulated gain) metric, described in [8], is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG.",
                "The decaying choice of log2(i + 1) is conventional, which does not have particular importance.",
                "The overall DCG of a system is the averaged DCG over queries.",
                "We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output.",
                "Therefore as a single metric, it is convenient for comparing the methods.",
                "Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers.",
                "Set 1 Method DCG-1 DCG-5 Oracle 7.58 ± 0.19 14.52 ± 0.40 Voting 5.28 ± 0.15 11.80 ± 0.31 Method A 5.48 ± 0.16 12.22 ± 0.34 Method B 5.36 ± 0.18 12.15 ± 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 ± 0.18 9.94 ± 0.32 Voting 3.50 ± 0.17 7.80 ± 0.28 Method A 3.63 ± 0.23 8.11 ± 0.33 Method B 3.55 ± 0.18 7.99 ± 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3.",
                "The oracle method is the best ranking of categories for each query after seeing human judgments.",
                "It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance.",
                "The simple voting method performs very well in our experiments.",
                "The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5).",
                "However, both methods are computationally more costly, and the potential gain is minor enough to be neglected.",
                "This means that as a simple method, voting is quite effective.",
                "We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement.",
                "This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results.",
                "It may be possible to improve this method by including other page-features that can differentiate top-ranked search results.",
                "However, the effectiveness will require further investigation which we did not test.",
                "We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries).",
                "One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4.",
                "RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words.",
                "Consequently, many researchers studied possible ways to enhance queries with additional information.",
                "One important direction in enhancing queries is through query expansion.",
                "This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback techniques that make use of a few top-scoring search results.",
                "Early work in information retrieval concentrated on manually reviewing the returned results [16, 15].",
                "However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on <br>blind relevance feedback</br>, which basically assumes top returned results to be relevant [23, 12, 4, 14].",
                "More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation.",
                "Indeed, Kowalczyk et al. [10] found that using query classes improved the performance of document retrieval.",
                "Studies in the field pursue different approaches for obtaining additional information about the queries.",
                "Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2].",
                "Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.",
                "The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21].",
                "The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.",
                "Several teams used the Web to enrich the queries and provide more context for classification.",
                "The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.",
                "The winning solution of the KDD Cup [18] proposed using an ensemble of classifiers in conjunction with searching multiple search engines.",
                "To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier.",
                "The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes.",
                "A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node.",
                "Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification.",
                "Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2.",
                "This simplifies the process and removes the need for mapping between taxonomies.",
                "This also streamlines taxonomy maintenance and development.",
                "Using this approach, we were able to achieve good performance in a very large scale taxonomy.",
                "We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.",
                "In a follow-up paper [19], Shen et al. proposed a framework for query classification based on bridging between two taxonomies.",
                "In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy.",
                "For this, an intermediate taxonomy with a training set (ODP) is used.",
                "Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy.",
                "As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy.",
                "While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5.",
                "CONCLUSIONS Query classification is an important information retrieval task.",
                "Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.",
                "Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.",
                "To address this problem, we proposed a methodology for using search results as a source of external knowledge.",
                "To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.",
                "Classifying these results then allows us to classify the original query with substantially higher accuracy.",
                "The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.",
                "Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.",
                "Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.",
                "We also experimented with different values of parameters that characterize our method.",
                "When using search results, one can either use only summaries of the results provided by 3 Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge.",
                "Overall, query classification performance was the best when using the full crawled pages (Table 1).",
                "These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries.",
                "Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results.",
                "We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.",
                "In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.",
                "Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.",
                "This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.",
                "We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.",
                "For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.",
                "On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.",
                "Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.",
                "When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.",
                "We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.",
                "Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.",
                "The best results were obtained when using 40 top search hits.",
                "In this work, we first classify search results, and then use their classifications directly to classify the original query.",
                "Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.",
                "In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.",
                "We plan to further investigate this direction in our future work.",
                "It is also essential to note that implementing our methodology incurs little overhead.",
                "If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.",
                "To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.",
                "This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.",
                "We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.",
                "In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones. 6.",
                "REFERENCES [1] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Automatic web query classification using labeled and unlabeled training data.",
                "In Proceedings of SIGIR05, 2005. [2] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury, and A. Kolcz.",
                "Improving automatic query classification via semi-supervised learning.",
                "In Proceedings of ICDM05, 2005. [3] R. Duda and P. Hart.",
                "Pattern Classification and Scene Analysis.",
                "John Wiley and Sons, 1973. [4] E. Efthimiadis and P. Biron.",
                "UCLA-Okapi at TREC-2: Query expansion experiments.",
                "In TREC-2, 1994. [5] E. Gabrilovich and S. Markovitch.",
                "Feature generation for text categorization using world knowledge.",
                "In IJCAI05, pages 1048-1053, 2005. [6] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.",
                "Categorizing web queries according to geographical locality.",
                "In CIKM03, 2003. [7] E. Han and G. Karypis.",
                "Centroid-based document classification: Analysis and experimental results.",
                "In PKDD00, September 2000. [8] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR00, 2000. [9] Z. Kardkovacs, D. Tikk, and Z. Bansaghi.",
                "The ferrety algorithm for the KDD Cup 2005 problem.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [10] P. Kowalczyk, I. Zukerman, and M. Niemann.",
                "Analyzing the effect of query class on document retrieval performance.",
                "In Proc.",
                "Australian Conf. on AI, pages 550-561, 2004. [11] Y. Li, Z. Zheng, and H. Dai.",
                "KDD CUP-2005 report: Facing a great challenge.",
                "In SIGKDD Explorations, volume 7, pages 91-99.",
                "ACM, December 2005. [12] M. Mitra, A. Singhal, and C. Buckley.",
                "Improving automatic query expansion.",
                "In SIGIR98, pages 206-214, 1998. [13] M. Moran and B.",
                "Hunt.",
                "Search Engine Marketing, Inc.: Driving Search Traffic to Your Companys Web Site.",
                "Prentice Hall, Upper Saddle River, NJ, 2005. [14] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In TREC-3, 1995. [15] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313-323.",
                "Prentice Hall, 1971. [16] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "JASIS, 41(4):288-297, 1990. [17] T. Santner and D. Duffy.",
                "The Statistical Analysis of Discrete Data.",
                "Springer-Verlag, 1989. [18] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Q2C@UST: Our winning solution to query classification in KDDCUP 2005.",
                "In SIGKDD Explorations, volume 7, pages 100-110.",
                "ACM, 2005. [19] D. Shen, R. Pan, J.",
                "Sun, J. Pan, K. Wu, J. Yin, and Q. Yang.",
                "Query enrichment for web-query classification.",
                "ACM TOIS, 24:320-352, July 2006. [20] D. Shen, J.",
                "Sun, Q. Yang, and Z. Chen.",
                "Building bridges for web query classification.",
                "In SIGIR06, pages 131-138, 2006. [21] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen, S. Bridges, and T. Scheffer.",
                "Classifying search engine queries using the web as background knowledge.",
                "In SIGKDD Explorations, volume 7.",
                "ACM, 2005. [22] E. Voorhees.",
                "Query expansion using lexical-semantic relations.",
                "In SIGIR94, 1994. [23] J. Xu and W. Bruce Croft.",
                "Improving the effectiveness of information retrieval with local context analysis.",
                "ACM TOIS, 18(1):79-112, 2000."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Sin embargo, el gran volumen de consultas hoy en día no se presta a la supervisión manual y, por lo tanto, los trabajos posteriores se centran en la \"retroalimentación de relevancia ciega\", que básicamente asume que los resultados devueltos son relevantes [23, 12, 4, 14]."
            ],
            "translated_text": "",
            "candidates": [
                "Comentarios de relevancia ciega",
                "retroalimentación de relevancia ciega"
            ],
            "error": []
        }
    }
}