{
    "id": "H-53",
    "original_text": "Context Sensitive Stemming for Web Search Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo! Inc. 701 First Avenue Sunnyvale, California 94089 {fuchun, nawaaz, xinli, yumaol}@yahoo-inc.com ABSTRACT Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms. Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation. In this paper, we propose a context sensitive stemming method that addresses these two issues. Two unique properties make our approach feasible for Web Search. First, based on statistical language modeling, we perform context sensitive analysis on the query side. We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine. This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time. Second, our approach performs a context sensitive document matching for those expanded variants. This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision. Using word pluralization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic. Categories and Subject Descriptors H.3.3 [Information Systems]: Information Storage and Retrieval-Query formulation General Terms Algorithms, Experimentation 1. INTRODUCTION Web search has now become a major tool in our daily lives for information seeking. One of the important issues in Web search is that user queries are often not best formulated to get optimal results. For example, running shoe is a query that occurs frequently in query logs. However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes. Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs. This is difficult even for experienced users, and especially difficult for non-native speakers. One traditional solution is to use stemming [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term. Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing. Stemming can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing. Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows handling each query differently, and hence is preferred. Although traditional stemming increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched. When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6]. In addition, it reduces system performance because the search engine has to match all the word variants. As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa. Thus, one needs to be very cautious when using stemming in Web search engines. One problem of traditional stemming is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word. For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored. For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query. Transforming book store to match book stores is fine, but matching book storing or booking store is not. A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query. However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20]. A second problem of traditional stemming is its blind matching of all occurrences in documents. For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store. Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected. Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases. To alleviate these two problems, we propose a context sensitive stemming approach for Web search. Our solution consists of two context sensitive analysis, one on the query side and the other on the document side. On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms. On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query. Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines. We use pluralization handling as a running example for our stemming approach. The motivation for using pluralization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance. As far as we know, no previous research has systematically investigated the usage of pluralization in Web search. As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion. Experiments on general stemming yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper. In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2. We describe the details of the context sensitive stemming approach in Section 3. We then perform extensive experiments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5. Finally, we conclude the paper in Section 6. 2. RELATED WORK Stemming is a long studied technology. Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18]. The Porter stemmer is widely used due to its simplicity and effectiveness in many applications. However, the Porter stemming makes many mistakes because its simple rules cannot fully describe English morphology. Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23]. We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates. Using stemming in information retrieval is also a well known technique [8, 10]. However, the effectiveness of stemming for English query systems was previously reported to be rather limited. Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance. Later, Harman [9] compares three general stemming techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper). They also proposed selective stemming based on query length and term importance, but no positive results were reported. On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%). However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search. These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2]. We suspect the previous failures were mainly due to the two problems we mentioned in the introduction. Blind stemming, or a simple query length based selective stemming as used in [9] is not enough. Stemming has to be decided on case by case basis, not only at the query level but also at the document level. As we will show, if handled correctly, significant improvement can be achieved. A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25]. To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21]. This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search. In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift). Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query. Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search. On the other hand, stemming is much more conservative since most of the time, stemming preserves the original search intent. While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision. The increase on recall is obvious. With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded. On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the expanded query. This is similar to the relevance feedback in the sense that it requires multiple passes retrieval. If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful. It is expensive to deploy this in production Web search engines. Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine. In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search - stemming. Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision. Its simple, yet very efficient and effective, making real time stemming feasible for Web search. Our results will affirm researchers that stemming is indeed very important to large scale information retrieval. 3. CONTEXT SENSITIVE STEMMING 3.1 Overview Our system has four components as illustrated in Figure 1: candidate generation, query segmentation and head word detection, context sensitive query stemming and context sensitive document matching. Candidate generation (component 1) is performed oﬄine and generated candidates are stored in a dictionary. For an input query, we first segment query into concepts and detect the head word for each concept (component 2). We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4). Below we discuss each of the components in more detail. Component 4: context sensitive document matching Input Query: and head word detection Component 2: segment Component 1: candidate generation comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: hotel comparisons hotel −> hotels Figure 1: System Components 3.2 Expansion candidate generation One of the ways to generate candidates is using the Porter stemmer [18]. The Porter stemmer simply uses morphological rules to convert a word to its base form. It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past. A more conservative way is based on using corpus analysis to improve the Porter stemmer results [26]. The corpus analysis we do is based on word distributional similarity [15]. The rationale of using distributional word similarity is that true variants tend to be used in similar contexts. In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word. We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus. The similarity between two words is the cosine similarity between the two corresponding feature vectors. The top 20 similar words to develop is shown in the following table. rank candidate score rank candidate score 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word develop. Column score is the similarity score. To determine the stemming candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list. After applying these rules, for the word develop, the stemming candidates are developing, developed, develops, development, developement, developer, developmental. For the pluralization handling purpose, only the candidate develops is retained. One thing we note from observing the distributionally similar words is that they are closely related semantically. These words might serve as candidates for general query expansion, a topic we will investigate in the future. 3.3 Segmentation and headword identification For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts. We first break a query into segments, each segment representing a concept which normally is a noun phrase. For each of the noun phrases, we then detect the most important word which we call the head word. Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity. To break a query into segments, we have to define a criteria to measure the strength of the relation between words. One effective method is to use mutual information as an indicator on whether or not to split two words [19]. We use a log of 25M queries and collect the bigram and unigram frequencies from it. For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word. We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here. Table 2 shows some examples of query segmentation. The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query. Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: Query segmentation: a segment is bracketed. parsing since many queries are not grammatical and are very short. Applying a parser trained on sentences from documents to queries will have poor performance. In our solution, we just use simple heuristics rules, and it works very well in practice for English. For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of/in/at/from UVW. In such cases, the head word is typically the last nonstop word of XYZ. 3.4 Context sensitive word expansion After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful. Our statistics show that about half of the queries can be transformed by pluralization via naive stemming. Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse. Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost. In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful. Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts. Head words hotel and comparison can be expanded to hotels and comparisons. Are both transformations useful? To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web. The more likely a query to occur on the Web, the more relevant documents this query is able to return. Now the whole problem becomes how to calculate the probability of query to occur on the Web. Calculating the probability of string occurring in a corpus is a well known language modeling problem. The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur). The simplest and most successful approach to language modeling is still based on the n-gram model. By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e. Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus. Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary. This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6). Also, because of the heavy tailed nature of language (i.e. Zipfs law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling. One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator. Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5]. We used absolute smoothing in our experiments. Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string. Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3. We can see that all alternatives are less likely than the input query. It is therefore not useful to make an expansion for this query. On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded. To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score. Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face. 3.5 Context sensitive document matching Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants. For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison. However, not every occurrence of comparison in the document is of interest. A page which is about comparing customer service can contain all of the words hotel price comparisons comparison. This page is not a good page for the query. If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval. To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document. A variant match is considered valid only if the variant occurs in the same context as the original word does. The context is the left or the right non-stop segments 1 of the original word. Taking the same query as an example, the context of comparisons is price. The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price. Thus, we should only match those occurrences of comparison in the document if they occur after the word price. Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size. If the expanded word comparison occurs within the context of price within a window, it is considered valid. The smaller the window size is, the more restrictive the matching. We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases. 4. EXPERIMENTAL EVALUATION 4.1 Evaluation metrics We will measure both relevance improvement and the stemming cost required to achieve the relevance. 1 a context segment can not be a single stop word. 4.1.1 Relevance measurement We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11]. Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) . (7) where gk is the weight for the document at rank k. Higher degree of relevance corresponds to a higher weight. A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights. We use dcg to represent the average DCG(5) over a set of test queries. 4.1.2 Stemming cost Another metric is to measure the additional cost incurred by stemming. Given the same level of relevance improvement, we prefer a stemming method that has less additional cost. We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed. 4.2 Data preparation We randomly sample 870 queries from a three month query log, with 290 from each month. Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming. We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words. In the end, we have 529 correctly spelled queries with at least 2 words. 4.3 Naive stemming for Web search Before explaining the experiments and results in detail, wed like to describe the traditional way of using stemming for Web search, referred as the naive model. This is to treat every word variant equivalent for all possible words in the query. The query book store will be transformed into (book OR books)(store OR stores) when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments. 4.4 Experimental setup The baseline model is the model without stemming. We first run the naive model to see how well it performs over the baseline. Then we improve the naive stemming model by document sensitive matching, referred as document sensitive matching model. This model makes the same stemming as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5. The naive model and document sensitive matching model stem the most queries. Out of the 529 queries, there are 408 queries that they stem, corresponding to 46.7% query traffic (out of a total of 870). We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as selective stemming model. Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model. We experiment with unigram language model and bigram language model. Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments. To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data. The oracle model only expands a word if the stemming will give better results. To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries. Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words. 4.5 Results We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5. Each row in Table 4 is a stemming strategy described in section 4.4. The first column is the name of the strategy. The second column is the number of queries affected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg. The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy). The fourth column is the relative improvement over the baseline, and the last column is the p-value of Wilcoxon significance test. There are several observations about the results. We can see the naively stemming only obtains a statistically insignificant improvement of 1.5%. Looking at Table 5, it gives an improvement of 2.7% on short queries. However, it also hurts long queries by -2.4%. Overall, the improvement is canceled out. The reason that it improves short queries is that most short queries only have one word that can be stemmed. Thus, blindly pluralizing short queries is relatively safe. However for long queries, most queries can have multiple words that can be pluralized. Expanding all of them without selection will significantly hurt precision. Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8%. The improvement comes from the conservative context sensitive document matching. An expanded word is valid only if it occurs within the context of original query in the document. This reduces many spurious matches. However, we still notice that for long queries, context sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem. While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches. It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases. Selective word pluralization further helps resolving the problem faced by document context sensitive stemming. It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stemming in the first place. By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision. With the unigram language model, we can reduce the stemming cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4%. In particular, it gives significant improvements on long queries. The dcg gain is turned from negative to positive, from −1.6% to 1.1%. This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement. For short queries too, we observe both dcg improvement and stemming cost reduction with the unigram language model. The advantages of predictive word expansion with a language model is further boosted with a better bigram language model. The overall dcg gain is lifted from 3.4% to 3.9%, and stemming cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic. For short queries, bigram language model improves the dcg gain from 4.4% to 4.7%, and reduces stemming cost from 272/272 to 150/272. For long queries, bigram language model improves dcg gain from 1.1% to 2.5%, and reduces stemming cost from 136/136 to 100/136. We observe that the bigram language model gives a larger lift for long queries. This is because the uncertainty in long queries is larger and a more powerful language model is needed. We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation. Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying. For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram language model. For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram language model. We may gain additional benefit with a more powerful language model for long queries. However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem. These problems have to be addressed separately. Looking at the the upper-bound of overhead reduction for oracle stemming, 75% (308/408) of the naive stemmings are wasteful. We currently capture about half of them. Further reduction of the overhead requires sacrificing the dcg gain. Now we can compare the stemming strategies from a different aspect. Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only. We can see that the number of affected queries decreases as the stemming strategy becomes more accurate (dcg improvement). For the bigram language model, over the 250/408 stemmed queries, the dcg improvement is 6.1%. An interesting observation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more difficult queries (low dcg queries). 5. DISCUSSIONS 5.1 Language models from query vs. from Web As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web. The language model should describe the occurrence of the string on the Web. However, the query log is also a good resource. 2 Note that this upperbound is for pluralization handling only, not for general stemming. General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics. Affected Queries dcg dcg Improvement p-value baseline 0/408 7.102 N/A N/A naive model 408/408 7.206 1.5% 0.22 document context sensitive model 408/408 7.302 2.8% 0.014 selective model: unigram LM 300/408 7.321 3.4% 0.001 selective model: bigram LM 250/408 7.381 3.9% 0.001 oracle model 100/408 7.519 5.9% 0.001 Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming Short Query Results Affected Queries dcg Improvement p-value baseline 0/272 N/A N/A naive model 272/272 2.7% 0.48 document context sensitive model 272/272 4.2% 0.002 selective model: unigram LM 185/272 4.4% 0.001 selective model: bigram LM 150/272 4.7% 0.001 oracle model 71/272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p-value baseline 0/136 N/A N/A naive model 136/136 -2.4% 0.25 document context sensitive model 136/136 -1.6% 0.27 selective model: unigram LM 115/136 1.1% 0.001 selective model: bigram LM 100/136 2.5% 0.001 oracle model 29/136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results. To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a language model from the same query top 25M queries as used to learn segmentation, and use that for prediction. We observed a slight performance decrease compared to the model trained on Web frequencies. In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries. Thus, the query log can serve as a good approximation of the Web frequencies. 5.2 How linguistics helps Some linguistic knowledge is useful in stemming. For the pluralization handling case, pluralization and de-pluralization is not symmetric. A plural word used in a query indicates a special intent. For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California. A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank. To capture this intent, we have to make sure the document is a general page about hotels in new york. We do this by requiring that the plural word hotels appears in the document. On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information. We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property. 5.3 Error analysis One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after stemming. Generally speaking, pluralization or depluralization keeps the original intent. However, the intent could change in a few cases. For one example of such a query, job at apple, we pluralize job to jobs. This stemming makes the original query ambiguous. The query job OR jobs at apple has two intents. One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company. Thus, the results after query stemming returns Steve Jobs as one of the results in top 5. One solution is performing results set based analysis to check if the intent is changed. This is similar to relevance feedback and requires second phase ranking. A second type of mistakes is the entity/concept recognition problem, These include two kinds. One is that the stemmed word variant now matches part of an entity or concept. For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco. The results will match cookie jar in san francisco. Although cookie still means the same thing as cookies, cookie jar is a different concept. Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words. For example, quote ICE is pluralized to quote OR quotes ICE. The original intent for this query is searching for stock quote for ticker ICE. However, we noticed that among the top results, one of the results is Food quotes: Ice cream. This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408/408 7.102 7.206 1.5% document context sensitive model 408/408 7.102 7.302 2.8% selective model: unigram LM 300/408 5.904 6.187 4.8% selective model: bigram LM 250/408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the affected queries before/after applying stemming the pluralized word quotes. The unchanged word ICE matches part of the noun phrase ice cream here. To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words. A third type of mistakes occurs in long queries. For the query bar code reader software, two words are pluralized. code to codes and reader to readers. In fact, bar code reader in the original query is a strong concept and the internal words should not be changed. This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking. For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept. 6. CONCLUSIONS AND FUTURE WORK We have presented a simple yet elegant way of stemming for Web search. It improves naive stemming in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side. Using pluralization handling as an example, experiments on a major Web search engine data show it significantly improves the Web relevance and reduces the stemming cost. It also significantly improves Web click through rate (details not reported in the paper). For the future work, we are investigating the problems we identified in the error analysis section. These include: entity and noun phrase matching mistakes, and improved segmentation. 7. REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais. Improving Web Search Ranking by Incorporating User Behavior Information. In SIGIR, 2006. [2] E. Airio. Word Normalization and Decompounding in Mono- and Bilingual IR. Information Retrieval, 9:249-271, 2006. [3] P. Anick. Using Terminological Feedback for Web Search Refinement: a Log-based Study. In SIGIR, 2003. [4] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. ACM Press/Addison Wesley, 1999. [5] S. Chen and J. Goodman. An Empirical Study of Smoothing Techniques for Language Modeling. Technical Report TR-10-98, Harvard University, 1998. [6] S. Cronen-Townsend, Y. Zhou, and B. Croft. A Framework for Selective Query Expansion. In CIKM, 2004. [7] H. Fang and C. Zhai. Semantic Term Matching in Axiomatic Approaches to Information Retrieval. In SIGIR, 2006. [8] W. B. Frakes. Term Conflation for Information Retrieval. In C. J. Rijsbergen, editor, Research and Development in Information Retrieval, pages 383-389. Cambridge University Press, 1984. [9] D. Harman. How Effective is Suffixing? JASIS, 42(1):7-15, 1991. [10] D. Hull. Stemming Algorithms - A Case Study for Detailed Evaluation. JASIS, 47(1):70-84, 1996. [11] K. Jarvelin and J. Kekalainen. Cumulated Gain-Based Evaluation Evaluation of IR Techniques. ACM TOIS, 20:422-446, 2002. [12] R. Jones, B. Rey, O. Madani, and W. Greiner. Generating Query Substitutions. In WWW, 2006. [13] W. Kraaij and R. Pohlmann. Viewing Stemming as Recall Enhancement. In SIGIR, 1996. [14] R. Krovetz. Viewing Morphology as an Inference Process. In SIGIR, 1993. [15] D. Lin. Automatic Retrieval and Clustering of Similar Words. In COLING-ACL, 1998. [16] J. B. Lovins. Development of a Stemming Algorithm. Mechanical Translation and Computational Linguistics, II:22-31, 1968. [17] M. Lennon and D. Peirce and B. Tarry and P. Willett. An Evaluation of Some Conflation Algorithms for Information Retrieval. Journal of Information Science, 3:177-188, 1981. [18] M. Porter. An Algorithm for Suffix Stripping. Program, 14(3):130-137, 1980. [19] K. M. Risvik, T. Mikolajewski, and P. Boros. Query Segmentation for Web Search. In WWW, 2003. [20] S. E. Robertson. On Term Selection for Query Expansion. Journal of Documentation, 46(4):359-364, 1990. [21] G. Salton and C. Buckley. Improving Retrieval Performance by Relevance Feedback. JASIS, 41(4):288 - 297, 1999. [22] R. Sun, C.-H. Ong, and T.-S. Chua. Mining Dependency Relations for Query Expansion in Passage Retrieval. In SIGIR, 2006. [23] C. Van Rijsbergen. Information Retrieval. Butterworths, second version, 1979. [24] B. V´elez, R. Weiss, M. A. Sheldon, and D. K. Gifford. Fast and Effective Query Refinement. In SIGIR, 1997. [25] J. Xu and B. Croft. Query Expansion using Local and Global Document Analysis. In SIGIR, 1996. [26] J. Xu and B. Croft. Corpus-based Stemming using Cooccurrence of Word Variants. ACM TOIS, 16 (1):61-81, 1998.",
    "original_translation": "Contexto Sensible Stemming for Web Search Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo! Inc. 701 First Avenue Sunnyvale, California 94089 {Fuchun, Nawaaz, Xinli, yumaolt.@yahoo-inc.com Resumen Tradicionalmente, se ha aplicado a las tareas de recuperación de información transformando palabras en documentos a su forma de raíz antes de indexar y aplicarUna transformación similar a los términos de consulta. Aunque aumenta el recuerdo, esta estrategia ingenua no funciona bien para la búsqueda web, ya que reduce la precisión y requiere una cantidad significativa de cálculo adicional. En este documento, proponemos un método de derivación sensible al contexto que aborde estos dos problemas. Dos propiedades únicas hacen que nuestro enfoque sea factible para la búsqueda web. Primero, basado en el modelado de lenguaje estadístico, realizamos un análisis sensible de contexto en el lado de la consulta. Predecimos con precisión cuál de sus variantes morfológicas es útil para expandir un término de consulta antes de enviar la consulta al motor de búsqueda. Esto reduce drásticamente el número de expansiones malas, lo que a su vez reduce el costo del cálculo adicional y mejora la precisión al mismo tiempo. En segundo lugar, nuestro enfoque realiza una coincidencia de documentos sensible al contexto para esas variantes expandidas. Esta estrategia conservadora sirve como una salvaguardia contra los siglos espurios, y resulta ser muy importante para mejorar la precisión. Utilizando el manejo de la pluralización de palabras como un ejemplo de nuestro enfoque de Stemming, nuestros experimentos en un motor de búsqueda web importante muestran que la derivación de solo el 29% del tráfico de la consulta, podemos mejorar la relevancia medida por una ganancia acumulativa con descuento promedio (DCG5) en un 6.1% en estosconsultas y 1.8% sobre todo el tráfico de consultas. Categorías y descriptores de sujetos H.3.3 [Sistemas de información]: Algoritmos de Términos generales de almacenamiento de información y recuperación de la Cuidera. Introducción La búsqueda web se ha convertido en una herramienta importante en nuestra vida diaria para la búsqueda de información. Uno de los problemas importantes en la búsqueda web es que las consultas de los usuarios a menudo no están mejor formuladas para obtener resultados óptimos. Por ejemplo, la zapatilla de carrera es una consulta que ocurre con frecuencia en los registros de consultas. Sin embargo, es mucho más probable que las zapatillas para correr de consulta dan mejores resultados de búsqueda que la consulta original porque los documentos que coinciden con la intención de esta consulta generalmente contienen las palabras de las zapatillas. Formular correctamente una consulta requiere que el usuario predice con precisión qué forma de palabra se usa en los documentos que mejor satisfacen sus necesidades de información. Esto es difícil incluso para los usuarios experimentados, y especialmente difícil para los oradores no nativos. Una solución tradicional es usar Stemming [16, 18], el proceso de transformación de palabras infectadas o derivadas en su forma raíz para que un término de búsqueda coincida y recupere documentos que contengan todas las formas del término. Por lo tanto, la palabra carrera coincidirá con correr, correr, correr y zapatos combinará con zapatos y zapatos. Se puede realizar la derecha en los términos en un documento durante la indexación (y aplicar la misma transformación a los términos de consulta durante el procesamiento de la consulta) o expandiendo la consulta con las variantes durante el procesamiento de la consulta. La derivación durante la indexación permite muy poca flexibilidad durante el procesamiento de la consulta, mientras que la expansión de la consulta permite el manejo de cada consulta de manera diferente y, por lo tanto, se prefiere. Aunque el retraso tradicional de los vallas aumenta el recuerdo de las variantes de palabras coincidentes [13], puede reducir la precisión al recuperar demasiados documentos que se han igualado incorrectamente. Al examinar los resultados de la aplicación de Stemming a una gran cantidad de consultas, generalmente se encuentra que la técnica [6] se da cuenta casi igual de consultas. Además, reduce el rendimiento del sistema porque el motor de búsqueda tiene que coincidir con todas las variantes de palabras. Como mostraremos en los experimentos, esto es cierto incluso si simplificamos las derivaciones del manejo de la pluralización, que es el proceso de convertir una palabra de su forma plural a singular, o viceversa. Por lo tanto, uno debe ser muy cauteloso al usar Stemming en los motores de búsqueda web. Un problema de los siglos tradicionales es su transformación ciega de todos los términos de consulta, es decir, siempre realiza la misma transformación para la misma palabra de consulta sin considerar el contexto de la palabra. Por ejemplo, The Word Book tiene cuatro formularios de libros, libros, reservas, reservas y tienda tiene cuatro formularios tiendas, tiendas, almacenamiento, almacenados. Para la librería de consultas, expandir ambas palabras a todas sus variantes aumenta significativamente el costo de cálculo y duele la precisión, ya que no todas las variantes son útiles para esta consulta. Transformar las librerías de las librerías para que las librerías estén bien, pero la tienda de almacenamiento o reserva de libros coincidentes no lo es. Un método de ponderación que proporciona palabras variantes más pequeñas pesas alivia los problemas en cierta medida si los pesos reflejan con precisión la importancia de la variante en esta consulta en particular. Sin embargo, la ponderación uniforme no va a funcionar y una ponderación dependiente de la consulta sigue siendo un problema desafiante sin resolver [20]. Un segundo problema de la derivación tradicional es su coincidencia ciega de todos los sucesos en los documentos. Para la librería de consultas, una transformación que permite que las tiendas variantes coincidan causará que cada aparición de tiendas en el documento sea tratada equivalente a la tienda de términos de consulta. Por lo tanto, se combinará un documento que contenga el fragmento que lee un libro en las tiendas de café, lo que provocará que se seleccionen muchos documentos incorrectos. Aunque esperamos que la función de clasificación pueda manejarlos correctamente, con muchos más candidatos para clasificar, aumenta el riesgo de cometer errores. Para aliviar estos dos problemas, proponemos un enfoque derivado del contexto para la búsqueda web. Nuestra solución consta de dos análisis sensibles al contexto, uno en el lado de la consulta y el otro en el lado del documento. En el lado de la consulta, proponemos un enfoque basado en modelado de lenguaje estadístico para predecir qué variantes de palabras son mejores formas que la palabra original para fines de búsqueda y expandir la consulta con solo esos formularios. En el lado del documento, proponemos una coincidencia sensible al contexto conservador para las variantes de palabras transformadas, solo ocurrencias de documentos coincidentes en el contexto de otros términos en la consulta. Nuestro modelo es simple pero efectivo y eficiente, lo que hace que sea factible ser utilizado en motores de búsqueda web comerciales reales. Utilizamos el manejo de la pluralización como un ejemplo de ejecución para nuestro enfoque de Stemming. La motivación para usar el manejo de la pluralización como ejemplo es mostrar que incluso tal simple tallo, si se maneja correctamente, puede dar beneficios significativos para la relevancia de búsqueda. Hasta donde sabemos, ninguna investigación previa ha investigado sistemáticamente el uso de la pluralización en la búsqueda web. Como tenemos que señalar, el método que proponemos no se limita al manejo de la pluralización, es una técnica general de Stemming y también se puede aplicar a la expansión general de la consulta. Los experimentos sobre votación general producen mejoras significativas adicionales sobre el manejo de la pluralización para consultas largas, aunque no se informarán detalles en este documento. En el resto del documento, primero presentamos el trabajo relacionado y distinguimos nuestro método del trabajo anterior en la Sección 2. Describimos los detalles del enfoque de derivación sensible al contexto en la Sección 3. Luego realizamos experimentos extensos en un motor de búsqueda web importante para respaldar nuestras afirmaciones en la Sección 4, seguidas de discusiones en la Sección 5. Finalmente, concluimos el documento en la Sección 6. 2. El trabajo relacionado con talla es una tecnología estudiada desde hace mucho tiempo. Se han desarrollado muchos taladros, como el Lovins Stemmer [16] y el Porter Stemmer [18]. El Porter Stemmer se usa ampliamente debido a su simplicidad y efectividad en muchas aplicaciones. Sin embargo, el Porter Stemming comete muchos errores porque sus reglas simples no pueden describir completamente la morfología inglesa. El análisis de corpus se usa para mejorar Porter Stemmer [26] mediante la creación de clases de equivalencia para palabras que son morfológicamente similares y ocurren en un contexto similar medido por la información mutua esperada [23]. Utilizamos un enfoque similar basado en el corpus para la derivación calculando la similitud entre dos palabras en función de sus características de contexto de distribución que pueden ser más que solo palabras adyacentes [15], y luego solo mantener las palabras morfológicamente similares a los candidatos. El uso de Stemming en la recuperación de la información también es una técnica bien conocida [8, 10]. Sin embargo, se informó previamente que la efectividad de los sistemas de consultas de la consulta en inglés era bastante limitada. Lennon et al.[17] comparó los algoritmos Lovins y Porter y encontraron poca mejora en el rendimiento de la recuperación. Más tarde, Harman [9] compara tres técnicas generales de derivación en los experimentos de recuperación de texto, incluida la entrega de pluralización (llamado S Stemmer en el documento). También propusieron una votación selectiva basada en la longitud de la consulta y la importancia del término, pero no se informaron resultados positivos. Por otro lado, Krovetz [14] realizó comparaciones sobre un pequeño número de documentos (de 400 a 12k) y mostró una mejora dramática de precisión (hasta 45%). Sin embargo, debido al número limitado de consultas probadas (menos de 100) y el pequeño tamaño de la colección, los resultados son difíciles de generalizar para la búsqueda web. Estos resultados mixtos, en su mayoría fallas, llevaron a los primeros investigadores IR a considerar irrelevantes en general para el inglés [4], aunque las investigaciones recientes han demostrado que las vistas tienen mayores beneficios para la recuperación en otros idiomas [2]. Sospechamos que las fallas anteriores se debieron principalmente a los dos problemas que mencionamos en la introducción. La derivación ciega, o una simple derecha selectiva basada en la longitud de consulta, como se usa en [9], no es suficiente. Se debe decidir sobre el caso por caso, no solo en el nivel de consulta sino también a nivel de documento. Como mostraremos, si se maneja correctamente, se puede lograr una mejora significativa. Un problema más general relacionado con la reformulación de la consulta [3, 12] y la expansión de la consulta que amplía las palabras no solo con las variantes de palabras [7, 22, 24, 25]. Para decidir qué palabras ampliadas usar, las personas a menudo usan la técnica de retroalimentación de pseudorelevancia, envíe la consulta original a un motor de búsqueda y recupere los documentos superiores, extraiga palabras relevantes de estos documentos superiores como palabras de consulta adicionales y vuelva a enviar la consulta ampliada nuevamente [21]. Esto normalmente requiere enviar una consulta varias veces al motor de búsqueda y no es rentable para procesar la gran cantidad de consultas involucradas en la búsqueda web. Además, la expansión de la consulta, incluida la reformulación de consultas [3, 12], tiene un alto riesgo de cambiar la intención del usuario (llamada deriva de consulta). Dado que las palabras expandidas pueden tener diferentes significados, agregarlos a la consulta podría cambiar la intención de la consulta original. Por lo tanto, la expansión de la consulta basada en pseudorelevancia y reformulación de consultas puede proporcionar sugerencias a los usuarios para el refinamiento interactivo, pero difícilmente se puede usar directamente para la búsqueda web. Por otro lado, Stemming es mucho más conservador ya que la mayoría de las veces, STEMMing conserva la intención de búsqueda original. Si bien la mayoría del trabajo en la expansión de la consulta se centra en la mejora del recuerdo, nuestro trabajo se centra en aumentar el recuerdo y la precisión. El aumento en el retiro es obvio. Con la calidad de la calidad, buenos documentos que no fueron seleccionados antes de que sean empujados y esos documentos de baja calidad se degraden. En la expansión selectiva de consultas, Cronen-Townsend et al.[6] propuso un método para la expansión de la consulta selectiva basada en comparar la divergencia de Kullback-Leiber de los resultados de la consulta no expandida y los resultados de la consulta expandida. Esto es similar a la retroalimentación de relevancia en el sentido de que requiere una recuperación de múltiples pases. Si una palabra se puede ampliar en varias palabras, requiere ejecutar este proceso varias veces para decidir qué palabra expandida es útil. Es costoso implementar esto en los motores de búsqueda web de producción. Nuestro método predice la calidad de la expansión basada en información sobre la información sin enviar la consulta a un motor de búsqueda. En resumen, proponemos un enfoque novedoso para atacar un problema antiguo, pero aún importante y desafiante para la búsqueda en la web, detrás. Nuestro enfoque es único en el sentido de que realiza la derivación predictiva sobre una base de consulta sin retroalimentación de relevancia de la Web, utilizando el contexto de las variantes en los documentos para preservar la precisión. Es simple, pero muy eficiente y efectivo, lo que hace que el tiempo real sea factible para la búsqueda web. Nuestros resultados afirmarán a los investigadores de que Stemming es muy importante para la recuperación de información a gran escala.3. Context Sensitive Stemming 3.1 Descripción general Nuestro sistema tiene cuatro componentes como se ilustra en la Figura 1: Generación de candidatos, segmentación de consultas y detección de palabras de cabeza, consulta sensible al contexto de consulta y coincidencia de documentos sensibles al contexto. La generación de candidatos (componente 1) se realiza o ﬄ ine y los candidatos generados se almacenan en un diccionario. Para una consulta de entrada, la primera consulta segmentaremos en conceptos y detectamos la palabra principal para cada concepto (componente 2). Luego usamos el modelado de lenguaje estadístico para decidir si una variante particular es útil (componente 3) y, finalmente, para las variantes expandidas, realizamos una coincidencia de documentos contextual (componente 4). A continuación discutimos cada uno de los componentes con más detalle. Componente 4: Consulta de entrada de coincidencia del documento sensible al contexto: y el componente de detección de palabras de la cabeza 2: Componente del segmento 1: Comparación de generación de candidatos −> Comparación de comparación 3: Decisión selectiva de expansión de palabras: comparaciones -> Ejemplo de comparación: Comparación de precios del hotel Salida: Hotel Hotel Hotel> Hoteles Figura 1: Componentes del sistema 3.2 Generación de candidatos de expansión Una de las formas de generar candidatos es usar el Porter Stemmer [18]. El Porter Stemmer simplemente usa reglas morfológicas para convertir una palabra en su forma base. No tiene conocimiento del significado semántico de las palabras y, a veces, comete serios errores, como ejecutivo para la ejecución, noticias a nuevas y pegar al pasado. Una forma más conservadora se basa en el uso del análisis de corpus para mejorar los resultados de Porter Stemmer [26]. El análisis del corpus que hacemos se basa en la similitud de distribución de palabras [15]. La justificación del uso de similitud de palabras de distribución es que las variantes verdaderas tienden a usarse en contextos similares. En el cálculo de similitud de palabras de distribución, cada palabra se representa con un vector de características derivado del contexto de la palabra. Utilizamos los BigRams a la izquierda y a la derecha de la palabra como características de contexto, minando un gran corpus web. La similitud entre dos palabras es la similitud cosena entre los dos vectores de características correspondientes. Las 20 palabras similares al desarrollo se muestran en la siguiente tabla.Rank Candidate Puntuación de rango Puntuación candidata 0 Desarrollar 1 10 Berts 0.119 1 Desarrollo de 0.339 11 Wads 0.116 2 Desarrollado 0.176 12 Desarrollador 0.107 3 Incubador 0.160 13 Promoción de 0.100 4 Desarrolla 0.150 14 Desarrollo 0.091 5 Desarrollo 0.148 15 Reengineing 0.090 6 Tutoría 0.138 16 Construir 0.083 7 Analización de análisis Analizante0.128 17 Construya 0.081 8 Desarrollo 0.128 18 Educational 0.081 9 Automatización 0.126 19 Instituto 0.077 Tabla 1: Los 20 candidatos más similares a Word se desarrollan. La puntuación de la columna es la puntuación de similitud. Para determinar a los candidatos de Stemming, aplicamos algunas reglas morfológicas de Porter Stemmer [18] a la lista de similitud. Después de aplicar estas reglas, para el desarrollo de la palabra, los candidatos derivados se desarrollan, desarrollan, desarrollan, desarrollo, desarrollo, desarrollador, desarrollo. Para el propósito de manejo de pluralización, solo se retiene el candidato. Una cosa que notamos al observar las palabras distributionalmente similares es que están estrechamente relacionadas semánticamente. Estas palabras pueden servir como candidatos para la expansión general de la consulta, un tema que investigaremos en el futuro.3.3 Segmentación e identificación de palabras de cabeza Para consultas largas, es bastante importante detectar los conceptos en la consulta y las palabras más importantes para esos conceptos. Primero dividimos una consulta en segmentos, cada segmento que representa un concepto que normalmente es una frase nominal. Para cada una de las frases sustantivas, detectamos la palabra más importante que llamamos la palabra de la cabeza. La segmentación también se usa en la coincidencia sensible del documento (Sección 3.5) para hacer cumplir la proximidad. Para romper una consulta en segmentos, tenemos que definir un criterio para medir la fuerza de la relación entre palabras. Un método efectivo es usar información mutua como indicador sobre si dividir o no dos palabras [19]. Utilizamos un registro de consultas de 25 m y recolectamos las frecuencias BigRam y unigram de él. Para cada consulta entrante, calculamos la información mutua de dos palabras adyacentes;Si pasa un umbral predefinido, no dividimos la consulta entre esas dos palabras y pasamos a la siguiente palabra. Continuamos este proceso hasta que la información mutua entre dos palabras está por debajo del umbral, luego creamos un límite de concepto aquí. La Tabla 2 muestra algunos ejemplos de segmentación de consultas. La forma ideal de encontrar la palabra principal de un concepto es hacer un análisis sintáctico para determinar la estructura de dependencia de la consulta. El análisis de consultas es más difícil que la oración [zapatilla de carrera] [mejor] [Nueva York] [Escuelas de medicina] [imágenes] [de] [Casa Blanca] [Cookies] [en] [San Francisco] [Hotel] [Comparación de precios] Table Table Table2: Segmentación de consulta: un segmento es entre paréntesis.Analizado ya que muchas consultas no son gramaticales y son muy cortas. Aplicar un analizador capacitado en oraciones de documentos a consultas tendrá un bajo rendimiento. En nuestra solución, solo utilizamos reglas de heurística simples, y funciona muy bien en la práctica para el inglés. Para una frase nominal inglesa, la palabra de la cabeza es típicamente la última palabra sin parar, a menos que la frase sea de un patrón particular, como xyz de/in/at/from UVW. En tales casos, la palabra principal es típicamente la última palabra sin parar de XYZ.3.4 Expansión de palabras sensibles al contexto Después de detectar qué palabras son las palabras más importantes para expandir, tenemos que decidir si las expansiones serán útiles. Nuestras estadísticas muestran que aproximadamente la mitad de las consultas pueden transformarse mediante pluralización a través de ingenuos derivaciones. Entre esta mitad, aproximadamente el 25% de las consultas mejoran la relevancia cuando se transforman, la mayoría (aproximadamente el 50%) no cambia sus 5 resultados principales, y el 25% restante funciona peor. Por lo tanto, es extremadamente importante identificar qué consultas no deben ser derivadas con el fin de maximizar la mejora de la relevancia y minimizar el costo de los siglos. Además, para una consulta con múltiples palabras que se pueden transformar, o una palabra con múltiples variantes, no todas las expansiones son útiles. Tomando la comparación de precios de la consulta como ejemplo, decidimos que la comparación de hoteles y precios son dos conceptos. El hotel y la comparación de palabras de cabeza se pueden ampliar a los hoteles y las comparaciones. ¿Son útiles ambas transformaciones? Para probar si una expansión es útil, tenemos que saber si es probable que la consulta ampliada obtenga documentos más relevantes de la web, que puede cuantificarse por la probabilidad de que la consulta ocurra como una cadena en la web. La mayor probabilidad de que ocurra una consulta en la web, más documentos relevantes esta consulta puede devolver. Ahora todo el problema se convierte en cómo calcular la probabilidad de que ocurra la consulta en la web. Calcular la probabilidad de que ocurra una cadena en un corpus es un problema de modelado de idiomas bien conocido. El objetivo del modelado de idiomas es predecir la probabilidad de secuencias de palabras naturales, s = w1w2 ... wn;o más simplemente, para poner una alta probabilidad en las secuencias de palabras que realmente ocurren (y la baja probabilidad de las secuencias de palabras que nunca ocurren). El enfoque más simple y exitoso para el modelado de idiomas todavía se basa en el modelo N-Gram. Por la regla de probabilidad de la cadena, se puede escribir la probabilidad de cualquier secuencia de palabras como PR (W1W2 ... Wn) = NY I = 1 PR (WI | W1 ... WI-1) (1) Un modelo N-Gram se aproximaEsta probabilidad suponiendo que las únicas palabras relevantes para predecir PR (wi | w1 ... wi - 1) son las palabras n - 1 anteriores;es decir. Pr (wi | w1 ... wi-1) = pr (wi | wi-n+1 ... wi-1) Una estimación de máxima verosimilitud sencilla de las probabilidades de N-gram de un corpus está dada por la frecuencia observada de cada unode los patrones pr (wi | wi - n+1 ... wi - 1) = #(wi - n+1 ... wi) #(wi - n+1 ... wi - 1) (2) donde#(.) Denota el número de ocurrencias de un gramo especificado en el corpus de entrenamiento. Aunque uno podría intentar usar modelos simples de N-Gram para capturar dependencias de largo alcance en el lenguaje, intentar hacerlo directamente crea inmediatamente problemas de datos dispersos: usar gramos de longitud hasta N implica estimar la probabilidad de eventos WN, donde W es del tamañode la palabra vocabulario. Esto rápidamente abruma los recursos computacionales y de datos modernos para opciones incluso modestas de N (más allá de 3 a 6). Además, debido a la naturaleza de cola pesada del lenguaje (es decir, Ley ZIPFS) Es probable que se encuentre con nuevos N-Grams que nunca fueron presenciados durante la capacitación en ningún corpus de prueba, y por lo tanto, algún mecanismo para asignar una probabilidad no cero a NEVE NEWN N-Grams es un tema central e inevitable en el modelado de lenguaje estadístico. Un enfoque estándar para suavizar las estimaciones de probabilidad para hacer frente a los problemas de datos dispersos (y hacer frente a los N-gramos potencialmente faltantes) es utilizar algún tipo de estimador de retroceso. Pr (wi | wi - n+1 ... wi - 1) = 8 >> <>>: ˆpr (wi | wi - n+1 ... wi - 1), si #(wi - n+1...wi)> 0 β (wi - n+1 ... wi - 1) × pr (wi | wi - n+2 ... wi - 1), de lo contrario (3) donde ˆpr (wi | wi - n+1 ... wi - 1) = descuento #(wi - n+1 ... wi) #(wi - n+1 ... wi - 1) (4) es la probabilidad y β con descuento (wi - n+1 ... wi - 1) es una constante de normalización β (wi - n+1 ... wi - 1) = 1 - x x∈ (wi - n+1 ... wi - 1x) ˆpr (x |wi - n+1 ... wi - 1) 1 - x x∈ (wi - n+1 ... wi - 1x) ˆpr (x | wi - n+2 ... wi - 1) (5) elLa probabilidad con descuento (4) se puede calcular con diferentes técnicas de suavizado, que incluyen suavizado absoluto, suavizado de buen turbulento, suavizado lineal y suavizado de pellado de Witten [5]. Utilizamos suavizado absoluto en nuestros experimentos. Dado que la probabilidad de una cadena, PR (W1W2 ... WN), es un número muy pequeño y difícil de interpretar, usamos la entropía como se define a continuación a continuación para calificar la cadena. Entropía = - 1 n log2 pr (w1w2 ... wn) (6) Ahora vuelve al ejemplo de las comparaciones de precios del hotel de consulta, hay cuatro variantes de esta consulta, y la entropía de estos cuatro candidatos se muestra en la Tabla 3. Podemos ver que todas las alternativas son menos probables que la consulta de entrada. Por lo tanto, no es útil hacer una expansión para esta consulta. Por otro lado, si la consulta de entrada es comparaciones de precios del hotel, que es la segunda alternativa en la tabla, entonces hay una mejor alternativa que la consulta de entrada y, por lo tanto, debe ampliarse. Para tolerar las variaciones en la estimación de probabilidad, relajamos el criterio de selección a esas alternativas de consulta si sus puntajes están a cierta distancia (10% en nuestros experimentos) a la mejor puntuación. Variaciones de consulta Entropía Comparación de precios del hotel 6.177 Comparaciones de precios del hotel 6.597 Hoteles Comparación de precios 6.937 Comparaciones de precios de hoteles 7.360 Tabla 3: Variaciones de la consulta Comparación de precios del hotel clasificado por puntaje de entropía, con la consulta original en negrita.3.5 COMPORTACIÓN DE DOCUMENTES SENTICIONALES DEL CONTEXTO Incluso después de saber qué variantes de palabras probablemente sean útiles, debemos ser conservadores en la coincidencia de documentos para las variantes expandidas. Para las comparaciones de precios de la consulta, decidimos que las comparaciones de palabras se amplían para incluir la comparación. Sin embargo, no todas las ocurrencias de comparación en el documento son de interés. Una página que se trata de comparar el servicio al cliente puede contener todas las palabras comparaciones de precios del hotel. Esta página no es una buena página para la consulta. Si aceptamos coincidencias de cada ocurrencia de comparación, dañará la precisión de la recuperación y esta es una de las principales razones por las cuales la mayoría de los enfoques de Stemming no funcionan bien para la recuperación de la información. Para abordar este problema, tenemos una restricción de proximidad que considera el contexto en torno a la variante expandida en el documento. Una coincidencia variante se considera válida solo si la variante ocurre en el mismo contexto que la palabra original. El contexto es los segmentos sin parar izquierdo o derecho 1 de la palabra original. Tomando la misma consulta como ejemplo, el contexto de las comparaciones es el precio. La comparación de palabras ampliada solo es válida si está en el mismo contexto de comparaciones, que es después del precio de las palabras. Por lo tanto, solo debemos coincidir con esas ocurrencias de comparación en el documento si ocurren después del precio de la palabra. Teniendo en cuenta el hecho de que las consultas y los documentos pueden no representar la intención exactamente de la misma manera, relajamos esta restricción de proximidad para permitir ocurrencias de variantes dentro de una ventana de algún tamaño fijo. Si la comparación de palabras ampliada ocurre dentro del contexto del precio dentro de una ventana, se considera válida. Cuanto más pequeño sea el tamaño de la ventana, más restrictivo es la coincidencia. Utilizamos un tamaño de ventana de 4, que generalmente captura contextos que incluyen las frases nominal de contención y adyacentes.4. Evaluación experimental 4.1 Métricas de evaluación Mediremos tanto la mejora de la relevancia como el costo vistoso requerido para lograr la relevancia.1 Un segmento de contexto no puede ser una sola palabra de parada.4.1.1 Medición de relevancia Utilizamos una variante de la ganancia acumulativa promedio con descuento (DCG), un esquema recientemente popularizado para medir la relevancia del motor de búsqueda [1, 11]. Dada una consulta y una lista clasificada de documentos k (k se establece en 5 en nuestros experimentos), la puntuación DCG (k) para esta consulta se calcula de la siguiente manera: DCG (k) = kx k = 1 GK log2 (1 + k).(7) donde GK es el peso para el documento en el rango k.Un mayor grado de relevancia corresponde a un peso más alto. Una página se califica en una de las cinco escalas: perfecta, excelente, buena, justa, mala, con pesas correspondientes. Usamos DCG para representar el DCG promedio (5) sobre un conjunto de consultas de prueba.4.1.2 Costo de Stemming Otra métrica es medir el costo adicional incurrido por Stemming. Dado el mismo nivel de mejora de relevancia, preferimos un método de punto de la derivación que tiene menos costo adicional. Medimos esto por el porcentaje de consultas que en realidad se sienten con votación, sobre todas las consultas que posiblemente podrían ser de STEMMed.4.2 Preparación de datos Muestra aleatoriamente 870 consultas de un registro de consultas de tres meses, con 290 de cada mes. Entre todas estas 870 consultas, eliminamos todas las consultas mal escritas ya que las consultas mal escritas no son de interés para las derivaciones. También eliminamos todas las consultas de una palabra, ya que las consultas de una palabra de las palabras sin contexto tienen un alto riesgo de cambiar la intención de consultas, especialmente para palabras cortas. Al final, tenemos 529 consultas deletreadas correctamente con al menos 2 palabras.4.3 ingenuo derivado para la búsqueda web Antes de explicar los experimentos y los resultados en detalle, a Wed como para describir la forma tradicional de usar STEMMing para la búsqueda web, denominada modelo ingenuo. Esto es para tratar cada variante de palabras equivalente para todas las palabras posibles en la consulta. La librería de la consulta se transformará en (libros o libros) (tiendas o tiendas) al limitar solo el manejamiento de la pluralización, donde o es un operador que denota la equivalencia de los argumentos de izquierda y derecha.4.4 Configuración experimental El modelo de línea de base es el modelo sin STEMMing. Primero ejecutamos el modelo ingenuo para ver qué tan bien funciona sobre la línea de base. Luego mejoramos el modelo de vástago ingenuo mediante la coincidencia sensible del documento, denominado modelo de coincidencia sensible al documento. Este modelo hace que el mismo modelo sea el modelo ingenuo en el lado de la consulta, pero realiza una coincidencia conservadora en el lado del documento utilizando la estrategia descrita en la Sección 3.5. El modelo ingenuo y el modelo de coincidencia sensible al documento paran la mayoría de las consultas. De las 529 consultas, hay 408 consultas que surgen, correspondientes al 46.7% de tráfico de consultas (de un total de 870). Luego mejoramos aún más el modelo de coincidencia confidencial del documento del lado de la consulta con una palabra selectiva derivada basada en el modelado de lenguaje estadístico (Sección 3.4), denominado modelo selectivo de Stemming. Según la predicción del modelado de idiomas, este modelo se detiene solo un subconjunto de las consultas 408 vistas por el modelo de coincidencia sensible al documento. Experimentamos con el modelo de lenguaje unigram y el modelo de lenguaje BigRam. Dado que solo nos importa cuánto podemos mejorar el modelo ingenuo, solo usaremos estas 408 consultas (todas las consultas que se ven afectadas por el modelo de derecha ingenuo) en los experimentos. Para tener una idea de cómo funcionan estos modelos, también tenemos un modelo Oracle que le da al rendimiento superior que un STEMMER puede lograr en estos datos. El modelo Oracle solo expande una palabra si la derivación dará mejores resultados. Para analizar la influencia del manejo de la pluralización en diferentes categorías de consultas, dividimos consultas en consultas cortas y consultas largas. Entre las 408 consultas surgidas por el modelo ingenuo, hay 272 consultas cortas con 2 o 3 palabras, y 136 consultas largas con al menos 4 palabras.4.5 Resultados Resumimos los resultados generales en la Tabla 4 y presentamos los resultados en consultas cortas y consultas largas por separado en la Tabla 5. Cada fila de la Tabla 4 es una estrategia de Stemming descrita en la Sección 4.4. La primera columna es el nombre de la estrategia. La segunda columna es el número de consultas afectadas por esta estrategia;Esta columna mide el costo de votación, y los números deben ser bajos para el mismo nivel de DCG. La tercera columna es la puntuación de DCG promedio en todas las consultas probadas en esta categoría (incluidas las que no fueron detectadas por la estrategia). La cuarta columna es la mejora relativa sobre la línea de base, y la última columna es el valor p de la prueba de significancia de Wilcoxon. Hay varias observaciones sobre los resultados. Podemos ver que la ingenuidad de Stemming solo obtiene una mejora estadísticamente insignificante de 1.5%. Mirando la Tabla 5, ofrece una mejora del 2.7% en consultas cortas. Sin embargo, también perjudica consultas largas en -2.4%. En general, la mejora se cancela. La razón por la que mejora consultas cortas es que la mayoría de las consultas cortas solo tienen una palabra que puede ser derivada. Por lo tanto, pluralizar ciegamente consultas cortas es relativamente segura. Sin embargo, para consultas largas, la mayoría de las consultas pueden tener múltiples palabras que pueden ser pluralizadas. Expandirlos a todos sin selección dañará significativamente la precisión. El contexto del documento Sensitive Stemming da un aumento significativo al rendimiento, de 2.7% a 4.2% para consultas cortas y de -2.4% a -1.6% para consultas largas, con un aumento general de 1.5% a 2.8%. La mejora proviene de la coincidencia de documentos sensible al contexto conservador. Una palabra ampliada es válida solo si ocurre dentro del contexto de la consulta original en el documento. Esto reduce muchos partidos espurios. Sin embargo, todavía notamos que para consultas largas, la derecha sensible al contexto no puede mejorar el rendimiento porque todavía selecciona demasiados documentos y le da a la función de clasificación un problema difícil. Si bien el tamaño de la ventana elegido de 4 funciona mejor entre todas las opciones, todavía permite coincidencias espurias. Es posible que el tamaño de la ventana deba ser elegido por consulta para garantizar restricciones de proximidad más estrictas para diferentes tipos de frases de sustantivos. La pluralización de palabras selectivas ayuda aún más a resolver el problema que enfrenta el contexto del documento sensible a la derivación. No detiene todas las palabras que colocan toda la carga en el algoritmo de clasificación, pero trata de eliminar innecesarias las derivaciones innecesarias en primer lugar. Al predecir qué variantes de palabras van a ser útiles, podemos reducir drásticamente el número de palabras de tallo, mejorando así tanto el recuerdo como la precisión. Con el modelo de lenguaje unigram, podemos reducir el costo de las medidas en un 26.7% (de 408/408 a 300/408) y levantar la mejora general de DCG de 2.8% a 3.4%. En particular, ofrece mejoras significativas en consultas largas. La ganancia de DCG se convierte de negativa a positiva, de −1.6% a 1.1%. Esto confirma nuestra hipótesis de que reducir la expansión innecesaria de palabras conduce a una mejora de precisión. Para consultas cortas también, observamos tanto la mejora de DCG como la reducción de costos con el modelo de idioma unigram. Las ventajas de la expansión de palabras predictivas con un modelo de idioma se impulsan aún más con un mejor modelo de idioma BigRam. La ganancia general de DCG se eleva del 3.4% al 3.9%, y el costo de las medidas se reduce drásticamente de 408/408 a 250/408, correspondiente a solo el 29% del tráfico de consultas (250 de 870) y una mejora general de 1.8% DCG en generalTodo el tráfico de consulta. Para consultas cortas, el modelo de lenguaje BigRam mejora la ganancia de DCG de 4.4% a 4.7%, y reduce el costo de 272/272 a 150/272. Para consultas largas, el modelo de lenguaje BigRam mejora la ganancia de DCG de 1.1% a 2.5%, y reduce el costo de edad de 136/136 a 100/136. Observamos que el modelo de lenguaje BigRam ofrece un ascensor más grande para consultas largas. Esto se debe a que la incertidumbre en consultas largas es más grande y se necesita un modelo de lenguaje más poderoso. Presumimos que un modelo de lenguaje Trigram daría un mayor ascensor para consultas largas y dejaría esto para futuras investigaciones. Teniendo en cuenta el ajuste de 2 en la parte superior en la mejora que se obtendrá del manejo de la pluralización (a través del modelo Oracle), el rendimiento actual en consultas cortas es muy satisfactorio. Para consultas cortas, la ganancia DCG superior es del 6.3% para el manejo perfecto de la pluralización, nuestra ganancia actual es del 4.7% con un modelo de idioma BigRam. Para consultas largas, la ganancia DCG superior es del 4.6% para el manejo perfecto de la pluralización, nuestra ganancia actual es del 2.5% con un modelo de idioma BigRam. Podemos obtener un beneficio adicional con un modelo de idioma más poderoso para consultas largas. Sin embargo, las dificultades de consultas largas provienen de muchos otros aspectos, incluida la proximidad y el problema de segmentación. Estos problemas deben abordarse por separado. Mirando la reducción superior de la reducción de la cabeza para Oracle Stemming, el 75% (308/408) de los tallos ingenuos son un desperdicio. Actualmente capturamos aproximadamente la mitad de ellos. Una reducción adicional de la sobrecarga requiere sacrificar la ganancia de DCG. Ahora podemos comparar las estrategias derivadas de un aspecto diferente. En lugar de observar la influencia sobre todas las consultas como describimos anteriormente, la Tabla 6 resume las mejoras de DCG solo sobre las consultas afectadas. Podemos ver que el número de consultas afectadas disminuye a medida que la estrategia de Stemming se vuelve más precisa (mejora de DCG). Para el modelo de idioma BigRam, durante las consultas de stemmed 250/408, la mejora de DCG es del 6.1%. Una observación interesante es que el DCG promedio disminuye con un mejor modelo, lo que indica una mejor estrategia de Stemming Salta consultas más difíciles (consultas bajas de DCG).5. Discusiones 5.1 Modelos de lenguaje de la consulta frente a la web Como mencionamos en la Sección 1, estamos tratando de predecir la probabilidad de que una cadena ocurra en la web. El modelo de idioma debe describir la aparición de la cadena en la web. Sin embargo, el registro de consultas también es un buen recurso.2 Tenga en cuenta que este conflicto superior es solo para el manejo de la pluralización, no para el punto de vista general. General Stemming da un 8% de fusión superior, que es bastante sustancial en términos de nuestras métricas. Consultas afectadas DCG DCG Mejora de valor P-Valor P-Valor 0/408 7.102 N/A N/A Modelo ingenuo 408/408 7.206 1.5% 0.22 Contexto del documento Contexto Sensitivo Modelo 408/408 7.302 2.8% 0.014 Modelo selectivo: Unigram LM 300/408 7.321 3.4% 0.001Modelo selectivo: BigRam LM 250/408 7.381 3.9% 0.001 Oracle Model 100/408 7.519 5.9% 0.001 Tabla 4: Comparación de resultados de diferentes estrategias de vástago sobre todas las consultas afectadas por ingenuos resultados de consultas cortas afectadas por las consultas de DCG P-Value P-Value 0/////////////////////////////////////////////272 N/A N/A Modelo ingenuo 272/272 2.7% 0.48 Documento Contexto Sensitivo Modelo 272/272 4.2% 0.002 Modelo selectivo: Unigram LM 185/272 4.4% 0.001 Modelo selectivo: BigRam LM 150/272 4.7% 0.001 Oracle Model 71/272 6.3% 0.001 Resultados de consultas largas afectadas consultas DCG DCG Valor de P -Baselija 0/136 N/A N/A Modelo ingenuo 136/136 -2.4% 0.25 Documento Contexto Sensitivo Modelo 136/136 -1.6% 0.27 Modelo selectivo: Unigram LM 115/136 1.1% 0.001 Modelo selectivo: BigRam LM 100/136 2.5% 0.001 Oracle Model 29/136 4.6% 0.001 Tabla 5: Comparación de resultados de diferentes estrategias de Stemming consultas cortas y de consultas largas reformulan una consulta utilizando muchas variantes diferentes para obtener buenos resultados para obtener buenos resultados para obtener buenos resultados para obtener buenos resultados. Para probar la hipótesis de que podemos aprender probabilidades de transformación confiables del registro de consultas, capacitamos un modelo de lenguaje de las mismas consultas de la misma consulta Top 25m que se usa para aprender segmentación, y lo usamos para predicción. Observamos una ligera disminución del rendimiento en comparación con el modelo capacitado en frecuencias web. En particular, el rendimiento para unigram LM no se vio afectado, pero la ganancia de DCG para BigRam LM cambió de 4.7% a 4.5% para consultas cortas. Por lo tanto, el registro de consultas puede servir como una buena aproximación de las frecuencias web.5.2 Cómo la lingüística ayuda a algunos conocimientos lingüísticos es útil para las vistas. Para el caso de manejo de pluralización, la pluralización y la desluralización no son simétricas. Una palabra plural utilizada en una consulta indica una intención especial. Por ejemplo, la consulta de New York Hotels está buscando una lista de hoteles en Nueva York, no el hotel específico de Nueva York que podría ser un hotel ubicado en California. Una simple equivalencia del hotel a los hoteles podría impulsar una página en particular sobre el hotel de Nueva York hasta el mejor rango. Para capturar esta intención, tenemos que asegurarnos de que el documento sea una página general sobre hoteles en Nueva York. Hacemos esto al requerir que los hoteles de la palabra plural aparezcan en el documento. Por otro lado, convertir una palabra singular a plural es más seguro ya que una página de propósito general normalmente contiene información específica. Observamos una ligera disminución general de DCG, aunque no estadísticamente significativa, para el contexto del documento sensible a la derivación si no consideramos esta propiedad asimétrica.5.3 Análisis de errores Un tipo de errores que notamos, aunque raro pero gravemente perjudicial, la relevancia, es el cambio de intención de búsqueda después de la derivación. En términos generales, la pluralización o la defluiización mantienen la intención original. Sin embargo, la intención podría cambiar en algunos casos. Para un ejemplo de tal consulta, trabajo en Apple, pluralizamos el trabajo a los trabajos. Este Stemming hace que la consulta original sea ambigua. El trabajo de consulta o trabajos en Apple tiene dos intentos. Una es las oportunidades de empleo en Apple, y otra es una persona que trabaja en Apple, Steve Jobs, quien es el CEO y cofundador de la compañía. Por lo tanto, los resultados posteriores a la consulta devuelven Steve Jobs como uno de los resultados en el top 5. Una solución es realizar un análisis basado en el conjunto de resultados para verificar si la intención se cambia. Esto es similar a la retroalimentación de relevancia y requiere una clasificación de segunda fase. Un segundo tipo de errores es el problema de reconocimiento de entidad/concepto, estos incluyen dos tipos. Una es que la variante de palabra Stemmed ahora coincide con parte de una entidad o concepto. Por ejemplo, las galletas de consulta en San Francisco están pluralizadas para galletas o galletas en San Francisco. Los resultados coincidirán con el frasco de galletas en San Francisco. Aunque Cookie todavía significa lo mismo que las galletas, el frasco de galletas es un concepto diferente. Otro tipo es que la palabra inexplicable coincide con una entidad o concepto debido a la derivación de las otras palabras. Por ejemplo, cita el hielo se pluralizan para citar o cita el hielo. La intención original para esta consulta es buscar cotización de existencias para Ticker Ice. Sin embargo, notamos que entre los mejores resultados, uno de los resultados son las citas de alimentos: helado. Esto coincide con consultas afectadas DCG New DCG DCG Mejora DCG Modelo ingenuo 408/408 7.102 7.206 1.5% Documento Contexto Sensible Modelo 408/408 7.102 7.302 2.8% Modelo selectivo: Unigram LM 300/408 5.904 6.187 4.8% Selective Model: BigRam LM250/408 5.551 5.891 6.1% Tabla 6: Comparación de resultados solo sobre las consultas con tallo: la columna antigua/nueva DCG es la puntuación DCG sobre las consultas afectadas antes/después de aplicar las citas de palabras pluralizadas. La palabra de hielo sin cambios coincide con parte del helado de frase nominal aquí. Para resolver este tipo de problema, tenemos que analizar los documentos y reconocer el frasco y el helado de galletas como conceptos en lugar de dos palabras independientes. Se produce un tercer tipo de errores en consultas largas. Para el software del lector de códigos de barras de consulta, se pluralizan dos palabras.código a códigos y lector a lectores. De hecho, el lector de códigos de barras en la consulta original es un concepto fuerte y las palabras internas no deben cambiarse. Este es la segmentación y el problema de detección de frases de entidad y sustantivo en consultas, que activamente estamos atacando. Para consultas largas, debemos identificar correctamente los conceptos en la consulta y aumentar la proximidad de las palabras dentro de un concepto.6. Conclusiones y trabajos futuros hemos presentado una forma simple pero elegante de detener la búsqueda web. Mejora la votación ingenua en dos aspectos: expansión selectiva de palabras en el lado de la consulta y la coincidencia conservadora de palabras en el lado del documento. Utilizando el manejo de la pluralización como ejemplo, los experimentos en un importante motor de búsqueda web muestran que mejora significativamente la relevancia web y reduce el costo de consumo. También mejora significativamente la tasa de clics web (detalles no informados en el documento). Para el trabajo futuro, estamos investigando los problemas que identificamos en la sección de análisis de errores. Estos incluyen: errores de coincidencia de la entidad y frase nominal y la segmentación mejorada.7. Referencias [1] E. Agichtein, E. Brill y S. T. Dumais. Mejora de la clasificación de búsqueda web incorporando información de comportamiento del usuario. En Sigir, 2006. [2] E. airio. Normalización de palabras y descomposición en IR mono y bilingüe. Recuperación de información, 9: 249-271, 2006. [3] P. Anick. Uso de comentarios terminológicos para el refinamiento de búsqueda web: un estudio basado en registros. En Sigir, 2003. [4] R. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. ACM Press/Addison Wesley, 1999. [5] S. Chen y J. Goodman. Un estudio empírico de las técnicas de suavizado para el modelado de idiomas. Informe técnico TR-10-98, Universidad de Harvard, 1998. [6] S. Cronen-Townsend, Y. Zhou y B. Croft. Un marco para la expansión de consultas selectivas. En Cikm, 2004. [7] H. Fang y C. Zhai. Matriota de término semántico en enfoques axiomáticos para la recuperación de información. En Sigir, 2006. [8] W. B. Frakes. Término de combinación para la recuperación de información. En C. J. Rijsbergen, editor, investigación y desarrollo en recuperación de información, páginas 383-389. Cambridge University Press, 1984. [9] D. Harman. ¿Qué tan efectivo es el sufijo? Jasis, 42 (1): 7-15, 1991. [10] D. Hull. Algoritmos de Stemming: un estudio de caso para una evaluación detallada. Jasis, 47 (1): 70-84, 1996. [11] K. Jarvelin y J. Kekalainen. Evaluación de evaluación basada en ganancias acumuladas de técnicas IR. ACM TOIS, 20: 422-446, 2002. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generación de sustituciones de consulta. En www, 2006. [13] W. Kraaij y R. Pohlmann. Visualización de la derivación como mejora del recuerdo. En Sigir, 1996. [14] R. Krovetz. Ver la morfología como un proceso de inferencia. En Sigir, 1993. [15] D. Lin. Recuperación automática y agrupación de palabras similares. En Coling-ACL, 1998. [16] J. B. Lovins. Desarrollo de un algoritmo Stemming. Traducción mecánica y lingüística computacional, II: 22-31, 1968. [17] M. Lennon y D. Peirce y B. Tarry y P. Willett. Una evaluación de algunos algoritmos de combinación para la recuperación de información. Journal of Information Science, 3: 177-188, 1981. [18] M. Porter. Un algoritmo para extracción sufijo. Programa, 14 (3): 130-137, 1980. [19] K. M. Risvik, T. Mikolajewski y P. Boros. Segmentación de consulta para la búsqueda web. En www, 2003. [20] S. E. Robertson. Selección de término para la expansión de la consulta. Journal of Documation, 46 (4): 359-364, 1990. [21] G. Salton y C. Buckley. Mejora del rendimiento de la recuperación por retroalimentación relevante. Jasis, 41 (4): 288 - 297, 1999. [22] R. Sun, C.-H.Ong y T.-S.Chua. Relaciones de dependencia minera para la expansión de la consulta en la recuperación del pasaje. En Sigir, 2006. [23] C. Van Rijsbergen. Recuperación de información. Butterworths, segunda versión, 1979. [24] B. V´Elez, R. Weiss, M. A. Sheldon y D. K. Gifford. Refinamiento de consulta rápido y efectivo. En Sigir, 1997. [25] J. Xu y B. Croft. Expansión de consulta utilizando análisis de documentos locales y globales. En Sigir, 1996. [26] J. Xu y B. Croft. Peque basado en el corpus utilizando la coocción de variantes de palabras. ACM TOIS, 16 (1): 61-81, 1998.",
    "original_sentences": [
        "Context Sensitive Stemming for Web Search Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo!",
        "Inc. 701 First Avenue Sunnyvale, California 94089 {fuchun, nawaaz, xinli, yumaol}@yahoo-inc.com ABSTRACT Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms.",
        "Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation.",
        "In this paper, we propose a context sensitive stemming method that addresses these two issues.",
        "Two unique properties make our approach feasible for Web Search.",
        "First, based on statistical language modeling, we perform context sensitive analysis on the query side.",
        "We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine.",
        "This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time.",
        "Second, our approach performs a context sensitive document matching for those expanded variants.",
        "This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision.",
        "Using word pluralization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic.",
        "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Storage and Retrieval-Query formulation General Terms Algorithms, Experimentation 1.",
        "INTRODUCTION Web search has now become a major tool in our daily lives for information seeking.",
        "One of the important issues in Web search is that user queries are often not best formulated to get optimal results.",
        "For example, running shoe is a query that occurs frequently in query logs.",
        "However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes.",
        "Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs.",
        "This is difficult even for experienced users, and especially difficult for non-native speakers.",
        "One traditional solution is to use stemming [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term.",
        "Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing.",
        "Stemming can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing.",
        "Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows handling each query differently, and hence is preferred.",
        "Although traditional stemming increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched.",
        "When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6].",
        "In addition, it reduces system performance because the search engine has to match all the word variants.",
        "As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa.",
        "Thus, one needs to be very cautious when using stemming in Web search engines.",
        "One problem of traditional stemming is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word.",
        "For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored.",
        "For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query.",
        "Transforming book store to match book stores is fine, but matching book storing or booking store is not.",
        "A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query.",
        "However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20].",
        "A second problem of traditional stemming is its blind matching of all occurrences in documents.",
        "For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store.",
        "Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected.",
        "Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases.",
        "To alleviate these two problems, we propose a context sensitive stemming approach for Web search.",
        "Our solution consists of two context sensitive analysis, one on the query side and the other on the document side.",
        "On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms.",
        "On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query.",
        "Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines.",
        "We use pluralization handling as a running example for our stemming approach.",
        "The motivation for using pluralization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance.",
        "As far as we know, no previous research has systematically investigated the usage of pluralization in Web search.",
        "As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion.",
        "Experiments on general stemming yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper.",
        "In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2.",
        "We describe the details of the context sensitive stemming approach in Section 3.",
        "We then perform extensive experiments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5.",
        "Finally, we conclude the paper in Section 6. 2.",
        "RELATED WORK Stemming is a long studied technology.",
        "Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18].",
        "The Porter stemmer is widely used due to its simplicity and effectiveness in many applications.",
        "However, the Porter stemming makes many mistakes because its simple rules cannot fully describe English morphology.",
        "Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23].",
        "We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates.",
        "Using stemming in information retrieval is also a well known technique [8, 10].",
        "However, the effectiveness of stemming for English query systems was previously reported to be rather limited.",
        "Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance.",
        "Later, Harman [9] compares three general stemming techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper).",
        "They also proposed selective stemming based on query length and term importance, but no positive results were reported.",
        "On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%).",
        "However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search.",
        "These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2].",
        "We suspect the previous failures were mainly due to the two problems we mentioned in the introduction.",
        "Blind stemming, or a simple query length based selective stemming as used in [9] is not enough.",
        "Stemming has to be decided on case by case basis, not only at the query level but also at the document level.",
        "As we will show, if handled correctly, significant improvement can be achieved.",
        "A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25].",
        "To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21].",
        "This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search.",
        "In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift).",
        "Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query.",
        "Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search.",
        "On the other hand, stemming is much more conservative since most of the time, stemming preserves the original search intent.",
        "While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision.",
        "The increase on recall is obvious.",
        "With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded.",
        "On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the expanded query.",
        "This is similar to the relevance feedback in the sense that it requires multiple passes retrieval.",
        "If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful.",
        "It is expensive to deploy this in production Web search engines.",
        "Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine.",
        "In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search - stemming.",
        "Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision.",
        "Its simple, yet very efficient and effective, making real time stemming feasible for Web search.",
        "Our results will affirm researchers that stemming is indeed very important to large scale information retrieval. 3.",
        "CONTEXT SENSITIVE STEMMING 3.1 Overview Our system has four components as illustrated in Figure 1: candidate generation, query segmentation and head word detection, context sensitive query stemming and context sensitive document matching.",
        "Candidate generation (component 1) is performed oﬄine and generated candidates are stored in a dictionary.",
        "For an input query, we first segment query into concepts and detect the head word for each concept (component 2).",
        "We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4).",
        "Below we discuss each of the components in more detail.",
        "Component 4: context sensitive document matching Input Query: and head word detection Component 2: segment Component 1: candidate generation comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: hotel comparisons hotel −> hotels Figure 1: System Components 3.2 Expansion candidate generation One of the ways to generate candidates is using the Porter stemmer [18].",
        "The Porter stemmer simply uses morphological rules to convert a word to its base form.",
        "It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past.",
        "A more conservative way is based on using corpus analysis to improve the Porter stemmer results [26].",
        "The corpus analysis we do is based on word distributional similarity [15].",
        "The rationale of using distributional word similarity is that true variants tend to be used in similar contexts.",
        "In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word.",
        "We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus.",
        "The similarity between two words is the cosine similarity between the two corresponding feature vectors.",
        "The top 20 similar words to develop is shown in the following table. rank candidate score rank candidate score 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word develop.",
        "Column score is the similarity score.",
        "To determine the stemming candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list.",
        "After applying these rules, for the word develop, the stemming candidates are developing, developed, develops, development, developement, developer, developmental.",
        "For the pluralization handling purpose, only the candidate develops is retained.",
        "One thing we note from observing the distributionally similar words is that they are closely related semantically.",
        "These words might serve as candidates for general query expansion, a topic we will investigate in the future. 3.3 Segmentation and headword identification For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts.",
        "We first break a query into segments, each segment representing a concept which normally is a noun phrase.",
        "For each of the noun phrases, we then detect the most important word which we call the head word.",
        "Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity.",
        "To break a query into segments, we have to define a criteria to measure the strength of the relation between words.",
        "One effective method is to use mutual information as an indicator on whether or not to split two words [19].",
        "We use a log of 25M queries and collect the bigram and unigram frequencies from it.",
        "For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word.",
        "We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here.",
        "Table 2 shows some examples of query segmentation.",
        "The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query.",
        "Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: Query segmentation: a segment is bracketed. parsing since many queries are not grammatical and are very short.",
        "Applying a parser trained on sentences from documents to queries will have poor performance.",
        "In our solution, we just use simple heuristics rules, and it works very well in practice for English.",
        "For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of/in/at/from UVW.",
        "In such cases, the head word is typically the last nonstop word of XYZ. 3.4 Context sensitive word expansion After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful.",
        "Our statistics show that about half of the queries can be transformed by pluralization via naive stemming.",
        "Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse.",
        "Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost.",
        "In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful.",
        "Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts.",
        "Head words hotel and comparison can be expanded to hotels and comparisons.",
        "Are both transformations useful?",
        "To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web.",
        "The more likely a query to occur on the Web, the more relevant documents this query is able to return.",
        "Now the whole problem becomes how to calculate the probability of query to occur on the Web.",
        "Calculating the probability of string occurring in a corpus is a well known language modeling problem.",
        "The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur).",
        "The simplest and most successful approach to language modeling is still based on the n-gram model.",
        "By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.",
        "Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus.",
        "Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary.",
        "This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6).",
        "Also, because of the heavy tailed nature of language (i.e.",
        "Zipfs law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling.",
        "One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator.",
        "Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5].",
        "We used absolute smoothing in our experiments.",
        "Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string.",
        "Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3.",
        "We can see that all alternatives are less likely than the input query.",
        "It is therefore not useful to make an expansion for this query.",
        "On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded.",
        "To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score.",
        "Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face. 3.5 Context sensitive document matching Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants.",
        "For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison.",
        "However, not every occurrence of comparison in the document is of interest.",
        "A page which is about comparing customer service can contain all of the words hotel price comparisons comparison.",
        "This page is not a good page for the query.",
        "If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval.",
        "To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document.",
        "A variant match is considered valid only if the variant occurs in the same context as the original word does.",
        "The context is the left or the right non-stop segments 1 of the original word.",
        "Taking the same query as an example, the context of comparisons is price.",
        "The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price.",
        "Thus, we should only match those occurrences of comparison in the document if they occur after the word price.",
        "Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size.",
        "If the expanded word comparison occurs within the context of price within a window, it is considered valid.",
        "The smaller the window size is, the more restrictive the matching.",
        "We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases. 4.",
        "EXPERIMENTAL EVALUATION 4.1 Evaluation metrics We will measure both relevance improvement and the stemming cost required to achieve the relevance. 1 a context segment can not be a single stop word. 4.1.1 Relevance measurement We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11].",
        "Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) . (7) where gk is the weight for the document at rank k. Higher degree of relevance corresponds to a higher weight.",
        "A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights.",
        "We use dcg to represent the average DCG(5) over a set of test queries. 4.1.2 Stemming cost Another metric is to measure the additional cost incurred by stemming.",
        "Given the same level of relevance improvement, we prefer a stemming method that has less additional cost.",
        "We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed. 4.2 Data preparation We randomly sample 870 queries from a three month query log, with 290 from each month.",
        "Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming.",
        "We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words.",
        "In the end, we have 529 correctly spelled queries with at least 2 words. 4.3 Naive stemming for Web search Before explaining the experiments and results in detail, wed like to describe the traditional way of using stemming for Web search, referred as the naive model.",
        "This is to treat every word variant equivalent for all possible words in the query.",
        "The query book store will be transformed into (book OR books)(store OR stores) when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments. 4.4 Experimental setup The baseline model is the model without stemming.",
        "We first run the naive model to see how well it performs over the baseline.",
        "Then we improve the naive stemming model by document sensitive matching, referred as document sensitive matching model.",
        "This model makes the same stemming as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5.",
        "The naive model and document sensitive matching model stem the most queries.",
        "Out of the 529 queries, there are 408 queries that they stem, corresponding to 46.7% query traffic (out of a total of 870).",
        "We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as selective stemming model.",
        "Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model.",
        "We experiment with unigram language model and bigram language model.",
        "Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments.",
        "To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data.",
        "The oracle model only expands a word if the stemming will give better results.",
        "To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries.",
        "Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words. 4.5 Results We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5.",
        "Each row in Table 4 is a stemming strategy described in section 4.4.",
        "The first column is the name of the strategy.",
        "The second column is the number of queries affected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg.",
        "The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy).",
        "The fourth column is the relative improvement over the baseline, and the last column is the p-value of Wilcoxon significance test.",
        "There are several observations about the results.",
        "We can see the naively stemming only obtains a statistically insignificant improvement of 1.5%.",
        "Looking at Table 5, it gives an improvement of 2.7% on short queries.",
        "However, it also hurts long queries by -2.4%.",
        "Overall, the improvement is canceled out.",
        "The reason that it improves short queries is that most short queries only have one word that can be stemmed.",
        "Thus, blindly pluralizing short queries is relatively safe.",
        "However for long queries, most queries can have multiple words that can be pluralized.",
        "Expanding all of them without selection will significantly hurt precision.",
        "Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8%.",
        "The improvement comes from the conservative context sensitive document matching.",
        "An expanded word is valid only if it occurs within the context of original query in the document.",
        "This reduces many spurious matches.",
        "However, we still notice that for long queries, context sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem.",
        "While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches.",
        "It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases.",
        "Selective word pluralization further helps resolving the problem faced by document context sensitive stemming.",
        "It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stemming in the first place.",
        "By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision.",
        "With the unigram language model, we can reduce the stemming cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4%.",
        "In particular, it gives significant improvements on long queries.",
        "The dcg gain is turned from negative to positive, from −1.6% to 1.1%.",
        "This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement.",
        "For short queries too, we observe both dcg improvement and stemming cost reduction with the unigram language model.",
        "The advantages of predictive word expansion with a language model is further boosted with a better bigram language model.",
        "The overall dcg gain is lifted from 3.4% to 3.9%, and stemming cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic.",
        "For short queries, bigram language model improves the dcg gain from 4.4% to 4.7%, and reduces stemming cost from 272/272 to 150/272.",
        "For long queries, bigram language model improves dcg gain from 1.1% to 2.5%, and reduces stemming cost from 136/136 to 100/136.",
        "We observe that the bigram language model gives a larger lift for long queries.",
        "This is because the uncertainty in long queries is larger and a more powerful language model is needed.",
        "We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation.",
        "Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying.",
        "For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram language model.",
        "For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram language model.",
        "We may gain additional benefit with a more powerful language model for long queries.",
        "However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem.",
        "These problems have to be addressed separately.",
        "Looking at the the upper-bound of overhead reduction for oracle stemming, 75% (308/408) of the naive stemmings are wasteful.",
        "We currently capture about half of them.",
        "Further reduction of the overhead requires sacrificing the dcg gain.",
        "Now we can compare the stemming strategies from a different aspect.",
        "Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only.",
        "We can see that the number of affected queries decreases as the stemming strategy becomes more accurate (dcg improvement).",
        "For the bigram language model, over the 250/408 stemmed queries, the dcg improvement is 6.1%.",
        "An interesting observation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more difficult queries (low dcg queries). 5.",
        "DISCUSSIONS 5.1 Language models from query vs. from Web As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web.",
        "The language model should describe the occurrence of the string on the Web.",
        "However, the query log is also a good resource. 2 Note that this upperbound is for pluralization handling only, not for general stemming.",
        "General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics.",
        "Affected Queries dcg dcg Improvement p-value baseline 0/408 7.102 N/A N/A naive model 408/408 7.206 1.5% 0.22 document context sensitive model 408/408 7.302 2.8% 0.014 selective model: unigram LM 300/408 7.321 3.4% 0.001 selective model: bigram LM 250/408 7.381 3.9% 0.001 oracle model 100/408 7.519 5.9% 0.001 Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming Short Query Results Affected Queries dcg Improvement p-value baseline 0/272 N/A N/A naive model 272/272 2.7% 0.48 document context sensitive model 272/272 4.2% 0.002 selective model: unigram LM 185/272 4.4% 0.001 selective model: bigram LM 150/272 4.7% 0.001 oracle model 71/272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p-value baseline 0/136 N/A N/A naive model 136/136 -2.4% 0.25 document context sensitive model 136/136 -1.6% 0.27 selective model: unigram LM 115/136 1.1% 0.001 selective model: bigram LM 100/136 2.5% 0.001 oracle model 29/136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results.",
        "To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a language model from the same query top 25M queries as used to learn segmentation, and use that for prediction.",
        "We observed a slight performance decrease compared to the model trained on Web frequencies.",
        "In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries.",
        "Thus, the query log can serve as a good approximation of the Web frequencies. 5.2 How linguistics helps Some linguistic knowledge is useful in stemming.",
        "For the pluralization handling case, pluralization and de-pluralization is not symmetric.",
        "A plural word used in a query indicates a special intent.",
        "For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California.",
        "A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank.",
        "To capture this intent, we have to make sure the document is a general page about hotels in new york.",
        "We do this by requiring that the plural word hotels appears in the document.",
        "On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information.",
        "We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property. 5.3 Error analysis One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after stemming.",
        "Generally speaking, pluralization or depluralization keeps the original intent.",
        "However, the intent could change in a few cases.",
        "For one example of such a query, job at apple, we pluralize job to jobs.",
        "This stemming makes the original query ambiguous.",
        "The query job OR jobs at apple has two intents.",
        "One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company.",
        "Thus, the results after query stemming returns Steve Jobs as one of the results in top 5.",
        "One solution is performing results set based analysis to check if the intent is changed.",
        "This is similar to relevance feedback and requires second phase ranking.",
        "A second type of mistakes is the entity/concept recognition problem, These include two kinds.",
        "One is that the stemmed word variant now matches part of an entity or concept.",
        "For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco.",
        "The results will match cookie jar in san francisco.",
        "Although cookie still means the same thing as cookies, cookie jar is a different concept.",
        "Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words.",
        "For example, quote ICE is pluralized to quote OR quotes ICE.",
        "The original intent for this query is searching for stock quote for ticker ICE.",
        "However, we noticed that among the top results, one of the results is Food quotes: Ice cream.",
        "This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408/408 7.102 7.206 1.5% document context sensitive model 408/408 7.102 7.302 2.8% selective model: unigram LM 300/408 5.904 6.187 4.8% selective model: bigram LM 250/408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the affected queries before/after applying stemming the pluralized word quotes.",
        "The unchanged word ICE matches part of the noun phrase ice cream here.",
        "To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words.",
        "A third type of mistakes occurs in long queries.",
        "For the query bar code reader software, two words are pluralized. code to codes and reader to readers.",
        "In fact, bar code reader in the original query is a strong concept and the internal words should not be changed.",
        "This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking.",
        "For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept. 6.",
        "CONCLUSIONS AND FUTURE WORK We have presented a simple yet elegant way of stemming for Web search.",
        "It improves naive stemming in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side.",
        "Using pluralization handling as an example, experiments on a major Web search engine data show it significantly improves the Web relevance and reduces the stemming cost.",
        "It also significantly improves Web click through rate (details not reported in the paper).",
        "For the future work, we are investigating the problems we identified in the error analysis section.",
        "These include: entity and noun phrase matching mistakes, and improved segmentation. 7.",
        "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
        "Improving Web Search Ranking by Incorporating User Behavior Information.",
        "In SIGIR, 2006. [2] E. Airio.",
        "Word Normalization and Decompounding in Mono- and Bilingual IR.",
        "Information Retrieval, 9:249-271, 2006. [3] P. Anick.",
        "Using Terminological Feedback for Web Search Refinement: a Log-based Study.",
        "In SIGIR, 2003. [4] R. Baeza-Yates and B. Ribeiro-Neto.",
        "Modern Information Retrieval.",
        "ACM Press/Addison Wesley, 1999. [5] S. Chen and J. Goodman.",
        "An Empirical Study of Smoothing Techniques for Language Modeling.",
        "Technical Report TR-10-98, Harvard University, 1998. [6] S. Cronen-Townsend, Y. Zhou, and B. Croft.",
        "A Framework for Selective Query Expansion.",
        "In CIKM, 2004. [7] H. Fang and C. Zhai.",
        "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
        "In SIGIR, 2006. [8] W. B. Frakes.",
        "Term Conflation for Information Retrieval.",
        "In C. J. Rijsbergen, editor, Research and Development in Information Retrieval, pages 383-389.",
        "Cambridge University Press, 1984. [9] D. Harman.",
        "How Effective is Suffixing?",
        "JASIS, 42(1):7-15, 1991. [10] D. Hull.",
        "Stemming Algorithms - A Case Study for Detailed Evaluation.",
        "JASIS, 47(1):70-84, 1996. [11] K. Jarvelin and J. Kekalainen.",
        "Cumulated Gain-Based Evaluation Evaluation of IR Techniques.",
        "ACM TOIS, 20:422-446, 2002. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
        "Generating Query Substitutions.",
        "In WWW, 2006. [13] W. Kraaij and R. Pohlmann.",
        "Viewing Stemming as Recall Enhancement.",
        "In SIGIR, 1996. [14] R. Krovetz.",
        "Viewing Morphology as an Inference Process.",
        "In SIGIR, 1993. [15] D. Lin.",
        "Automatic Retrieval and Clustering of Similar Words.",
        "In COLING-ACL, 1998. [16] J.",
        "B. Lovins.",
        "Development of a Stemming Algorithm.",
        "Mechanical Translation and Computational Linguistics, II:22-31, 1968. [17] M. Lennon and D. Peirce and B. Tarry and P. Willett.",
        "An Evaluation of Some Conflation Algorithms for Information Retrieval.",
        "Journal of Information Science, 3:177-188, 1981. [18] M. Porter.",
        "An Algorithm for Suffix Stripping.",
        "Program, 14(3):130-137, 1980. [19] K. M. Risvik, T. Mikolajewski, and P. Boros.",
        "Query Segmentation for Web Search.",
        "In WWW, 2003. [20] S. E. Robertson.",
        "On Term Selection for Query Expansion.",
        "Journal of Documentation, 46(4):359-364, 1990. [21] G. Salton and C. Buckley.",
        "Improving Retrieval Performance by Relevance Feedback.",
        "JASIS, 41(4):288 - 297, 1999. [22] R. Sun, C.-H. Ong, and T.-S. Chua.",
        "Mining Dependency Relations for Query Expansion in Passage Retrieval.",
        "In SIGIR, 2006. [23] C. Van Rijsbergen.",
        "Information Retrieval.",
        "Butterworths, second version, 1979. [24] B. V´elez, R. Weiss, M. A. Sheldon, and D. K. Gifford.",
        "Fast and Effective Query Refinement.",
        "In SIGIR, 1997. [25] J. Xu and B. Croft.",
        "Query Expansion using Local and Global Document Analysis.",
        "In SIGIR, 1996. [26] J. Xu and B. Croft.",
        "Corpus-based Stemming using Cooccurrence of Word Variants.",
        "ACM TOIS, 16 (1):61-81, 1998."
    ],
    "error_count": 0,
    "keys": {
        "web search": {
            "translated_key": "búsqueda Web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context Sensitive Stemming for <br>web search</br> Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo!",
                "Inc. 701 First Avenue Sunnyvale, California 94089 {fuchun, nawaaz, xinli, yumaol}@yahoo-inc.com ABSTRACT Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms.",
                "Although it increases recall, this naive strategy does not work well for <br>web search</br> since it lowers precision and requires a significant amount of additional computation.",
                "In this paper, we propose a context sensitive stemming method that addresses these two issues.",
                "Two unique properties make our approach feasible for <br>web search</br>.",
                "First, based on statistical language modeling, we perform context sensitive analysis on the query side.",
                "We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine.",
                "This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time.",
                "Second, our approach performs a context sensitive document matching for those expanded variants.",
                "This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision.",
                "Using word pluralization handling as an example of our stemming approach, our experiments on a major <br>web search</br> engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Storage and Retrieval-Query formulation General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION <br>web search</br> has now become a major tool in our daily lives for information seeking.",
                "One of the important issues in <br>web search</br> is that user queries are often not best formulated to get optimal results.",
                "For example, running shoe is a query that occurs frequently in query logs.",
                "However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes.",
                "Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs.",
                "This is difficult even for experienced users, and especially difficult for non-native speakers.",
                "One traditional solution is to use stemming [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term.",
                "Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing.",
                "Stemming can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing.",
                "Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows handling each query differently, and hence is preferred.",
                "Although traditional stemming increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched.",
                "When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6].",
                "In addition, it reduces system performance because the search engine has to match all the word variants.",
                "As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa.",
                "Thus, one needs to be very cautious when using stemming in <br>web search</br> engines.",
                "One problem of traditional stemming is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word.",
                "For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored.",
                "For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query.",
                "Transforming book store to match book stores is fine, but matching book storing or booking store is not.",
                "A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query.",
                "However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20].",
                "A second problem of traditional stemming is its blind matching of all occurrences in documents.",
                "For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store.",
                "Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected.",
                "Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases.",
                "To alleviate these two problems, we propose a context sensitive stemming approach for <br>web search</br>.",
                "Our solution consists of two context sensitive analysis, one on the query side and the other on the document side.",
                "On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms.",
                "On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query.",
                "Our model is simple yet effective and efficient, making it feasible to be used in real commercial <br>web search</br> engines.",
                "We use pluralization handling as a running example for our stemming approach.",
                "The motivation for using pluralization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance.",
                "As far as we know, no previous research has systematically investigated the usage of pluralization in <br>web search</br>.",
                "As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion.",
                "Experiments on general stemming yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper.",
                "In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2.",
                "We describe the details of the context sensitive stemming approach in Section 3.",
                "We then perform extensive experiments on a major <br>web search</br> engine to support our claims in Section 4, followed by discussions in Section 5.",
                "Finally, we conclude the paper in Section 6. 2.",
                "RELATED WORK Stemming is a long studied technology.",
                "Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18].",
                "The Porter stemmer is widely used due to its simplicity and effectiveness in many applications.",
                "However, the Porter stemming makes many mistakes because its simple rules cannot fully describe English morphology.",
                "Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23].",
                "We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates.",
                "Using stemming in information retrieval is also a well known technique [8, 10].",
                "However, the effectiveness of stemming for English query systems was previously reported to be rather limited.",
                "Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance.",
                "Later, Harman [9] compares three general stemming techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper).",
                "They also proposed selective stemming based on query length and term importance, but no positive results were reported.",
                "On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%).",
                "However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to <br>web search</br>.",
                "These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2].",
                "We suspect the previous failures were mainly due to the two problems we mentioned in the introduction.",
                "Blind stemming, or a simple query length based selective stemming as used in [9] is not enough.",
                "Stemming has to be decided on case by case basis, not only at the query level but also at the document level.",
                "As we will show, if handled correctly, significant improvement can be achieved.",
                "A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25].",
                "To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21].",
                "This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in <br>web search</br>.",
                "In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift).",
                "Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query.",
                "Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for <br>web search</br>.",
                "On the other hand, stemming is much more conservative since most of the time, stemming preserves the original search intent.",
                "While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision.",
                "The increase on recall is obvious.",
                "With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded.",
                "On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the expanded query.",
                "This is similar to the relevance feedback in the sense that it requires multiple passes retrieval.",
                "If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful.",
                "It is expensive to deploy this in production <br>web search</br> engines.",
                "Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine.",
                "In summary, we propose a novel approach to attack an old, yet still important and challenging problem for <br>web search</br> - stemming.",
                "Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision.",
                "Its simple, yet very efficient and effective, making real time stemming feasible for <br>web search</br>.",
                "Our results will affirm researchers that stemming is indeed very important to large scale information retrieval. 3.",
                "CONTEXT SENSITIVE STEMMING 3.1 Overview Our system has four components as illustrated in Figure 1: candidate generation, query segmentation and head word detection, context sensitive query stemming and context sensitive document matching.",
                "Candidate generation (component 1) is performed oﬄine and generated candidates are stored in a dictionary.",
                "For an input query, we first segment query into concepts and detect the head word for each concept (component 2).",
                "We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4).",
                "Below we discuss each of the components in more detail.",
                "Component 4: context sensitive document matching Input Query: and head word detection Component 2: segment Component 1: candidate generation comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: hotel comparisons hotel −> hotels Figure 1: System Components 3.2 Expansion candidate generation One of the ways to generate candidates is using the Porter stemmer [18].",
                "The Porter stemmer simply uses morphological rules to convert a word to its base form.",
                "It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past.",
                "A more conservative way is based on using corpus analysis to improve the Porter stemmer results [26].",
                "The corpus analysis we do is based on word distributional similarity [15].",
                "The rationale of using distributional word similarity is that true variants tend to be used in similar contexts.",
                "In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word.",
                "We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus.",
                "The similarity between two words is the cosine similarity between the two corresponding feature vectors.",
                "The top 20 similar words to develop is shown in the following table. rank candidate score rank candidate score 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word develop.",
                "Column score is the similarity score.",
                "To determine the stemming candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list.",
                "After applying these rules, for the word develop, the stemming candidates are developing, developed, develops, development, developement, developer, developmental.",
                "For the pluralization handling purpose, only the candidate develops is retained.",
                "One thing we note from observing the distributionally similar words is that they are closely related semantically.",
                "These words might serve as candidates for general query expansion, a topic we will investigate in the future. 3.3 Segmentation and headword identification For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts.",
                "We first break a query into segments, each segment representing a concept which normally is a noun phrase.",
                "For each of the noun phrases, we then detect the most important word which we call the head word.",
                "Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity.",
                "To break a query into segments, we have to define a criteria to measure the strength of the relation between words.",
                "One effective method is to use mutual information as an indicator on whether or not to split two words [19].",
                "We use a log of 25M queries and collect the bigram and unigram frequencies from it.",
                "For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word.",
                "We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here.",
                "Table 2 shows some examples of query segmentation.",
                "The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query.",
                "Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: Query segmentation: a segment is bracketed. parsing since many queries are not grammatical and are very short.",
                "Applying a parser trained on sentences from documents to queries will have poor performance.",
                "In our solution, we just use simple heuristics rules, and it works very well in practice for English.",
                "For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of/in/at/from UVW.",
                "In such cases, the head word is typically the last nonstop word of XYZ. 3.4 Context sensitive word expansion After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful.",
                "Our statistics show that about half of the queries can be transformed by pluralization via naive stemming.",
                "Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse.",
                "Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost.",
                "In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful.",
                "Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts.",
                "Head words hotel and comparison can be expanded to hotels and comparisons.",
                "Are both transformations useful?",
                "To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web.",
                "The more likely a query to occur on the Web, the more relevant documents this query is able to return.",
                "Now the whole problem becomes how to calculate the probability of query to occur on the Web.",
                "Calculating the probability of string occurring in a corpus is a well known language modeling problem.",
                "The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur).",
                "The simplest and most successful approach to language modeling is still based on the n-gram model.",
                "By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.",
                "Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus.",
                "Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary.",
                "This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6).",
                "Also, because of the heavy tailed nature of language (i.e.",
                "Zipfs law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling.",
                "One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator.",
                "Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5].",
                "We used absolute smoothing in our experiments.",
                "Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string.",
                "Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3.",
                "We can see that all alternatives are less likely than the input query.",
                "It is therefore not useful to make an expansion for this query.",
                "On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded.",
                "To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score.",
                "Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face. 3.5 Context sensitive document matching Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants.",
                "For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison.",
                "However, not every occurrence of comparison in the document is of interest.",
                "A page which is about comparing customer service can contain all of the words hotel price comparisons comparison.",
                "This page is not a good page for the query.",
                "If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval.",
                "To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document.",
                "A variant match is considered valid only if the variant occurs in the same context as the original word does.",
                "The context is the left or the right non-stop segments 1 of the original word.",
                "Taking the same query as an example, the context of comparisons is price.",
                "The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price.",
                "Thus, we should only match those occurrences of comparison in the document if they occur after the word price.",
                "Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size.",
                "If the expanded word comparison occurs within the context of price within a window, it is considered valid.",
                "The smaller the window size is, the more restrictive the matching.",
                "We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases. 4.",
                "EXPERIMENTAL EVALUATION 4.1 Evaluation metrics We will measure both relevance improvement and the stemming cost required to achieve the relevance. 1 a context segment can not be a single stop word. 4.1.1 Relevance measurement We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11].",
                "Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) . (7) where gk is the weight for the document at rank k. Higher degree of relevance corresponds to a higher weight.",
                "A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights.",
                "We use dcg to represent the average DCG(5) over a set of test queries. 4.1.2 Stemming cost Another metric is to measure the additional cost incurred by stemming.",
                "Given the same level of relevance improvement, we prefer a stemming method that has less additional cost.",
                "We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed. 4.2 Data preparation We randomly sample 870 queries from a three month query log, with 290 from each month.",
                "Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming.",
                "We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words.",
                "In the end, we have 529 correctly spelled queries with at least 2 words. 4.3 Naive stemming for <br>web search</br> Before explaining the experiments and results in detail, wed like to describe the traditional way of using stemming for <br>web search</br>, referred as the naive model.",
                "This is to treat every word variant equivalent for all possible words in the query.",
                "The query book store will be transformed into (book OR books)(store OR stores) when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments. 4.4 Experimental setup The baseline model is the model without stemming.",
                "We first run the naive model to see how well it performs over the baseline.",
                "Then we improve the naive stemming model by document sensitive matching, referred as document sensitive matching model.",
                "This model makes the same stemming as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5.",
                "The naive model and document sensitive matching model stem the most queries.",
                "Out of the 529 queries, there are 408 queries that they stem, corresponding to 46.7% query traffic (out of a total of 870).",
                "We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as selective stemming model.",
                "Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model.",
                "We experiment with unigram language model and bigram language model.",
                "Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments.",
                "To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data.",
                "The oracle model only expands a word if the stemming will give better results.",
                "To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries.",
                "Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words. 4.5 Results We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5.",
                "Each row in Table 4 is a stemming strategy described in section 4.4.",
                "The first column is the name of the strategy.",
                "The second column is the number of queries affected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg.",
                "The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy).",
                "The fourth column is the relative improvement over the baseline, and the last column is the p-value of Wilcoxon significance test.",
                "There are several observations about the results.",
                "We can see the naively stemming only obtains a statistically insignificant improvement of 1.5%.",
                "Looking at Table 5, it gives an improvement of 2.7% on short queries.",
                "However, it also hurts long queries by -2.4%.",
                "Overall, the improvement is canceled out.",
                "The reason that it improves short queries is that most short queries only have one word that can be stemmed.",
                "Thus, blindly pluralizing short queries is relatively safe.",
                "However for long queries, most queries can have multiple words that can be pluralized.",
                "Expanding all of them without selection will significantly hurt precision.",
                "Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8%.",
                "The improvement comes from the conservative context sensitive document matching.",
                "An expanded word is valid only if it occurs within the context of original query in the document.",
                "This reduces many spurious matches.",
                "However, we still notice that for long queries, context sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem.",
                "While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches.",
                "It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases.",
                "Selective word pluralization further helps resolving the problem faced by document context sensitive stemming.",
                "It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stemming in the first place.",
                "By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision.",
                "With the unigram language model, we can reduce the stemming cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4%.",
                "In particular, it gives significant improvements on long queries.",
                "The dcg gain is turned from negative to positive, from −1.6% to 1.1%.",
                "This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement.",
                "For short queries too, we observe both dcg improvement and stemming cost reduction with the unigram language model.",
                "The advantages of predictive word expansion with a language model is further boosted with a better bigram language model.",
                "The overall dcg gain is lifted from 3.4% to 3.9%, and stemming cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic.",
                "For short queries, bigram language model improves the dcg gain from 4.4% to 4.7%, and reduces stemming cost from 272/272 to 150/272.",
                "For long queries, bigram language model improves dcg gain from 1.1% to 2.5%, and reduces stemming cost from 136/136 to 100/136.",
                "We observe that the bigram language model gives a larger lift for long queries.",
                "This is because the uncertainty in long queries is larger and a more powerful language model is needed.",
                "We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation.",
                "Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying.",
                "For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram language model.",
                "For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram language model.",
                "We may gain additional benefit with a more powerful language model for long queries.",
                "However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem.",
                "These problems have to be addressed separately.",
                "Looking at the the upper-bound of overhead reduction for oracle stemming, 75% (308/408) of the naive stemmings are wasteful.",
                "We currently capture about half of them.",
                "Further reduction of the overhead requires sacrificing the dcg gain.",
                "Now we can compare the stemming strategies from a different aspect.",
                "Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only.",
                "We can see that the number of affected queries decreases as the stemming strategy becomes more accurate (dcg improvement).",
                "For the bigram language model, over the 250/408 stemmed queries, the dcg improvement is 6.1%.",
                "An interesting observation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more difficult queries (low dcg queries). 5.",
                "DISCUSSIONS 5.1 Language models from query vs. from Web As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web.",
                "The language model should describe the occurrence of the string on the Web.",
                "However, the query log is also a good resource. 2 Note that this upperbound is for pluralization handling only, not for general stemming.",
                "General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics.",
                "Affected Queries dcg dcg Improvement p-value baseline 0/408 7.102 N/A N/A naive model 408/408 7.206 1.5% 0.22 document context sensitive model 408/408 7.302 2.8% 0.014 selective model: unigram LM 300/408 7.321 3.4% 0.001 selective model: bigram LM 250/408 7.381 3.9% 0.001 oracle model 100/408 7.519 5.9% 0.001 Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming Short Query Results Affected Queries dcg Improvement p-value baseline 0/272 N/A N/A naive model 272/272 2.7% 0.48 document context sensitive model 272/272 4.2% 0.002 selective model: unigram LM 185/272 4.4% 0.001 selective model: bigram LM 150/272 4.7% 0.001 oracle model 71/272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p-value baseline 0/136 N/A N/A naive model 136/136 -2.4% 0.25 document context sensitive model 136/136 -1.6% 0.27 selective model: unigram LM 115/136 1.1% 0.001 selective model: bigram LM 100/136 2.5% 0.001 oracle model 29/136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results.",
                "To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a language model from the same query top 25M queries as used to learn segmentation, and use that for prediction.",
                "We observed a slight performance decrease compared to the model trained on Web frequencies.",
                "In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries.",
                "Thus, the query log can serve as a good approximation of the Web frequencies. 5.2 How linguistics helps Some linguistic knowledge is useful in stemming.",
                "For the pluralization handling case, pluralization and de-pluralization is not symmetric.",
                "A plural word used in a query indicates a special intent.",
                "For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California.",
                "A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank.",
                "To capture this intent, we have to make sure the document is a general page about hotels in new york.",
                "We do this by requiring that the plural word hotels appears in the document.",
                "On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information.",
                "We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property. 5.3 Error analysis One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after stemming.",
                "Generally speaking, pluralization or depluralization keeps the original intent.",
                "However, the intent could change in a few cases.",
                "For one example of such a query, job at apple, we pluralize job to jobs.",
                "This stemming makes the original query ambiguous.",
                "The query job OR jobs at apple has two intents.",
                "One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company.",
                "Thus, the results after query stemming returns Steve Jobs as one of the results in top 5.",
                "One solution is performing results set based analysis to check if the intent is changed.",
                "This is similar to relevance feedback and requires second phase ranking.",
                "A second type of mistakes is the entity/concept recognition problem, These include two kinds.",
                "One is that the stemmed word variant now matches part of an entity or concept.",
                "For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco.",
                "The results will match cookie jar in san francisco.",
                "Although cookie still means the same thing as cookies, cookie jar is a different concept.",
                "Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words.",
                "For example, quote ICE is pluralized to quote OR quotes ICE.",
                "The original intent for this query is searching for stock quote for ticker ICE.",
                "However, we noticed that among the top results, one of the results is Food quotes: Ice cream.",
                "This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408/408 7.102 7.206 1.5% document context sensitive model 408/408 7.102 7.302 2.8% selective model: unigram LM 300/408 5.904 6.187 4.8% selective model: bigram LM 250/408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the affected queries before/after applying stemming the pluralized word quotes.",
                "The unchanged word ICE matches part of the noun phrase ice cream here.",
                "To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words.",
                "A third type of mistakes occurs in long queries.",
                "For the query bar code reader software, two words are pluralized. code to codes and reader to readers.",
                "In fact, bar code reader in the original query is a strong concept and the internal words should not be changed.",
                "This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking.",
                "For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept. 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented a simple yet elegant way of stemming for <br>web search</br>.",
                "It improves naive stemming in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side.",
                "Using pluralization handling as an example, experiments on a major <br>web search</br> engine data show it significantly improves the Web relevance and reduces the stemming cost.",
                "It also significantly improves Web click through rate (details not reported in the paper).",
                "For the future work, we are investigating the problems we identified in the error analysis section.",
                "These include: entity and noun phrase matching mistakes, and improved segmentation. 7.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving <br>web search</br> Ranking by Incorporating User Behavior Information.",
                "In SIGIR, 2006. [2] E. Airio.",
                "Word Normalization and Decompounding in Mono- and Bilingual IR.",
                "Information Retrieval, 9:249-271, 2006. [3] P. Anick.",
                "Using Terminological Feedback for <br>web search</br> Refinement: a Log-based Study.",
                "In SIGIR, 2003. [4] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press/Addison Wesley, 1999. [5] S. Chen and J. Goodman.",
                "An Empirical Study of Smoothing Techniques for Language Modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [6] S. Cronen-Townsend, Y. Zhou, and B. Croft.",
                "A Framework for Selective Query Expansion.",
                "In CIKM, 2004. [7] H. Fang and C. Zhai.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In SIGIR, 2006. [8] W. B. Frakes.",
                "Term Conflation for Information Retrieval.",
                "In C. J. Rijsbergen, editor, Research and Development in Information Retrieval, pages 383-389.",
                "Cambridge University Press, 1984. [9] D. Harman.",
                "How Effective is Suffixing?",
                "JASIS, 42(1):7-15, 1991. [10] D. Hull.",
                "Stemming Algorithms - A Case Study for Detailed Evaluation.",
                "JASIS, 47(1):70-84, 1996. [11] K. Jarvelin and J. Kekalainen.",
                "Cumulated Gain-Based Evaluation Evaluation of IR Techniques.",
                "ACM TOIS, 20:422-446, 2002. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating Query Substitutions.",
                "In WWW, 2006. [13] W. Kraaij and R. Pohlmann.",
                "Viewing Stemming as Recall Enhancement.",
                "In SIGIR, 1996. [14] R. Krovetz.",
                "Viewing Morphology as an Inference Process.",
                "In SIGIR, 1993. [15] D. Lin.",
                "Automatic Retrieval and Clustering of Similar Words.",
                "In COLING-ACL, 1998. [16] J.",
                "B. Lovins.",
                "Development of a Stemming Algorithm.",
                "Mechanical Translation and Computational Linguistics, II:22-31, 1968. [17] M. Lennon and D. Peirce and B. Tarry and P. Willett.",
                "An Evaluation of Some Conflation Algorithms for Information Retrieval.",
                "Journal of Information Science, 3:177-188, 1981. [18] M. Porter.",
                "An Algorithm for Suffix Stripping.",
                "Program, 14(3):130-137, 1980. [19] K. M. Risvik, T. Mikolajewski, and P. Boros.",
                "Query Segmentation for <br>web search</br>.",
                "In WWW, 2003. [20] S. E. Robertson.",
                "On Term Selection for Query Expansion.",
                "Journal of Documentation, 46(4):359-364, 1990. [21] G. Salton and C. Buckley.",
                "Improving Retrieval Performance by Relevance Feedback.",
                "JASIS, 41(4):288 - 297, 1999. [22] R. Sun, C.-H. Ong, and T.-S. Chua.",
                "Mining Dependency Relations for Query Expansion in Passage Retrieval.",
                "In SIGIR, 2006. [23] C. Van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, second version, 1979. [24] B. V´elez, R. Weiss, M. A. Sheldon, and D. K. Gifford.",
                "Fast and Effective Query Refinement.",
                "In SIGIR, 1997. [25] J. Xu and B. Croft.",
                "Query Expansion using Local and Global Document Analysis.",
                "In SIGIR, 1996. [26] J. Xu and B. Croft.",
                "Corpus-based Stemming using Cooccurrence of Word Variants.",
                "ACM TOIS, 16 (1):61-81, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Contexto Sensible Stemming para \"Búsqueda web\" Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo!",
                "Aunque aumenta el recuerdo, esta estrategia ingenua no funciona bien para la \"búsqueda web\", ya que reduce la precisión y requiere una cantidad significativa de cálculo adicional.",
                "Dos propiedades únicas hacen que nuestro enfoque sea factible para la \"búsqueda web\".",
                "Utilizando el manejo de la pluralización de palabras como un ejemplo de nuestro enfoque de Stemming, nuestros experimentos en un importante motor de \"búsqueda web\" muestran que solo el 29% del tráfico de la consulta, podemos mejorar la relevancia medida por una ganancia acumulada con descuento promedio (DCG5) en un 6.1%en estas consultas y 1.8% sobre todo el tráfico de consultas.",
                "Introducción La \"búsqueda web\" se ha convertido en una herramienta importante en nuestra vida diaria para la búsqueda de información.",
                "Uno de los problemas importantes en la \"búsqueda web\" es que las consultas de los usuarios a menudo no están mejor formuladas para obtener resultados óptimos.",
                "Por lo tanto, uno debe ser muy cauteloso al usar los motores de \"búsqueda web\".",
                "Para aliviar estos dos problemas, proponemos un enfoque derivado del contexto para la \"búsqueda web\".",
                "Nuestro modelo es simple pero efectivo y eficiente, lo que hace que sea factible ser utilizado en motores comerciales reales de \"búsqueda web\".",
                "Hasta donde sabemos, ninguna investigación previa ha investigado sistemáticamente el uso de la pluralización en la \"búsqueda web\".",
                "Luego realizamos experimentos extensos en un importante motor de \"búsqueda web\" para respaldar nuestros reclamos en la Sección 4, seguido de discusiones en la Sección 5.",
                "Sin embargo, debido al número limitado de consultas probadas (menos de 100) y el pequeño tamaño de la colección, los resultados son difíciles de generalizar para la \"búsqueda web\".",
                "Esto normalmente requiere enviar una consulta varias veces al motor de búsqueda y no es rentable para procesar la gran cantidad de consultas involucradas en la \"búsqueda web\".",
                "Por lo tanto, la expansión de la consulta basada en pseudorelevancia y reformulación de consultas puede proporcionar sugerencias a los usuarios para el refinamiento interactivo, pero difícilmente se puede usar directamente para la \"búsqueda web\".",
                "Es costoso implementar esto en los motores de \"búsqueda web\" de producción.",
                "En resumen, proponemos un enfoque novedoso para atacar un problema antiguo, pero aún importante y desafiante para la \"búsqueda web\", se detiene.",
                "Es simple, pero muy eficiente y efectivo, lo que hace que el tiempo real sea factible para la \"búsqueda web\".",
                "Al final, tenemos 529 consultas deletreadas correctamente con al menos 2 palabras.4.3 ingenuo derivado para la \"búsqueda web\" antes de explicar los experimentos y los resultados en detalle, a Wed le gusta describir la forma tradicional de usar Stemming para la \"búsqueda web\", denominada modelo ingenuo.",
                "Conclusiones y trabajo futuro hemos presentado una forma simple pero elegante de detener la \"búsqueda web\".",
                "Utilizando el manejo de la pluralización como ejemplo, los experimentos en un importante motor de \"búsqueda web\" muestran que mejora significativamente la relevancia web y reduce el costo de las medidas.",
                "Mejora de la clasificación de \"búsqueda web\" incorporando información de comportamiento del usuario.",
                "Utilizando la retroalimentación terminológica para el refinamiento de \"búsqueda web\": un estudio basado en registros.",
                "Segmentación de consulta para \"búsqueda web\"."
            ],
            "translated_text": "",
            "candidates": [
                "búsqueda Web",
                "Búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "Búsqueda web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web"
            ],
            "error": []
        },
        "stemming": {
            "translated_key": "stemming",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context Sensitive <br>stemming</br> for Web Search Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo!",
                "Inc. 701 First Avenue Sunnyvale, California 94089 {fuchun, nawaaz, xinli, yumaol}@yahoo-inc.com ABSTRACT Traditionally, <br>stemming</br> has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms.",
                "Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation.",
                "In this paper, we propose a context sensitive <br>stemming</br> method that addresses these two issues.",
                "Two unique properties make our approach feasible for Web Search.",
                "First, based on statistical language modeling, we perform context sensitive analysis on the query side.",
                "We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine.",
                "This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time.",
                "Second, our approach performs a context sensitive document matching for those expanded variants.",
                "This conservative strategy serves as a safeguard against spurious <br>stemming</br>, and it turns out to be very important for improving precision.",
                "Using word pluralization handling as an example of our <br>stemming</br> approach, our experiments on a major Web search engine show that <br>stemming</br> only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Storage and Retrieval-Query formulation General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Web search has now become a major tool in our daily lives for information seeking.",
                "One of the important issues in Web search is that user queries are often not best formulated to get optimal results.",
                "For example, running shoe is a query that occurs frequently in query logs.",
                "However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes.",
                "Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs.",
                "This is difficult even for experienced users, and especially difficult for non-native speakers.",
                "One traditional solution is to use <br>stemming</br> [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term.",
                "Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing.",
                "<br>stemming</br> can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing.",
                "<br>stemming</br> during indexing allows very little flexibility during query processing, while <br>stemming</br> by query expansion allows handling each query differently, and hence is preferred.",
                "Although traditional <br>stemming</br> increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched.",
                "When examining the results of applying <br>stemming</br> to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6].",
                "In addition, it reduces system performance because the search engine has to match all the word variants.",
                "As we will show in the experiments, this is true even if we simplify <br>stemming</br> to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa.",
                "Thus, one needs to be very cautious when using <br>stemming</br> in Web search engines.",
                "One problem of traditional <br>stemming</br> is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word.",
                "For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored.",
                "For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query.",
                "Transforming book store to match book stores is fine, but matching book storing or booking store is not.",
                "A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query.",
                "However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20].",
                "A second problem of traditional <br>stemming</br> is its blind matching of all occurrences in documents.",
                "For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store.",
                "Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected.",
                "Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases.",
                "To alleviate these two problems, we propose a context sensitive <br>stemming</br> approach for Web search.",
                "Our solution consists of two context sensitive analysis, one on the query side and the other on the document side.",
                "On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms.",
                "On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query.",
                "Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines.",
                "We use pluralization handling as a running example for our <br>stemming</br> approach.",
                "The motivation for using pluralization handling as an example is to show that even such simple <br>stemming</br>, if handled correctly, can give significant benefits to search relevance.",
                "As far as we know, no previous research has systematically investigated the usage of pluralization in Web search.",
                "As we have to point out, the method we propose is not limited to pluralization handling, it is a general <br>stemming</br> technique, and can also be applied to general query expansion.",
                "Experiments on general <br>stemming</br> yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper.",
                "In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2.",
                "We describe the details of the context sensitive <br>stemming</br> approach in Section 3.",
                "We then perform extensive experiments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5.",
                "Finally, we conclude the paper in Section 6. 2.",
                "RELATED WORK <br>stemming</br> is a long studied technology.",
                "Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18].",
                "The Porter stemmer is widely used due to its simplicity and effectiveness in many applications.",
                "However, the Porter <br>stemming</br> makes many mistakes because its simple rules cannot fully describe English morphology.",
                "Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23].",
                "We use a similar corpus based approach for <br>stemming</br> by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates.",
                "Using <br>stemming</br> in information retrieval is also a well known technique [8, 10].",
                "However, the effectiveness of <br>stemming</br> for English query systems was previously reported to be rather limited.",
                "Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance.",
                "Later, Harman [9] compares three general <br>stemming</br> techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper).",
                "They also proposed selective <br>stemming</br> based on query length and term importance, but no positive results were reported.",
                "On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%).",
                "However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search.",
                "These mixed results, mostly failures, led early IR researchers to deem <br>stemming</br> irrelevant in general for English [4], although recent research has shown <br>stemming</br> has greater benefits for retrieval in other languages [2].",
                "We suspect the previous failures were mainly due to the two problems we mentioned in the introduction.",
                "Blind <br>stemming</br>, or a simple query length based selective <br>stemming</br> as used in [9] is not enough.",
                "<br>stemming</br> has to be decided on case by case basis, not only at the query level but also at the document level.",
                "As we will show, if handled correctly, significant improvement can be achieved.",
                "A more general problem related to <br>stemming</br> is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25].",
                "To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21].",
                "This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search.",
                "In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift).",
                "Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query.",
                "Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search.",
                "On the other hand, <br>stemming</br> is much more conservative since most of the time, <br>stemming</br> preserves the original search intent.",
                "While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision.",
                "The increase on recall is obvious.",
                "With quality <br>stemming</br>, good documents which were not selected before <br>stemming</br> will be pushed up and those low quality documents will be degraded.",
                "On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the expanded query.",
                "This is similar to the relevance feedback in the sense that it requires multiple passes retrieval.",
                "If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful.",
                "It is expensive to deploy this in production Web search engines.",
                "Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine.",
                "In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search - <br>stemming</br>.",
                "Our approach is unique in that it performs predictive <br>stemming</br> on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision.",
                "Its simple, yet very efficient and effective, making real time <br>stemming</br> feasible for Web search.",
                "Our results will affirm researchers that <br>stemming</br> is indeed very important to large scale information retrieval. 3.",
                "CONTEXT SENSITIVE <br>stemming</br> 3.1 Overview Our system has four components as illustrated in Figure 1: candidate generation, query segmentation and head word detection, context sensitive query <br>stemming</br> and context sensitive document matching.",
                "Candidate generation (component 1) is performed oﬄine and generated candidates are stored in a dictionary.",
                "For an input query, we first segment query into concepts and detect the head word for each concept (component 2).",
                "We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4).",
                "Below we discuss each of the components in more detail.",
                "Component 4: context sensitive document matching Input Query: and head word detection Component 2: segment Component 1: candidate generation comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: hotel comparisons hotel −> hotels Figure 1: System Components 3.2 Expansion candidate generation One of the ways to generate candidates is using the Porter stemmer [18].",
                "The Porter stemmer simply uses morphological rules to convert a word to its base form.",
                "It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past.",
                "A more conservative way is based on using corpus analysis to improve the Porter stemmer results [26].",
                "The corpus analysis we do is based on word distributional similarity [15].",
                "The rationale of using distributional word similarity is that true variants tend to be used in similar contexts.",
                "In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word.",
                "We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus.",
                "The similarity between two words is the cosine similarity between the two corresponding feature vectors.",
                "The top 20 similar words to develop is shown in the following table. rank candidate score rank candidate score 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word develop.",
                "Column score is the similarity score.",
                "To determine the <br>stemming</br> candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list.",
                "After applying these rules, for the word develop, the <br>stemming</br> candidates are developing, developed, develops, development, developement, developer, developmental.",
                "For the pluralization handling purpose, only the candidate develops is retained.",
                "One thing we note from observing the distributionally similar words is that they are closely related semantically.",
                "These words might serve as candidates for general query expansion, a topic we will investigate in the future. 3.3 Segmentation and headword identification For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts.",
                "We first break a query into segments, each segment representing a concept which normally is a noun phrase.",
                "For each of the noun phrases, we then detect the most important word which we call the head word.",
                "Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity.",
                "To break a query into segments, we have to define a criteria to measure the strength of the relation between words.",
                "One effective method is to use mutual information as an indicator on whether or not to split two words [19].",
                "We use a log of 25M queries and collect the bigram and unigram frequencies from it.",
                "For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word.",
                "We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here.",
                "Table 2 shows some examples of query segmentation.",
                "The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query.",
                "Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: Query segmentation: a segment is bracketed. parsing since many queries are not grammatical and are very short.",
                "Applying a parser trained on sentences from documents to queries will have poor performance.",
                "In our solution, we just use simple heuristics rules, and it works very well in practice for English.",
                "For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of/in/at/from UVW.",
                "In such cases, the head word is typically the last nonstop word of XYZ. 3.4 Context sensitive word expansion After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful.",
                "Our statistics show that about half of the queries can be transformed by pluralization via naive <br>stemming</br>.",
                "Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse.",
                "Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing <br>stemming</br> cost.",
                "In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful.",
                "Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts.",
                "Head words hotel and comparison can be expanded to hotels and comparisons.",
                "Are both transformations useful?",
                "To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web.",
                "The more likely a query to occur on the Web, the more relevant documents this query is able to return.",
                "Now the whole problem becomes how to calculate the probability of query to occur on the Web.",
                "Calculating the probability of string occurring in a corpus is a well known language modeling problem.",
                "The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur).",
                "The simplest and most successful approach to language modeling is still based on the n-gram model.",
                "By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.",
                "Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus.",
                "Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary.",
                "This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6).",
                "Also, because of the heavy tailed nature of language (i.e.",
                "Zipfs law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling.",
                "One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator.",
                "Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5].",
                "We used absolute smoothing in our experiments.",
                "Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string.",
                "Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3.",
                "We can see that all alternatives are less likely than the input query.",
                "It is therefore not useful to make an expansion for this query.",
                "On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded.",
                "To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score.",
                "Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face. 3.5 Context sensitive document matching Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants.",
                "For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison.",
                "However, not every occurrence of comparison in the document is of interest.",
                "A page which is about comparing customer service can contain all of the words hotel price comparisons comparison.",
                "This page is not a good page for the query.",
                "If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most <br>stemming</br> approaches do not work well for information retrieval.",
                "To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document.",
                "A variant match is considered valid only if the variant occurs in the same context as the original word does.",
                "The context is the left or the right non-stop segments 1 of the original word.",
                "Taking the same query as an example, the context of comparisons is price.",
                "The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price.",
                "Thus, we should only match those occurrences of comparison in the document if they occur after the word price.",
                "Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size.",
                "If the expanded word comparison occurs within the context of price within a window, it is considered valid.",
                "The smaller the window size is, the more restrictive the matching.",
                "We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases. 4.",
                "EXPERIMENTAL EVALUATION 4.1 Evaluation metrics We will measure both relevance improvement and the <br>stemming</br> cost required to achieve the relevance. 1 a context segment can not be a single stop word. 4.1.1 Relevance measurement We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11].",
                "Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) . (7) where gk is the weight for the document at rank k. Higher degree of relevance corresponds to a higher weight.",
                "A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights.",
                "We use dcg to represent the average DCG(5) over a set of test queries. 4.1.2 <br>stemming</br> cost Another metric is to measure the additional cost incurred by <br>stemming</br>.",
                "Given the same level of relevance improvement, we prefer a <br>stemming</br> method that has less additional cost.",
                "We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed. 4.2 Data preparation We randomly sample 870 queries from a three month query log, with 290 from each month.",
                "Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to <br>stemming</br>.",
                "We also remove all one word queries since <br>stemming</br> one word queries without context has a high risk of changing query intent, especially for short words.",
                "In the end, we have 529 correctly spelled queries with at least 2 words. 4.3 Naive <br>stemming</br> for Web search Before explaining the experiments and results in detail, wed like to describe the traditional way of using <br>stemming</br> for Web search, referred as the naive model.",
                "This is to treat every word variant equivalent for all possible words in the query.",
                "The query book store will be transformed into (book OR books)(store OR stores) when limiting <br>stemming</br> to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments. 4.4 Experimental setup The baseline model is the model without <br>stemming</br>.",
                "We first run the naive model to see how well it performs over the baseline.",
                "Then we improve the naive <br>stemming</br> model by document sensitive matching, referred as document sensitive matching model.",
                "This model makes the same <br>stemming</br> as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5.",
                "The naive model and document sensitive matching model stem the most queries.",
                "Out of the 529 queries, there are 408 queries that they stem, corresponding to 46.7% query traffic (out of a total of 870).",
                "We then further improve the document sensitive matching model from the query side with selective word <br>stemming</br> based on statistical language modeling (section 3.4), referred as selective <br>stemming</br> model.",
                "Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model.",
                "We experiment with unigram language model and bigram language model.",
                "Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive <br>stemming</br> model) in the experiments.",
                "To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data.",
                "The oracle model only expands a word if the <br>stemming</br> will give better results.",
                "To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries.",
                "Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words. 4.5 Results We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5.",
                "Each row in Table 4 is a <br>stemming</br> strategy described in section 4.4.",
                "The first column is the name of the strategy.",
                "The second column is the number of queries affected by this strategy; this column measures the <br>stemming</br> cost, and the numbers should be low for the same level of dcg.",
                "The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy).",
                "The fourth column is the relative improvement over the baseline, and the last column is the p-value of Wilcoxon significance test.",
                "There are several observations about the results.",
                "We can see the naively <br>stemming</br> only obtains a statistically insignificant improvement of 1.5%.",
                "Looking at Table 5, it gives an improvement of 2.7% on short queries.",
                "However, it also hurts long queries by -2.4%.",
                "Overall, the improvement is canceled out.",
                "The reason that it improves short queries is that most short queries only have one word that can be stemmed.",
                "Thus, blindly pluralizing short queries is relatively safe.",
                "However for long queries, most queries can have multiple words that can be pluralized.",
                "Expanding all of them without selection will significantly hurt precision.",
                "Document context sensitive <br>stemming</br> gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8%.",
                "The improvement comes from the conservative context sensitive document matching.",
                "An expanded word is valid only if it occurs within the context of original query in the document.",
                "This reduces many spurious matches.",
                "However, we still notice that for long queries, context sensitive <br>stemming</br> is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem.",
                "While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches.",
                "It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases.",
                "Selective word pluralization further helps resolving the problem faced by document context sensitive <br>stemming</br>.",
                "It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary <br>stemming</br> in the first place.",
                "By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision.",
                "With the unigram language model, we can reduce the <br>stemming</br> cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4%.",
                "In particular, it gives significant improvements on long queries.",
                "The dcg gain is turned from negative to positive, from −1.6% to 1.1%.",
                "This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement.",
                "For short queries too, we observe both dcg improvement and <br>stemming</br> cost reduction with the unigram language model.",
                "The advantages of predictive word expansion with a language model is further boosted with a better bigram language model.",
                "The overall dcg gain is lifted from 3.4% to 3.9%, and <br>stemming</br> cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic.",
                "For short queries, bigram language model improves the dcg gain from 4.4% to 4.7%, and reduces <br>stemming</br> cost from 272/272 to 150/272.",
                "For long queries, bigram language model improves dcg gain from 1.1% to 2.5%, and reduces <br>stemming</br> cost from 136/136 to 100/136.",
                "We observe that the bigram language model gives a larger lift for long queries.",
                "This is because the uncertainty in long queries is larger and a more powerful language model is needed.",
                "We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation.",
                "Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying.",
                "For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram language model.",
                "For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram language model.",
                "We may gain additional benefit with a more powerful language model for long queries.",
                "However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem.",
                "These problems have to be addressed separately.",
                "Looking at the the upper-bound of overhead reduction for oracle <br>stemming</br>, 75% (308/408) of the naive stemmings are wasteful.",
                "We currently capture about half of them.",
                "Further reduction of the overhead requires sacrificing the dcg gain.",
                "Now we can compare the <br>stemming</br> strategies from a different aspect.",
                "Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only.",
                "We can see that the number of affected queries decreases as the <br>stemming</br> strategy becomes more accurate (dcg improvement).",
                "For the bigram language model, over the 250/408 stemmed queries, the dcg improvement is 6.1%.",
                "An interesting observation is the average dcg decreases with a better model, which indicates a better <br>stemming</br> strategy stems more difficult queries (low dcg queries). 5.",
                "DISCUSSIONS 5.1 Language models from query vs. from Web As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web.",
                "The language model should describe the occurrence of the string on the Web.",
                "However, the query log is also a good resource. 2 Note that this upperbound is for pluralization handling only, not for general <br>stemming</br>.",
                "General <br>stemming</br> gives a 8% upperbound, which is quite substantial in terms of our metrics.",
                "Affected Queries dcg dcg Improvement p-value baseline 0/408 7.102 N/A N/A naive model 408/408 7.206 1.5% 0.22 document context sensitive model 408/408 7.302 2.8% 0.014 selective model: unigram LM 300/408 7.321 3.4% 0.001 selective model: bigram LM 250/408 7.381 3.9% 0.001 oracle model 100/408 7.519 5.9% 0.001 Table 4: Results comparison of different <br>stemming</br> strategies over all queries affected by naive <br>stemming</br> Short Query Results Affected Queries dcg Improvement p-value baseline 0/272 N/A N/A naive model 272/272 2.7% 0.48 document context sensitive model 272/272 4.2% 0.002 selective model: unigram LM 185/272 4.4% 0.001 selective model: bigram LM 150/272 4.7% 0.001 oracle model 71/272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p-value baseline 0/136 N/A N/A naive model 136/136 -2.4% 0.25 document context sensitive model 136/136 -1.6% 0.27 selective model: unigram LM 115/136 1.1% 0.001 selective model: bigram LM 100/136 2.5% 0.001 oracle model 29/136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results.",
                "To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a language model from the same query top 25M queries as used to learn segmentation, and use that for prediction.",
                "We observed a slight performance decrease compared to the model trained on Web frequencies.",
                "In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries.",
                "Thus, the query log can serve as a good approximation of the Web frequencies. 5.2 How linguistics helps Some linguistic knowledge is useful in <br>stemming</br>.",
                "For the pluralization handling case, pluralization and de-pluralization is not symmetric.",
                "A plural word used in a query indicates a special intent.",
                "For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California.",
                "A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank.",
                "To capture this intent, we have to make sure the document is a general page about hotels in new york.",
                "We do this by requiring that the plural word hotels appears in the document.",
                "On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information.",
                "We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive <br>stemming</br> if we do not consider this asymmetric property. 5.3 Error analysis One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after <br>stemming</br>.",
                "Generally speaking, pluralization or depluralization keeps the original intent.",
                "However, the intent could change in a few cases.",
                "For one example of such a query, job at apple, we pluralize job to jobs.",
                "This <br>stemming</br> makes the original query ambiguous.",
                "The query job OR jobs at apple has two intents.",
                "One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company.",
                "Thus, the results after query <br>stemming</br> returns Steve Jobs as one of the results in top 5.",
                "One solution is performing results set based analysis to check if the intent is changed.",
                "This is similar to relevance feedback and requires second phase ranking.",
                "A second type of mistakes is the entity/concept recognition problem, These include two kinds.",
                "One is that the stemmed word variant now matches part of an entity or concept.",
                "For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco.",
                "The results will match cookie jar in san francisco.",
                "Although cookie still means the same thing as cookies, cookie jar is a different concept.",
                "Another kind is the unstemmed word matches an entity or concept because of the <br>stemming</br> of the other words.",
                "For example, quote ICE is pluralized to quote OR quotes ICE.",
                "The original intent for this query is searching for stock quote for ticker ICE.",
                "However, we noticed that among the top results, one of the results is Food quotes: Ice cream.",
                "This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408/408 7.102 7.206 1.5% document context sensitive model 408/408 7.102 7.302 2.8% selective model: unigram LM 300/408 5.904 6.187 4.8% selective model: bigram LM 250/408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the affected queries before/after applying <br>stemming</br> the pluralized word quotes.",
                "The unchanged word ICE matches part of the noun phrase ice cream here.",
                "To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words.",
                "A third type of mistakes occurs in long queries.",
                "For the query bar code reader software, two words are pluralized. code to codes and reader to readers.",
                "In fact, bar code reader in the original query is a strong concept and the internal words should not be changed.",
                "This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking.",
                "For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept. 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented a simple yet elegant way of <br>stemming</br> for Web search.",
                "It improves naive <br>stemming</br> in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side.",
                "Using pluralization handling as an example, experiments on a major Web search engine data show it significantly improves the Web relevance and reduces the <br>stemming</br> cost.",
                "It also significantly improves Web click through rate (details not reported in the paper).",
                "For the future work, we are investigating the problems we identified in the error analysis section.",
                "These include: entity and noun phrase matching mistakes, and improved segmentation. 7.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving Web Search Ranking by Incorporating User Behavior Information.",
                "In SIGIR, 2006. [2] E. Airio.",
                "Word Normalization and Decompounding in Mono- and Bilingual IR.",
                "Information Retrieval, 9:249-271, 2006. [3] P. Anick.",
                "Using Terminological Feedback for Web Search Refinement: a Log-based Study.",
                "In SIGIR, 2003. [4] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press/Addison Wesley, 1999. [5] S. Chen and J. Goodman.",
                "An Empirical Study of Smoothing Techniques for Language Modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [6] S. Cronen-Townsend, Y. Zhou, and B. Croft.",
                "A Framework for Selective Query Expansion.",
                "In CIKM, 2004. [7] H. Fang and C. Zhai.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In SIGIR, 2006. [8] W. B. Frakes.",
                "Term Conflation for Information Retrieval.",
                "In C. J. Rijsbergen, editor, Research and Development in Information Retrieval, pages 383-389.",
                "Cambridge University Press, 1984. [9] D. Harman.",
                "How Effective is Suffixing?",
                "JASIS, 42(1):7-15, 1991. [10] D. Hull.",
                "<br>stemming</br> Algorithms - A Case Study for Detailed Evaluation.",
                "JASIS, 47(1):70-84, 1996. [11] K. Jarvelin and J. Kekalainen.",
                "Cumulated Gain-Based Evaluation Evaluation of IR Techniques.",
                "ACM TOIS, 20:422-446, 2002. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating Query Substitutions.",
                "In WWW, 2006. [13] W. Kraaij and R. Pohlmann.",
                "Viewing <br>stemming</br> as Recall Enhancement.",
                "In SIGIR, 1996. [14] R. Krovetz.",
                "Viewing Morphology as an Inference Process.",
                "In SIGIR, 1993. [15] D. Lin.",
                "Automatic Retrieval and Clustering of Similar Words.",
                "In COLING-ACL, 1998. [16] J.",
                "B. Lovins.",
                "Development of a <br>stemming</br> Algorithm.",
                "Mechanical Translation and Computational Linguistics, II:22-31, 1968. [17] M. Lennon and D. Peirce and B. Tarry and P. Willett.",
                "An Evaluation of Some Conflation Algorithms for Information Retrieval.",
                "Journal of Information Science, 3:177-188, 1981. [18] M. Porter.",
                "An Algorithm for Suffix Stripping.",
                "Program, 14(3):130-137, 1980. [19] K. M. Risvik, T. Mikolajewski, and P. Boros.",
                "Query Segmentation for Web Search.",
                "In WWW, 2003. [20] S. E. Robertson.",
                "On Term Selection for Query Expansion.",
                "Journal of Documentation, 46(4):359-364, 1990. [21] G. Salton and C. Buckley.",
                "Improving Retrieval Performance by Relevance Feedback.",
                "JASIS, 41(4):288 - 297, 1999. [22] R. Sun, C.-H. Ong, and T.-S. Chua.",
                "Mining Dependency Relations for Query Expansion in Passage Retrieval.",
                "In SIGIR, 2006. [23] C. Van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, second version, 1979. [24] B. V´elez, R. Weiss, M. A. Sheldon, and D. K. Gifford.",
                "Fast and Effective Query Refinement.",
                "In SIGIR, 1997. [25] J. Xu and B. Croft.",
                "Query Expansion using Local and Global Document Analysis.",
                "In SIGIR, 1996. [26] J. Xu and B. Croft.",
                "Corpus-based <br>stemming</br> using Cooccurrence of Word Variants.",
                "ACM TOIS, 16 (1):61-81, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Contexto sensible a \"Stemming\" para la búsqueda web Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo!",
                "Inc. 701 First Avenue Sunnyvale, California 94089 {Fuchun, Nawaaz, Xinli, yumaolt.@yahoo-inc.com Resumen tradicionalmente, \"Stemming\" se ha aplicado a tareas de recuperación de información transformando palabras en documentos a su forma raíz antes de la indexación,y aplicar una transformación similar a los términos de consulta.",
                "En este documento, proponemos un método \"derivado\" sensible al contexto que aborde estos dos problemas.",
                "Esta estrategia conservadora sirve como una salvaguardia contra espurios \"Stemming\", y resulta ser muy importante para mejorar la precisión.",
                "Utilizando el manejo de la pluralización de palabras como un ejemplo de nuestro enfoque de \"derivación\", nuestros experimentos en un motor de búsqueda web importante muestran que \"vadeo\" solo el 29% del tráfico de la consulta, podemos mejorar la relevancia medida por la ganancia acumulativa con descuento promedio (DCG5) por6.1% en estas consultas y 1.8% sobre todo el tráfico de consultas.",
                "Una solución tradicional es usar \"Stemming\" [16, 18], el proceso de transformación de palabras infectadas o derivadas en su forma raíz para que un término de búsqueda coincida y recupere documentos que contengan todas las formas del término.",
                "\"Stemming\" se puede hacer en los términos en un documento durante la indexación (y aplicar la misma transformación a los términos de consulta durante el procesamiento de la consulta) o expandiendo la consulta con las variantes durante el procesamiento de la consulta.",
                "\"Stemming\" durante la indexación permite muy poca flexibilidad durante el procesamiento de la consulta, mientras que \"Stemming\" por la expansión de la consulta permite manejar cada consulta de manera diferente y, por lo tanto, se prefiere.",
                "Aunque el \"siglo\" tradicional aumenta el recuerdo al igualar las variantes de palabras [13], puede reducir la precisión al recuperar demasiados documentos que se han combinado incorrectamente.",
                "Al examinar los resultados de la aplicación de \"derivación\" a una gran cantidad de consultas, generalmente se encuentra que la técnica [6] es un número casi igual de consultas.",
                "Como mostraremos en los experimentos, esto es cierto incluso si simplificamos \"derivado\" al manejo de la pluralización, que es el proceso de convertir una palabra de su forma plural a singular, o viceversa.",
                "Por lo tanto, uno debe ser muy cauteloso cuando se usa \"Stemming\" en los motores de búsqueda web.",
                "Un problema de \"Stemming\" tradicional es su transformación ciega de todos los términos de consulta, es decir, siempre realiza la misma transformación para la misma palabra de consulta sin considerar el contexto de la palabra.",
                "Un segundo problema de \"Stemming\" tradicional es su coincidencia ciega de todos los sucesos en los documentos.",
                "Para aliviar estos dos problemas, proponemos un enfoque \"derivado\" sensible al contexto para la búsqueda web.",
                "Utilizamos el manejo de la pluralización como un ejemplo de ejecución para nuestro enfoque \"derivado\".",
                "La motivación para usar el manejo de la pluralización como ejemplo es mostrar que incluso tan simple \"derivación\", si se maneja correctamente, puede dar beneficios significativos para la relevancia de búsqueda.",
                "Como tenemos que señalar, el método que proponemos no se limita al manejo de la pluralización, es una técnica general \"derivada\" y también se puede aplicar a la expansión general de la consulta.",
                "Los experimentos sobre \"vallas\" general producen mejoras significativas adicionales sobre el manejo de la pluralización para consultas largas, aunque no se informarán detalles en este documento.",
                "Describimos los detalles del enfoque del contexto sensible a \"Stemming\" en la Sección 3.",
                "El trabajo relacionado \"Stemming\" es una tecnología estudiada desde hace mucho tiempo.",
                "Sin embargo, el Porter \"Stemming\" comete muchos errores porque sus reglas simples no pueden describir completamente la morfología inglesa.",
                "Utilizamos un enfoque basado en el corpus similar para \"Stemming\" calculando la similitud entre dos palabras basadas en sus características de contexto de distribución que pueden ser más que solo palabras adyacentes [15], y luego mantener las palabras morfológicamente similares a los candidatos.",
                "El uso de \"Stemming\" en la recuperación de la información también es una técnica bien conocida [8, 10].",
                "Sin embargo, la efectividad de \"Stemming\" para los sistemas de consultas en inglés se informó previamente que era bastante limitada.",
                "Más tarde, Harman [9] compara tres técnicas generales de \"derivación\" en los experimentos de recuperación de texto, incluida la entrega de pluralización (llamado S Stemmer en el documento).",
                "También propusieron \"derivación\" selectivo basado en la longitud de la consulta y la importancia a término, pero no se informaron resultados positivos.",
                "Estos resultados mixtos, en su mayoría fallas, llevaron a los primeros investigadores IR a considerar \"derivado\" irrelevante en general para el inglés [4], aunque las investigaciones recientes han demostrado que \"Stemming\" tiene mayores beneficios para la recuperación en otros idiomas [2].",
                "Ciego \"Stemming\", o una simple longitud de consulta, una \"derivación\" basada en la longitud de consulta como se usa en [9] no es suficiente.",
                "\"Stemming\" debe decidirse caso por caso, no solo en el nivel de consulta sino también a nivel de documento.",
                "Un problema más general relacionado con \"Stemming\" es la reformulación de consultas [3, 12] y la expansión de la consulta que expande las palabras no solo con variantes de palabras [7, 22, 24, 25].",
                "Por otro lado, \"Stemming\" es mucho más conservador ya que la mayoría de las veces, \"Stemming\" conserva la intención de búsqueda original.",
                "Con la calidad de \"Stemming\", los buenos documentos que no fueron seleccionados antes de \"Stemming\" serán subidos y esos documentos de baja calidad se degradarán.",
                "En resumen, proponemos un enfoque novedoso para atacar un problema antiguo, pero aún importante y desafiante para la búsqueda web, \"Stemming\".",
                "Nuestro enfoque es único en el sentido de que realiza predictivo \"derivado\" por consulta sin retroalimentación de relevancia de la Web, utilizando el contexto de las variantes en los documentos para preservar la precisión.",
                "Es simple, pero muy eficiente y efectivo, lo que hace que el tiempo real sea factible para la búsqueda web.",
                "Nuestros resultados afirmarán a los investigadores que \"Stemming\" es muy importante para la recuperación de información a gran escala.3.",
                "Context Sensitive \"Stemming\" 3.1 Descripción general Nuestro sistema tiene cuatro componentes como se ilustra en la Figura 1: Generación de candidatos, segmentación de consultas y detección de palabras de cabeza, consulta sensible al contexto \"Stemming\" y coincidencia de documentos sensibles al contexto.",
                "Para determinar los candidatos \"derivados\", aplicamos algunas reglas morfológicas de Porter Stemmer [18] a la lista de similitud.",
                "Después de aplicar estas reglas, para el desarrollo de la palabra, los candidatos \"derivados\" están desarrollando, desarrollan, desarrollan, desarrollo, desarrollo, desarrollador, desarrollo.",
                "Nuestras estadísticas muestran que aproximadamente la mitad de las consultas pueden transformarse mediante pluralización a través de ingenuo \"Stemming\".",
                "Por lo tanto, es extremadamente importante identificar qué consultas no deben ser frecuentes con el fin de maximizar la mejora de la relevancia y minimizar el costo de \"derivación\".",
                "Si aceptamos coincidencias de cada aparición de comparación, dañará la precisión de la recuperación y esta es una de las principales razones por las cuales la mayoría de los enfoques \"derivados\" no funcionan bien para la recuperación de la información.",
                "Evaluación experimental 4.1 Métricas de evaluación Mediremos tanto la mejora de la relevancia como el costo de \"siguen\" requerido para lograr la relevancia.1 Un segmento de contexto no puede ser una sola palabra de parada.4.1.1 Medición de relevancia Utilizamos una variante de la ganancia acumulativa promedio con descuento (DCG), un esquema recientemente popularizado para medir la relevancia del motor de búsqueda [1, 11].",
                "Usamos DCG para representar el DCG promedio (5) sobre un conjunto de consultas de prueba.4.1.2 Costo \"Stemming\" Otra métrica es medir el costo adicional incurrido por \"Stemming\".",
                "Dado el mismo nivel de mejora de relevancia, preferimos un método \"derivado\" que tiene menos costo adicional.",
                "Entre todas estas 870 consultas, eliminamos todas las consultas mal escritas ya que las consultas mal escritas no son de interés para \"Stemming\".",
                "También eliminamos todas las consultas de una palabra, ya que las consultas de una palabra \"derivadas\" de una palabra sin contexto tienen un alto riesgo de cambiar la intención de consulta, especialmente para palabras cortas.",
                "Al final, tenemos 529 consultas deletreadas correctamente con al menos 2 palabras.4.3 NIVIS \"STEMMING\" para la búsqueda web antes de explicar los experimentos y los resultados en detalle, a Wed le gusta describir la forma tradicional de usar \"Stemming\" para la búsqueda web, denominado modelo ingenuo.",
                "La librería de la consulta se transformará en (libros o libros) (tiendas o tiendas) al limitar solo el manejar la pluralización, donde o es un operador que denota la equivalencia de los argumentos de izquierda y derecha.4.4 Configuración experimental El modelo de línea de base es el modelo sin \"Stemming\".",
                "Luego mejoramos el modelo ingenuo \"Stemming\" mediante la coincidencia sensible del documento, denominado modelo de coincidencia sensible al documento.",
                "Este modelo hace el mismo \"derivado\" que el modelo ingenuo en el lado de la consulta, pero realiza una coincidencia conservadora en el lado del documento utilizando la estrategia descrita en la Sección 3.5.",
                "Luego mejoramos aún más el modelo de coincidencia sensible del documento del lado de la consulta con la palabra selectiva \"Stemming\" basado en el modelado de lenguaje estadístico (Sección 3.4), denominado modelo selectivo \"Stemming\".",
                "Dado que solo nos importa cuánto podemos mejorar el modelo ingenuo, solo usaremos estas 408 consultas (todas las consultas que se ven afectadas por el modelo ingenuo \"derivado\") en los experimentos.",
                "El modelo Oracle solo expande una palabra si el \"Stemming\" dará mejores resultados.",
                "Cada fila de la Tabla 4 es una estrategia \"derivada\" descrita en la Sección 4.4.",
                "La segunda columna es el número de consultas afectadas por esta estrategia;Esta columna mide el costo de \"derivación\", y los números deben ser bajos para el mismo nivel de DCG.",
                "Podemos ver que la ingenuidad \"derivada\" solo obtiene una mejora estadísticamente insignificante del 1.5%.",
                "El contexto del documento sensible a la \"derecha\" del contexto da un ascensor significativo al rendimiento, de 2.7% a 4.2% para consultas cortas y de -2.4% a -1.6% para consultas largas, con un aumento general de 1.5% a 2.8%.",
                "Sin embargo, todavía notamos que para consultas largas, \"Stemming\" del contexto no puede mejorar el rendimiento porque todavía selecciona demasiados documentos y le da a la función de clasificación un problema difícil.",
                "La pluralización de palabras selectivas ayuda aún más a resolver el problema que enfrenta el contexto del documento sensible al contexto \"Stemming\".",
                "No detiene todas las palabras que coloca toda la carga en el algoritmo de clasificación, pero trata de eliminar innecesario \"Stemming\" en primer lugar.",
                "Con el modelo de lenguaje Unigram, podemos reducir el costo \"derivado\" en un 26.7% (de 408/408 a 300/408) y levantar la mejora general de DCG de 2.8% a 3.4%.",
                "Para consultas cortas también, observamos la mejora de DCG y la reducción de costos \"derivadas\" con el modelo de idioma unigram.",
                "La ganancia general de DCG se eleva del 3.4% al 3.9%, y el costo \"derivado\" se reduce drásticamente de 408/408 a 250/408, correspondiente a solo el 29% del tráfico de consultas (250 de 870) y un 1.8% DCG total de 1.8%Mejora en general todo el tráfico de consultas.",
                "Para consultas cortas, el modelo de lenguaje BigRam mejora la ganancia de DCG de 4.4% a 4.7%, y reduce el costo \"derivado\" de 272/272 a 150/272.",
                "Para consultas largas, el modelo de lenguaje BigRam mejora la ganancia de DCG de 1.1% a 2.5%, y reduce el costo \"derivado\" de 136/136 a 100/136.",
                "Al observar la reducción superior de la reducción de la cabeza para Oracle \"Stemming\", el 75% (308/408) de los tallos ingenuos son un desperdicio.",
                "Ahora podemos comparar las estrategias \"derivadas\" de un aspecto diferente.",
                "Podemos ver que el número de consultas afectadas disminuye a medida que la estrategia \"derivada\" se vuelve más precisa (mejora de DCG).",
                "Una observación interesante es que el DCG promedio disminuye con un mejor modelo, lo que indica una mejor estrategia \"derivada\" detiene consultas más difíciles (consultas bajas de DCG).5.",
                "Sin embargo, el registro de consultas también es un buen recurso.2 Tenga en cuenta que este conflicto superior es solo para el manejo de la pluralización, no para el \"derivación\" general.",
                "El general \"Stemming\" ofrece un 8% de confusión superior, que es bastante sustancial en términos de nuestras métricas.",
                "Consultas afectadas DCG DCG Mejora de valor P-Valor P-Valor 0/408 7.102 N/A N/A Modelo ingenuo 408/408 7.206 1.5% 0.22 Contexto del documento Contexto Sensitivo Modelo 408/408 7.302 2.8% 0.014 Modelo selectivo: Unigram LM 300/408 7.321 3.4% 0.001Modelo selectivo: BigRam LM 250/408 7.381 3.9% 0.001 Oracle Model 100/408 7.519 5.9% 0.001 Tabla 4: Comparación de resultados de diferentes estrategias \"vallas\" sobre todas las consultas afectadas por los resultados de consultas cortos \"vistas\" ingenuos afectadosValor Base Base 0/272 N/A N/A Modelo ingenuo 272/272 2.7% 0.48 Documento Contexto Sensible Modelo 272/272 4.2% 0.002 Modelo selectivo: Unigram LM 185/272 4.4% 0.001 Modelo selectivo: BigRam LM 150/272 4.7% 0.001Oracle Modelo 71/272 6.3% 0.001 Resultados de consultas largas afectó las consultas DCG Mejora de P -Valor BASEAL 0/136 N/A N/A Modelo ingenuo 136/136 -2.4% 0.25 Documento Contexto Sensitivo Modelo 136/136 -1.6% 0.27 Modelo selectivo: Modelo selectivo:Unigram LM 115/136 1.1% 0.001 Modelo selectivo: BigRam LM 100/136 2.5% 0.001 Oracle Model 29/136 4.6% 0.001 Tabla 5: Comparación de resultados de diferentes estrategias de consulta corta en general y consultas largas reformulan una consulta utilizando muchas diferentes variantesPara obtener buenos resultados.",
                "Por lo tanto, el registro de consultas puede servir como una buena aproximación de las frecuencias web.5.2 Cómo la lingüística ayuda a algún conocimiento lingüístico es útil en \"Stemming\".",
                "Observamos una ligera disminución de DCG general, aunque no estadísticamente significativa, para el contexto del documento sensible al \"derivación\" del contexto si no consideramos esta propiedad asimétrica.5.3 Análisis de errores Un tipo de errores que notamos, aunque raro pero gravemente perjudicial, es el cambio de intención de búsqueda después de \"Stemming\".",
                "Este \"Stemming\" hace que la consulta original sea ambigua.",
                "",
                "Otro tipo es que la palabra inexplicable coincide con una entidad o concepto debido a la \"derivación\" de las otras palabras.",
                "Esto coincide con consultas afectadas DCG New DCG DCG Mejora DCG Modelo ingenuo 408/408 7.102 7.206 1.5% Documento Contexto Sensible Modelo 408/408 7.102 7.302 2.8% Modelo selectivo: Unigram LM 300/408 5.904 6.187 4.8% Selective Model: BigRam LM250/408 5.551 5.891 6.1% Tabla 6: Comparación de resultados solo sobre las consultas de tallo: la columna antigua/nueva DCG es la puntuación DCG sobre las consultas afectadas antes/después de aplicar \"STEMMing\" las citas de palabras pluralizadas.",
                "Conclusiones y trabajo futuro hemos presentado una forma simple pero elegante de \"Stemming\" para la búsqueda web.",
                "Mejora ingenuo \"Stemming\" en dos aspectos: expansión de palabras selectivas en el lado de la consulta y la coincidencia de palabras conservadoras en el lado del documento.",
                "Utilizando el manejo de la pluralización como ejemplo, los experimentos en un importante motor de búsqueda de la web muestran que mejora significativamente la relevancia web y reduce el costo de \"siguen\".",
                "Algoritmos \"Stemming\": un estudio de caso para una evaluación detallada.",
                "Ver \"Stemming\" como mejora de retiro.",
                "Desarrollo de un algoritmo \"Stemming\".",
                "\"Stemming\" basado en el corpus utilizando la coocción de variantes de palabras."
            ],
            "translated_text": "",
            "candidates": [
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "derivado",
                "derivado",
                "Stemming",
                "derivado",
                "derivación",
                "vadeo",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "Stemming",
                "derivado",
                "siglo",
                "derivado",
                "derivación",
                "derivado",
                "derivado",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "derivado",
                "derivado",
                "derivado",
                "derivado",
                "derivación",
                "derivado",
                "derivada",
                "derivado",
                "vallas",
                "STEMMING",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "derivación",
                "derivado",
                "derivación",
                "derivado",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "derivación",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "Stemming",
                "derivado",
                "Stemming",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "derivado",
                "derivado",
                "Stemming",
                "Stemming",
                "derivado",
                "Stemming",
                "Stemming",
                "derivado",
                "derivados",
                "derivado",
                "derivados",
                "derivado",
                "Stemming",
                "derivado",
                "derivación",
                "derivado",
                "derivados",
                "derivado",
                "siguen",
                "derivado",
                "Stemming",
                "Stemming",
                "derivado",
                "derivado",
                "derivado",
                "Stemming",
                "derivado",
                "derivadas",
                "derivado",
                "STEMMING",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "derivado",
                "derivado",
                "Stemming",
                "Stemming",
                "derivado",
                "derivado",
                "derivado",
                "Stemming",
                "derivado",
                "derivada",
                "derivado",
                "derivación",
                "derivado",
                "derivada",
                "derivado",
                "derecha",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "derivado",
                "derivado",
                "derivadas",
                "derivado",
                "derivado",
                "derivado",
                "derivado",
                "derivado",
                "derivado",
                "derivado",
                "Stemming",
                "derivado",
                "derivadas",
                "derivado",
                "derivada",
                "Stemming",
                "derivada",
                "derivado",
                "derivación",
                "derivado",
                "Stemming",
                "derivado",
                "vallas",
                "vistas",
                "derivado",
                "Stemming",
                "derivado",
                "derivación",
                "Stemming",
                "derivado",
                "Stemming",
                "",
                "Stemming",
                "derivado",
                "derivación",
                "derivado",
                "STEMMing",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "siguen",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming",
                "derivado",
                "Stemming"
            ],
            "error": []
        },
        "lovin stemmer": {
            "translated_key": "Lovin Stemmer",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Context Sensitive Stemming for Web Search Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo!",
                "Inc. 701 First Avenue Sunnyvale, California 94089 {fuchun, nawaaz, xinli, yumaol}@yahoo-inc.com ABSTRACT Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms.",
                "Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation.",
                "In this paper, we propose a context sensitive stemming method that addresses these two issues.",
                "Two unique properties make our approach feasible for Web Search.",
                "First, based on statistical language modeling, we perform context sensitive analysis on the query side.",
                "We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine.",
                "This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time.",
                "Second, our approach performs a context sensitive document matching for those expanded variants.",
                "This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision.",
                "Using word pluralization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Storage and Retrieval-Query formulation General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Web search has now become a major tool in our daily lives for information seeking.",
                "One of the important issues in Web search is that user queries are often not best formulated to get optimal results.",
                "For example, running shoe is a query that occurs frequently in query logs.",
                "However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes.",
                "Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs.",
                "This is difficult even for experienced users, and especially difficult for non-native speakers.",
                "One traditional solution is to use stemming [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term.",
                "Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing.",
                "Stemming can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing.",
                "Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows handling each query differently, and hence is preferred.",
                "Although traditional stemming increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched.",
                "When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6].",
                "In addition, it reduces system performance because the search engine has to match all the word variants.",
                "As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa.",
                "Thus, one needs to be very cautious when using stemming in Web search engines.",
                "One problem of traditional stemming is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word.",
                "For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored.",
                "For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query.",
                "Transforming book store to match book stores is fine, but matching book storing or booking store is not.",
                "A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query.",
                "However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20].",
                "A second problem of traditional stemming is its blind matching of all occurrences in documents.",
                "For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store.",
                "Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected.",
                "Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases.",
                "To alleviate these two problems, we propose a context sensitive stemming approach for Web search.",
                "Our solution consists of two context sensitive analysis, one on the query side and the other on the document side.",
                "On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms.",
                "On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query.",
                "Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines.",
                "We use pluralization handling as a running example for our stemming approach.",
                "The motivation for using pluralization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance.",
                "As far as we know, no previous research has systematically investigated the usage of pluralization in Web search.",
                "As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion.",
                "Experiments on general stemming yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper.",
                "In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2.",
                "We describe the details of the context sensitive stemming approach in Section 3.",
                "We then perform extensive experiments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5.",
                "Finally, we conclude the paper in Section 6. 2.",
                "RELATED WORK Stemming is a long studied technology.",
                "Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18].",
                "The Porter stemmer is widely used due to its simplicity and effectiveness in many applications.",
                "However, the Porter stemming makes many mistakes because its simple rules cannot fully describe English morphology.",
                "Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23].",
                "We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates.",
                "Using stemming in information retrieval is also a well known technique [8, 10].",
                "However, the effectiveness of stemming for English query systems was previously reported to be rather limited.",
                "Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance.",
                "Later, Harman [9] compares three general stemming techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper).",
                "They also proposed selective stemming based on query length and term importance, but no positive results were reported.",
                "On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%).",
                "However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search.",
                "These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2].",
                "We suspect the previous failures were mainly due to the two problems we mentioned in the introduction.",
                "Blind stemming, or a simple query length based selective stemming as used in [9] is not enough.",
                "Stemming has to be decided on case by case basis, not only at the query level but also at the document level.",
                "As we will show, if handled correctly, significant improvement can be achieved.",
                "A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25].",
                "To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21].",
                "This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search.",
                "In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift).",
                "Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query.",
                "Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search.",
                "On the other hand, stemming is much more conservative since most of the time, stemming preserves the original search intent.",
                "While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision.",
                "The increase on recall is obvious.",
                "With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded.",
                "On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the expanded query.",
                "This is similar to the relevance feedback in the sense that it requires multiple passes retrieval.",
                "If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful.",
                "It is expensive to deploy this in production Web search engines.",
                "Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine.",
                "In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search - stemming.",
                "Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision.",
                "Its simple, yet very efficient and effective, making real time stemming feasible for Web search.",
                "Our results will affirm researchers that stemming is indeed very important to large scale information retrieval. 3.",
                "CONTEXT SENSITIVE STEMMING 3.1 Overview Our system has four components as illustrated in Figure 1: candidate generation, query segmentation and head word detection, context sensitive query stemming and context sensitive document matching.",
                "Candidate generation (component 1) is performed oﬄine and generated candidates are stored in a dictionary.",
                "For an input query, we first segment query into concepts and detect the head word for each concept (component 2).",
                "We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4).",
                "Below we discuss each of the components in more detail.",
                "Component 4: context sensitive document matching Input Query: and head word detection Component 2: segment Component 1: candidate generation comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: hotel comparisons hotel −> hotels Figure 1: System Components 3.2 Expansion candidate generation One of the ways to generate candidates is using the Porter stemmer [18].",
                "The Porter stemmer simply uses morphological rules to convert a word to its base form.",
                "It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past.",
                "A more conservative way is based on using corpus analysis to improve the Porter stemmer results [26].",
                "The corpus analysis we do is based on word distributional similarity [15].",
                "The rationale of using distributional word similarity is that true variants tend to be used in similar contexts.",
                "In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word.",
                "We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus.",
                "The similarity between two words is the cosine similarity between the two corresponding feature vectors.",
                "The top 20 similar words to develop is shown in the following table. rank candidate score rank candidate score 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word develop.",
                "Column score is the similarity score.",
                "To determine the stemming candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list.",
                "After applying these rules, for the word develop, the stemming candidates are developing, developed, develops, development, developement, developer, developmental.",
                "For the pluralization handling purpose, only the candidate develops is retained.",
                "One thing we note from observing the distributionally similar words is that they are closely related semantically.",
                "These words might serve as candidates for general query expansion, a topic we will investigate in the future. 3.3 Segmentation and headword identification For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts.",
                "We first break a query into segments, each segment representing a concept which normally is a noun phrase.",
                "For each of the noun phrases, we then detect the most important word which we call the head word.",
                "Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity.",
                "To break a query into segments, we have to define a criteria to measure the strength of the relation between words.",
                "One effective method is to use mutual information as an indicator on whether or not to split two words [19].",
                "We use a log of 25M queries and collect the bigram and unigram frequencies from it.",
                "For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word.",
                "We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here.",
                "Table 2 shows some examples of query segmentation.",
                "The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query.",
                "Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: Query segmentation: a segment is bracketed. parsing since many queries are not grammatical and are very short.",
                "Applying a parser trained on sentences from documents to queries will have poor performance.",
                "In our solution, we just use simple heuristics rules, and it works very well in practice for English.",
                "For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of/in/at/from UVW.",
                "In such cases, the head word is typically the last nonstop word of XYZ. 3.4 Context sensitive word expansion After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful.",
                "Our statistics show that about half of the queries can be transformed by pluralization via naive stemming.",
                "Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse.",
                "Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost.",
                "In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful.",
                "Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts.",
                "Head words hotel and comparison can be expanded to hotels and comparisons.",
                "Are both transformations useful?",
                "To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web.",
                "The more likely a query to occur on the Web, the more relevant documents this query is able to return.",
                "Now the whole problem becomes how to calculate the probability of query to occur on the Web.",
                "Calculating the probability of string occurring in a corpus is a well known language modeling problem.",
                "The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur).",
                "The simplest and most successful approach to language modeling is still based on the n-gram model.",
                "By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.",
                "Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus.",
                "Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary.",
                "This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6).",
                "Also, because of the heavy tailed nature of language (i.e.",
                "Zipfs law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling.",
                "One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator.",
                "Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5].",
                "We used absolute smoothing in our experiments.",
                "Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string.",
                "Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3.",
                "We can see that all alternatives are less likely than the input query.",
                "It is therefore not useful to make an expansion for this query.",
                "On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded.",
                "To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score.",
                "Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face. 3.5 Context sensitive document matching Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants.",
                "For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison.",
                "However, not every occurrence of comparison in the document is of interest.",
                "A page which is about comparing customer service can contain all of the words hotel price comparisons comparison.",
                "This page is not a good page for the query.",
                "If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval.",
                "To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document.",
                "A variant match is considered valid only if the variant occurs in the same context as the original word does.",
                "The context is the left or the right non-stop segments 1 of the original word.",
                "Taking the same query as an example, the context of comparisons is price.",
                "The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price.",
                "Thus, we should only match those occurrences of comparison in the document if they occur after the word price.",
                "Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size.",
                "If the expanded word comparison occurs within the context of price within a window, it is considered valid.",
                "The smaller the window size is, the more restrictive the matching.",
                "We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases. 4.",
                "EXPERIMENTAL EVALUATION 4.1 Evaluation metrics We will measure both relevance improvement and the stemming cost required to achieve the relevance. 1 a context segment can not be a single stop word. 4.1.1 Relevance measurement We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11].",
                "Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) . (7) where gk is the weight for the document at rank k. Higher degree of relevance corresponds to a higher weight.",
                "A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights.",
                "We use dcg to represent the average DCG(5) over a set of test queries. 4.1.2 Stemming cost Another metric is to measure the additional cost incurred by stemming.",
                "Given the same level of relevance improvement, we prefer a stemming method that has less additional cost.",
                "We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed. 4.2 Data preparation We randomly sample 870 queries from a three month query log, with 290 from each month.",
                "Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming.",
                "We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words.",
                "In the end, we have 529 correctly spelled queries with at least 2 words. 4.3 Naive stemming for Web search Before explaining the experiments and results in detail, wed like to describe the traditional way of using stemming for Web search, referred as the naive model.",
                "This is to treat every word variant equivalent for all possible words in the query.",
                "The query book store will be transformed into (book OR books)(store OR stores) when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments. 4.4 Experimental setup The baseline model is the model without stemming.",
                "We first run the naive model to see how well it performs over the baseline.",
                "Then we improve the naive stemming model by document sensitive matching, referred as document sensitive matching model.",
                "This model makes the same stemming as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5.",
                "The naive model and document sensitive matching model stem the most queries.",
                "Out of the 529 queries, there are 408 queries that they stem, corresponding to 46.7% query traffic (out of a total of 870).",
                "We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as selective stemming model.",
                "Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model.",
                "We experiment with unigram language model and bigram language model.",
                "Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments.",
                "To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data.",
                "The oracle model only expands a word if the stemming will give better results.",
                "To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries.",
                "Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words. 4.5 Results We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5.",
                "Each row in Table 4 is a stemming strategy described in section 4.4.",
                "The first column is the name of the strategy.",
                "The second column is the number of queries affected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg.",
                "The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy).",
                "The fourth column is the relative improvement over the baseline, and the last column is the p-value of Wilcoxon significance test.",
                "There are several observations about the results.",
                "We can see the naively stemming only obtains a statistically insignificant improvement of 1.5%.",
                "Looking at Table 5, it gives an improvement of 2.7% on short queries.",
                "However, it also hurts long queries by -2.4%.",
                "Overall, the improvement is canceled out.",
                "The reason that it improves short queries is that most short queries only have one word that can be stemmed.",
                "Thus, blindly pluralizing short queries is relatively safe.",
                "However for long queries, most queries can have multiple words that can be pluralized.",
                "Expanding all of them without selection will significantly hurt precision.",
                "Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8%.",
                "The improvement comes from the conservative context sensitive document matching.",
                "An expanded word is valid only if it occurs within the context of original query in the document.",
                "This reduces many spurious matches.",
                "However, we still notice that for long queries, context sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem.",
                "While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches.",
                "It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases.",
                "Selective word pluralization further helps resolving the problem faced by document context sensitive stemming.",
                "It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stemming in the first place.",
                "By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision.",
                "With the unigram language model, we can reduce the stemming cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4%.",
                "In particular, it gives significant improvements on long queries.",
                "The dcg gain is turned from negative to positive, from −1.6% to 1.1%.",
                "This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement.",
                "For short queries too, we observe both dcg improvement and stemming cost reduction with the unigram language model.",
                "The advantages of predictive word expansion with a language model is further boosted with a better bigram language model.",
                "The overall dcg gain is lifted from 3.4% to 3.9%, and stemming cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic.",
                "For short queries, bigram language model improves the dcg gain from 4.4% to 4.7%, and reduces stemming cost from 272/272 to 150/272.",
                "For long queries, bigram language model improves dcg gain from 1.1% to 2.5%, and reduces stemming cost from 136/136 to 100/136.",
                "We observe that the bigram language model gives a larger lift for long queries.",
                "This is because the uncertainty in long queries is larger and a more powerful language model is needed.",
                "We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation.",
                "Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying.",
                "For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram language model.",
                "For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram language model.",
                "We may gain additional benefit with a more powerful language model for long queries.",
                "However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem.",
                "These problems have to be addressed separately.",
                "Looking at the the upper-bound of overhead reduction for oracle stemming, 75% (308/408) of the naive stemmings are wasteful.",
                "We currently capture about half of them.",
                "Further reduction of the overhead requires sacrificing the dcg gain.",
                "Now we can compare the stemming strategies from a different aspect.",
                "Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only.",
                "We can see that the number of affected queries decreases as the stemming strategy becomes more accurate (dcg improvement).",
                "For the bigram language model, over the 250/408 stemmed queries, the dcg improvement is 6.1%.",
                "An interesting observation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more difficult queries (low dcg queries). 5.",
                "DISCUSSIONS 5.1 Language models from query vs. from Web As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web.",
                "The language model should describe the occurrence of the string on the Web.",
                "However, the query log is also a good resource. 2 Note that this upperbound is for pluralization handling only, not for general stemming.",
                "General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics.",
                "Affected Queries dcg dcg Improvement p-value baseline 0/408 7.102 N/A N/A naive model 408/408 7.206 1.5% 0.22 document context sensitive model 408/408 7.302 2.8% 0.014 selective model: unigram LM 300/408 7.321 3.4% 0.001 selective model: bigram LM 250/408 7.381 3.9% 0.001 oracle model 100/408 7.519 5.9% 0.001 Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming Short Query Results Affected Queries dcg Improvement p-value baseline 0/272 N/A N/A naive model 272/272 2.7% 0.48 document context sensitive model 272/272 4.2% 0.002 selective model: unigram LM 185/272 4.4% 0.001 selective model: bigram LM 150/272 4.7% 0.001 oracle model 71/272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p-value baseline 0/136 N/A N/A naive model 136/136 -2.4% 0.25 document context sensitive model 136/136 -1.6% 0.27 selective model: unigram LM 115/136 1.1% 0.001 selective model: bigram LM 100/136 2.5% 0.001 oracle model 29/136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results.",
                "To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a language model from the same query top 25M queries as used to learn segmentation, and use that for prediction.",
                "We observed a slight performance decrease compared to the model trained on Web frequencies.",
                "In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries.",
                "Thus, the query log can serve as a good approximation of the Web frequencies. 5.2 How linguistics helps Some linguistic knowledge is useful in stemming.",
                "For the pluralization handling case, pluralization and de-pluralization is not symmetric.",
                "A plural word used in a query indicates a special intent.",
                "For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California.",
                "A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank.",
                "To capture this intent, we have to make sure the document is a general page about hotels in new york.",
                "We do this by requiring that the plural word hotels appears in the document.",
                "On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information.",
                "We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property. 5.3 Error analysis One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after stemming.",
                "Generally speaking, pluralization or depluralization keeps the original intent.",
                "However, the intent could change in a few cases.",
                "For one example of such a query, job at apple, we pluralize job to jobs.",
                "This stemming makes the original query ambiguous.",
                "The query job OR jobs at apple has two intents.",
                "One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company.",
                "Thus, the results after query stemming returns Steve Jobs as one of the results in top 5.",
                "One solution is performing results set based analysis to check if the intent is changed.",
                "This is similar to relevance feedback and requires second phase ranking.",
                "A second type of mistakes is the entity/concept recognition problem, These include two kinds.",
                "One is that the stemmed word variant now matches part of an entity or concept.",
                "For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco.",
                "The results will match cookie jar in san francisco.",
                "Although cookie still means the same thing as cookies, cookie jar is a different concept.",
                "Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words.",
                "For example, quote ICE is pluralized to quote OR quotes ICE.",
                "The original intent for this query is searching for stock quote for ticker ICE.",
                "However, we noticed that among the top results, one of the results is Food quotes: Ice cream.",
                "This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408/408 7.102 7.206 1.5% document context sensitive model 408/408 7.102 7.302 2.8% selective model: unigram LM 300/408 5.904 6.187 4.8% selective model: bigram LM 250/408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the affected queries before/after applying stemming the pluralized word quotes.",
                "The unchanged word ICE matches part of the noun phrase ice cream here.",
                "To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words.",
                "A third type of mistakes occurs in long queries.",
                "For the query bar code reader software, two words are pluralized. code to codes and reader to readers.",
                "In fact, bar code reader in the original query is a strong concept and the internal words should not be changed.",
                "This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking.",
                "For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept. 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented a simple yet elegant way of stemming for Web search.",
                "It improves naive stemming in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side.",
                "Using pluralization handling as an example, experiments on a major Web search engine data show it significantly improves the Web relevance and reduces the stemming cost.",
                "It also significantly improves Web click through rate (details not reported in the paper).",
                "For the future work, we are investigating the problems we identified in the error analysis section.",
                "These include: entity and noun phrase matching mistakes, and improved segmentation. 7.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving Web Search Ranking by Incorporating User Behavior Information.",
                "In SIGIR, 2006. [2] E. Airio.",
                "Word Normalization and Decompounding in Mono- and Bilingual IR.",
                "Information Retrieval, 9:249-271, 2006. [3] P. Anick.",
                "Using Terminological Feedback for Web Search Refinement: a Log-based Study.",
                "In SIGIR, 2003. [4] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press/Addison Wesley, 1999. [5] S. Chen and J. Goodman.",
                "An Empirical Study of Smoothing Techniques for Language Modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [6] S. Cronen-Townsend, Y. Zhou, and B. Croft.",
                "A Framework for Selective Query Expansion.",
                "In CIKM, 2004. [7] H. Fang and C. Zhai.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In SIGIR, 2006. [8] W. B. Frakes.",
                "Term Conflation for Information Retrieval.",
                "In C. J. Rijsbergen, editor, Research and Development in Information Retrieval, pages 383-389.",
                "Cambridge University Press, 1984. [9] D. Harman.",
                "How Effective is Suffixing?",
                "JASIS, 42(1):7-15, 1991. [10] D. Hull.",
                "Stemming Algorithms - A Case Study for Detailed Evaluation.",
                "JASIS, 47(1):70-84, 1996. [11] K. Jarvelin and J. Kekalainen.",
                "Cumulated Gain-Based Evaluation Evaluation of IR Techniques.",
                "ACM TOIS, 20:422-446, 2002. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating Query Substitutions.",
                "In WWW, 2006. [13] W. Kraaij and R. Pohlmann.",
                "Viewing Stemming as Recall Enhancement.",
                "In SIGIR, 1996. [14] R. Krovetz.",
                "Viewing Morphology as an Inference Process.",
                "In SIGIR, 1993. [15] D. Lin.",
                "Automatic Retrieval and Clustering of Similar Words.",
                "In COLING-ACL, 1998. [16] J.",
                "B. Lovins.",
                "Development of a Stemming Algorithm.",
                "Mechanical Translation and Computational Linguistics, II:22-31, 1968. [17] M. Lennon and D. Peirce and B. Tarry and P. Willett.",
                "An Evaluation of Some Conflation Algorithms for Information Retrieval.",
                "Journal of Information Science, 3:177-188, 1981. [18] M. Porter.",
                "An Algorithm for Suffix Stripping.",
                "Program, 14(3):130-137, 1980. [19] K. M. Risvik, T. Mikolajewski, and P. Boros.",
                "Query Segmentation for Web Search.",
                "In WWW, 2003. [20] S. E. Robertson.",
                "On Term Selection for Query Expansion.",
                "Journal of Documentation, 46(4):359-364, 1990. [21] G. Salton and C. Buckley.",
                "Improving Retrieval Performance by Relevance Feedback.",
                "JASIS, 41(4):288 - 297, 1999. [22] R. Sun, C.-H. Ong, and T.-S. Chua.",
                "Mining Dependency Relations for Query Expansion in Passage Retrieval.",
                "In SIGIR, 2006. [23] C. Van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, second version, 1979. [24] B. V´elez, R. Weiss, M. A. Sheldon, and D. K. Gifford.",
                "Fast and Effective Query Refinement.",
                "In SIGIR, 1997. [25] J. Xu and B. Croft.",
                "Query Expansion using Local and Global Document Analysis.",
                "In SIGIR, 1996. [26] J. Xu and B. Croft.",
                "Corpus-based Stemming using Cooccurrence of Word Variants.",
                "ACM TOIS, 16 (1):61-81, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "porter stemmer": {
            "translated_key": "Porter Stemmer",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context Sensitive Stemming for Web Search Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo!",
                "Inc. 701 First Avenue Sunnyvale, California 94089 {fuchun, nawaaz, xinli, yumaol}@yahoo-inc.com ABSTRACT Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms.",
                "Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation.",
                "In this paper, we propose a context sensitive stemming method that addresses these two issues.",
                "Two unique properties make our approach feasible for Web Search.",
                "First, based on statistical language modeling, we perform context sensitive analysis on the query side.",
                "We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine.",
                "This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time.",
                "Second, our approach performs a context sensitive document matching for those expanded variants.",
                "This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision.",
                "Using word pluralization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Storage and Retrieval-Query formulation General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Web search has now become a major tool in our daily lives for information seeking.",
                "One of the important issues in Web search is that user queries are often not best formulated to get optimal results.",
                "For example, running shoe is a query that occurs frequently in query logs.",
                "However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes.",
                "Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs.",
                "This is difficult even for experienced users, and especially difficult for non-native speakers.",
                "One traditional solution is to use stemming [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term.",
                "Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing.",
                "Stemming can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing.",
                "Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows handling each query differently, and hence is preferred.",
                "Although traditional stemming increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched.",
                "When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6].",
                "In addition, it reduces system performance because the search engine has to match all the word variants.",
                "As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa.",
                "Thus, one needs to be very cautious when using stemming in Web search engines.",
                "One problem of traditional stemming is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word.",
                "For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored.",
                "For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query.",
                "Transforming book store to match book stores is fine, but matching book storing or booking store is not.",
                "A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query.",
                "However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20].",
                "A second problem of traditional stemming is its blind matching of all occurrences in documents.",
                "For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store.",
                "Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected.",
                "Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases.",
                "To alleviate these two problems, we propose a context sensitive stemming approach for Web search.",
                "Our solution consists of two context sensitive analysis, one on the query side and the other on the document side.",
                "On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms.",
                "On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query.",
                "Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines.",
                "We use pluralization handling as a running example for our stemming approach.",
                "The motivation for using pluralization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance.",
                "As far as we know, no previous research has systematically investigated the usage of pluralization in Web search.",
                "As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion.",
                "Experiments on general stemming yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper.",
                "In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2.",
                "We describe the details of the context sensitive stemming approach in Section 3.",
                "We then perform extensive experiments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5.",
                "Finally, we conclude the paper in Section 6. 2.",
                "RELATED WORK Stemming is a long studied technology.",
                "Many stemmers have been developed, such as the Lovins stemmer [16] and the <br>porter stemmer</br> [18].",
                "The <br>porter stemmer</br> is widely used due to its simplicity and effectiveness in many applications.",
                "However, the Porter stemming makes many mistakes because its simple rules cannot fully describe English morphology.",
                "Corpus analysis is used to improve <br>porter stemmer</br> [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23].",
                "We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates.",
                "Using stemming in information retrieval is also a well known technique [8, 10].",
                "However, the effectiveness of stemming for English query systems was previously reported to be rather limited.",
                "Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance.",
                "Later, Harman [9] compares three general stemming techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper).",
                "They also proposed selective stemming based on query length and term importance, but no positive results were reported.",
                "On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%).",
                "However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search.",
                "These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2].",
                "We suspect the previous failures were mainly due to the two problems we mentioned in the introduction.",
                "Blind stemming, or a simple query length based selective stemming as used in [9] is not enough.",
                "Stemming has to be decided on case by case basis, not only at the query level but also at the document level.",
                "As we will show, if handled correctly, significant improvement can be achieved.",
                "A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25].",
                "To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21].",
                "This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search.",
                "In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift).",
                "Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query.",
                "Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search.",
                "On the other hand, stemming is much more conservative since most of the time, stemming preserves the original search intent.",
                "While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision.",
                "The increase on recall is obvious.",
                "With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded.",
                "On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the expanded query.",
                "This is similar to the relevance feedback in the sense that it requires multiple passes retrieval.",
                "If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful.",
                "It is expensive to deploy this in production Web search engines.",
                "Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine.",
                "In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search - stemming.",
                "Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision.",
                "Its simple, yet very efficient and effective, making real time stemming feasible for Web search.",
                "Our results will affirm researchers that stemming is indeed very important to large scale information retrieval. 3.",
                "CONTEXT SENSITIVE STEMMING 3.1 Overview Our system has four components as illustrated in Figure 1: candidate generation, query segmentation and head word detection, context sensitive query stemming and context sensitive document matching.",
                "Candidate generation (component 1) is performed oﬄine and generated candidates are stored in a dictionary.",
                "For an input query, we first segment query into concepts and detect the head word for each concept (component 2).",
                "We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4).",
                "Below we discuss each of the components in more detail.",
                "Component 4: context sensitive document matching Input Query: and head word detection Component 2: segment Component 1: candidate generation comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: hotel comparisons hotel −> hotels Figure 1: System Components 3.2 Expansion candidate generation One of the ways to generate candidates is using the <br>porter stemmer</br> [18].",
                "The <br>porter stemmer</br> simply uses morphological rules to convert a word to its base form.",
                "It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past.",
                "A more conservative way is based on using corpus analysis to improve the <br>porter stemmer</br> results [26].",
                "The corpus analysis we do is based on word distributional similarity [15].",
                "The rationale of using distributional word similarity is that true variants tend to be used in similar contexts.",
                "In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word.",
                "We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus.",
                "The similarity between two words is the cosine similarity between the two corresponding feature vectors.",
                "The top 20 similar words to develop is shown in the following table. rank candidate score rank candidate score 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word develop.",
                "Column score is the similarity score.",
                "To determine the stemming candidates, we apply a few <br>porter stemmer</br> [18] morphological rules to the similarity list.",
                "After applying these rules, for the word develop, the stemming candidates are developing, developed, develops, development, developement, developer, developmental.",
                "For the pluralization handling purpose, only the candidate develops is retained.",
                "One thing we note from observing the distributionally similar words is that they are closely related semantically.",
                "These words might serve as candidates for general query expansion, a topic we will investigate in the future. 3.3 Segmentation and headword identification For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts.",
                "We first break a query into segments, each segment representing a concept which normally is a noun phrase.",
                "For each of the noun phrases, we then detect the most important word which we call the head word.",
                "Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity.",
                "To break a query into segments, we have to define a criteria to measure the strength of the relation between words.",
                "One effective method is to use mutual information as an indicator on whether or not to split two words [19].",
                "We use a log of 25M queries and collect the bigram and unigram frequencies from it.",
                "For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word.",
                "We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here.",
                "Table 2 shows some examples of query segmentation.",
                "The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query.",
                "Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: Query segmentation: a segment is bracketed. parsing since many queries are not grammatical and are very short.",
                "Applying a parser trained on sentences from documents to queries will have poor performance.",
                "In our solution, we just use simple heuristics rules, and it works very well in practice for English.",
                "For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of/in/at/from UVW.",
                "In such cases, the head word is typically the last nonstop word of XYZ. 3.4 Context sensitive word expansion After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful.",
                "Our statistics show that about half of the queries can be transformed by pluralization via naive stemming.",
                "Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse.",
                "Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost.",
                "In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful.",
                "Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts.",
                "Head words hotel and comparison can be expanded to hotels and comparisons.",
                "Are both transformations useful?",
                "To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web.",
                "The more likely a query to occur on the Web, the more relevant documents this query is able to return.",
                "Now the whole problem becomes how to calculate the probability of query to occur on the Web.",
                "Calculating the probability of string occurring in a corpus is a well known language modeling problem.",
                "The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur).",
                "The simplest and most successful approach to language modeling is still based on the n-gram model.",
                "By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.",
                "Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus.",
                "Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary.",
                "This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6).",
                "Also, because of the heavy tailed nature of language (i.e.",
                "Zipfs law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling.",
                "One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator.",
                "Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5].",
                "We used absolute smoothing in our experiments.",
                "Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string.",
                "Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3.",
                "We can see that all alternatives are less likely than the input query.",
                "It is therefore not useful to make an expansion for this query.",
                "On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded.",
                "To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score.",
                "Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face. 3.5 Context sensitive document matching Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants.",
                "For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison.",
                "However, not every occurrence of comparison in the document is of interest.",
                "A page which is about comparing customer service can contain all of the words hotel price comparisons comparison.",
                "This page is not a good page for the query.",
                "If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval.",
                "To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document.",
                "A variant match is considered valid only if the variant occurs in the same context as the original word does.",
                "The context is the left or the right non-stop segments 1 of the original word.",
                "Taking the same query as an example, the context of comparisons is price.",
                "The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price.",
                "Thus, we should only match those occurrences of comparison in the document if they occur after the word price.",
                "Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size.",
                "If the expanded word comparison occurs within the context of price within a window, it is considered valid.",
                "The smaller the window size is, the more restrictive the matching.",
                "We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases. 4.",
                "EXPERIMENTAL EVALUATION 4.1 Evaluation metrics We will measure both relevance improvement and the stemming cost required to achieve the relevance. 1 a context segment can not be a single stop word. 4.1.1 Relevance measurement We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11].",
                "Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) . (7) where gk is the weight for the document at rank k. Higher degree of relevance corresponds to a higher weight.",
                "A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights.",
                "We use dcg to represent the average DCG(5) over a set of test queries. 4.1.2 Stemming cost Another metric is to measure the additional cost incurred by stemming.",
                "Given the same level of relevance improvement, we prefer a stemming method that has less additional cost.",
                "We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed. 4.2 Data preparation We randomly sample 870 queries from a three month query log, with 290 from each month.",
                "Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming.",
                "We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words.",
                "In the end, we have 529 correctly spelled queries with at least 2 words. 4.3 Naive stemming for Web search Before explaining the experiments and results in detail, wed like to describe the traditional way of using stemming for Web search, referred as the naive model.",
                "This is to treat every word variant equivalent for all possible words in the query.",
                "The query book store will be transformed into (book OR books)(store OR stores) when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments. 4.4 Experimental setup The baseline model is the model without stemming.",
                "We first run the naive model to see how well it performs over the baseline.",
                "Then we improve the naive stemming model by document sensitive matching, referred as document sensitive matching model.",
                "This model makes the same stemming as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5.",
                "The naive model and document sensitive matching model stem the most queries.",
                "Out of the 529 queries, there are 408 queries that they stem, corresponding to 46.7% query traffic (out of a total of 870).",
                "We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as selective stemming model.",
                "Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model.",
                "We experiment with unigram language model and bigram language model.",
                "Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments.",
                "To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data.",
                "The oracle model only expands a word if the stemming will give better results.",
                "To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries.",
                "Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words. 4.5 Results We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5.",
                "Each row in Table 4 is a stemming strategy described in section 4.4.",
                "The first column is the name of the strategy.",
                "The second column is the number of queries affected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg.",
                "The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy).",
                "The fourth column is the relative improvement over the baseline, and the last column is the p-value of Wilcoxon significance test.",
                "There are several observations about the results.",
                "We can see the naively stemming only obtains a statistically insignificant improvement of 1.5%.",
                "Looking at Table 5, it gives an improvement of 2.7% on short queries.",
                "However, it also hurts long queries by -2.4%.",
                "Overall, the improvement is canceled out.",
                "The reason that it improves short queries is that most short queries only have one word that can be stemmed.",
                "Thus, blindly pluralizing short queries is relatively safe.",
                "However for long queries, most queries can have multiple words that can be pluralized.",
                "Expanding all of them without selection will significantly hurt precision.",
                "Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8%.",
                "The improvement comes from the conservative context sensitive document matching.",
                "An expanded word is valid only if it occurs within the context of original query in the document.",
                "This reduces many spurious matches.",
                "However, we still notice that for long queries, context sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem.",
                "While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches.",
                "It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases.",
                "Selective word pluralization further helps resolving the problem faced by document context sensitive stemming.",
                "It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stemming in the first place.",
                "By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision.",
                "With the unigram language model, we can reduce the stemming cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4%.",
                "In particular, it gives significant improvements on long queries.",
                "The dcg gain is turned from negative to positive, from −1.6% to 1.1%.",
                "This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement.",
                "For short queries too, we observe both dcg improvement and stemming cost reduction with the unigram language model.",
                "The advantages of predictive word expansion with a language model is further boosted with a better bigram language model.",
                "The overall dcg gain is lifted from 3.4% to 3.9%, and stemming cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic.",
                "For short queries, bigram language model improves the dcg gain from 4.4% to 4.7%, and reduces stemming cost from 272/272 to 150/272.",
                "For long queries, bigram language model improves dcg gain from 1.1% to 2.5%, and reduces stemming cost from 136/136 to 100/136.",
                "We observe that the bigram language model gives a larger lift for long queries.",
                "This is because the uncertainty in long queries is larger and a more powerful language model is needed.",
                "We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation.",
                "Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying.",
                "For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram language model.",
                "For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram language model.",
                "We may gain additional benefit with a more powerful language model for long queries.",
                "However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem.",
                "These problems have to be addressed separately.",
                "Looking at the the upper-bound of overhead reduction for oracle stemming, 75% (308/408) of the naive stemmings are wasteful.",
                "We currently capture about half of them.",
                "Further reduction of the overhead requires sacrificing the dcg gain.",
                "Now we can compare the stemming strategies from a different aspect.",
                "Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only.",
                "We can see that the number of affected queries decreases as the stemming strategy becomes more accurate (dcg improvement).",
                "For the bigram language model, over the 250/408 stemmed queries, the dcg improvement is 6.1%.",
                "An interesting observation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more difficult queries (low dcg queries). 5.",
                "DISCUSSIONS 5.1 Language models from query vs. from Web As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web.",
                "The language model should describe the occurrence of the string on the Web.",
                "However, the query log is also a good resource. 2 Note that this upperbound is for pluralization handling only, not for general stemming.",
                "General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics.",
                "Affected Queries dcg dcg Improvement p-value baseline 0/408 7.102 N/A N/A naive model 408/408 7.206 1.5% 0.22 document context sensitive model 408/408 7.302 2.8% 0.014 selective model: unigram LM 300/408 7.321 3.4% 0.001 selective model: bigram LM 250/408 7.381 3.9% 0.001 oracle model 100/408 7.519 5.9% 0.001 Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming Short Query Results Affected Queries dcg Improvement p-value baseline 0/272 N/A N/A naive model 272/272 2.7% 0.48 document context sensitive model 272/272 4.2% 0.002 selective model: unigram LM 185/272 4.4% 0.001 selective model: bigram LM 150/272 4.7% 0.001 oracle model 71/272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p-value baseline 0/136 N/A N/A naive model 136/136 -2.4% 0.25 document context sensitive model 136/136 -1.6% 0.27 selective model: unigram LM 115/136 1.1% 0.001 selective model: bigram LM 100/136 2.5% 0.001 oracle model 29/136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results.",
                "To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a language model from the same query top 25M queries as used to learn segmentation, and use that for prediction.",
                "We observed a slight performance decrease compared to the model trained on Web frequencies.",
                "In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries.",
                "Thus, the query log can serve as a good approximation of the Web frequencies. 5.2 How linguistics helps Some linguistic knowledge is useful in stemming.",
                "For the pluralization handling case, pluralization and de-pluralization is not symmetric.",
                "A plural word used in a query indicates a special intent.",
                "For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California.",
                "A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank.",
                "To capture this intent, we have to make sure the document is a general page about hotels in new york.",
                "We do this by requiring that the plural word hotels appears in the document.",
                "On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information.",
                "We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property. 5.3 Error analysis One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after stemming.",
                "Generally speaking, pluralization or depluralization keeps the original intent.",
                "However, the intent could change in a few cases.",
                "For one example of such a query, job at apple, we pluralize job to jobs.",
                "This stemming makes the original query ambiguous.",
                "The query job OR jobs at apple has two intents.",
                "One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company.",
                "Thus, the results after query stemming returns Steve Jobs as one of the results in top 5.",
                "One solution is performing results set based analysis to check if the intent is changed.",
                "This is similar to relevance feedback and requires second phase ranking.",
                "A second type of mistakes is the entity/concept recognition problem, These include two kinds.",
                "One is that the stemmed word variant now matches part of an entity or concept.",
                "For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco.",
                "The results will match cookie jar in san francisco.",
                "Although cookie still means the same thing as cookies, cookie jar is a different concept.",
                "Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words.",
                "For example, quote ICE is pluralized to quote OR quotes ICE.",
                "The original intent for this query is searching for stock quote for ticker ICE.",
                "However, we noticed that among the top results, one of the results is Food quotes: Ice cream.",
                "This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408/408 7.102 7.206 1.5% document context sensitive model 408/408 7.102 7.302 2.8% selective model: unigram LM 300/408 5.904 6.187 4.8% selective model: bigram LM 250/408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the affected queries before/after applying stemming the pluralized word quotes.",
                "The unchanged word ICE matches part of the noun phrase ice cream here.",
                "To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words.",
                "A third type of mistakes occurs in long queries.",
                "For the query bar code reader software, two words are pluralized. code to codes and reader to readers.",
                "In fact, bar code reader in the original query is a strong concept and the internal words should not be changed.",
                "This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking.",
                "For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept. 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented a simple yet elegant way of stemming for Web search.",
                "It improves naive stemming in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side.",
                "Using pluralization handling as an example, experiments on a major Web search engine data show it significantly improves the Web relevance and reduces the stemming cost.",
                "It also significantly improves Web click through rate (details not reported in the paper).",
                "For the future work, we are investigating the problems we identified in the error analysis section.",
                "These include: entity and noun phrase matching mistakes, and improved segmentation. 7.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving Web Search Ranking by Incorporating User Behavior Information.",
                "In SIGIR, 2006. [2] E. Airio.",
                "Word Normalization and Decompounding in Mono- and Bilingual IR.",
                "Information Retrieval, 9:249-271, 2006. [3] P. Anick.",
                "Using Terminological Feedback for Web Search Refinement: a Log-based Study.",
                "In SIGIR, 2003. [4] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press/Addison Wesley, 1999. [5] S. Chen and J. Goodman.",
                "An Empirical Study of Smoothing Techniques for Language Modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [6] S. Cronen-Townsend, Y. Zhou, and B. Croft.",
                "A Framework for Selective Query Expansion.",
                "In CIKM, 2004. [7] H. Fang and C. Zhai.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In SIGIR, 2006. [8] W. B. Frakes.",
                "Term Conflation for Information Retrieval.",
                "In C. J. Rijsbergen, editor, Research and Development in Information Retrieval, pages 383-389.",
                "Cambridge University Press, 1984. [9] D. Harman.",
                "How Effective is Suffixing?",
                "JASIS, 42(1):7-15, 1991. [10] D. Hull.",
                "Stemming Algorithms - A Case Study for Detailed Evaluation.",
                "JASIS, 47(1):70-84, 1996. [11] K. Jarvelin and J. Kekalainen.",
                "Cumulated Gain-Based Evaluation Evaluation of IR Techniques.",
                "ACM TOIS, 20:422-446, 2002. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating Query Substitutions.",
                "In WWW, 2006. [13] W. Kraaij and R. Pohlmann.",
                "Viewing Stemming as Recall Enhancement.",
                "In SIGIR, 1996. [14] R. Krovetz.",
                "Viewing Morphology as an Inference Process.",
                "In SIGIR, 1993. [15] D. Lin.",
                "Automatic Retrieval and Clustering of Similar Words.",
                "In COLING-ACL, 1998. [16] J.",
                "B. Lovins.",
                "Development of a Stemming Algorithm.",
                "Mechanical Translation and Computational Linguistics, II:22-31, 1968. [17] M. Lennon and D. Peirce and B. Tarry and P. Willett.",
                "An Evaluation of Some Conflation Algorithms for Information Retrieval.",
                "Journal of Information Science, 3:177-188, 1981. [18] M. Porter.",
                "An Algorithm for Suffix Stripping.",
                "Program, 14(3):130-137, 1980. [19] K. M. Risvik, T. Mikolajewski, and P. Boros.",
                "Query Segmentation for Web Search.",
                "In WWW, 2003. [20] S. E. Robertson.",
                "On Term Selection for Query Expansion.",
                "Journal of Documentation, 46(4):359-364, 1990. [21] G. Salton and C. Buckley.",
                "Improving Retrieval Performance by Relevance Feedback.",
                "JASIS, 41(4):288 - 297, 1999. [22] R. Sun, C.-H. Ong, and T.-S. Chua.",
                "Mining Dependency Relations for Query Expansion in Passage Retrieval.",
                "In SIGIR, 2006. [23] C. Van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, second version, 1979. [24] B. V´elez, R. Weiss, M. A. Sheldon, and D. K. Gifford.",
                "Fast and Effective Query Refinement.",
                "In SIGIR, 1997. [25] J. Xu and B. Croft.",
                "Query Expansion using Local and Global Document Analysis.",
                "In SIGIR, 1996. [26] J. Xu and B. Croft.",
                "Corpus-based Stemming using Cooccurrence of Word Variants.",
                "ACM TOIS, 16 (1):61-81, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Se han desarrollado muchos taladros, como el Lovins Stemmer [16] y el \"Porter Stemmer\" [18].",
                "El \"Porter Stemmer\" se usa ampliamente debido a su simplicidad y efectividad en muchas aplicaciones.",
                "El análisis de corpus se utiliza para mejorar el \"Porter Stemmer\" [26] creando clases de equivalencia para palabras que son morfológicamente similares y ocurren en un contexto similar a la medida de la información mutua esperada [23].",
                "Componente 4: Consulta de entrada de coincidencia del documento sensible al contexto: y el componente de detección de palabras de la cabeza 2: Componente del segmento 1: Comparación de generación de candidatos −> Comparación de comparación 3: Decisión selectiva de expansión de palabras: comparaciones -> Ejemplo de comparación: Comparación de precios del hotel Salida: Hotel Hotel Hotel> Hoteles Figura 1: Componentes del sistema 3.2 Generación de candidatos de expansión Una de las formas de generar candidatos es usar el \"Porter Stemmer\" [18].",
                "El \"Porter Stemmer\" simplemente usa reglas morfológicas para convertir una palabra en su forma base.",
                "Una forma más conservadora se basa en el uso del análisis de corpus para mejorar los resultados de \"Porter Stemmer\" [26].",
                "Para determinar a los candidatos a la derecha, aplicamos algunas reglas morfológicas \"Porter Stemmer\" [18] a la lista de similitud."
            ],
            "translated_text": "",
            "candidates": [
                "Porter Stemmer",
                "Porter Stemmer",
                "Porter Stemmer",
                "Porter Stemmer",
                "Porter Stemmer",
                "Porter Stemmer",
                "Porter Stemmer",
                "Porter Stemmer",
                "Porter Stemmer",
                "Porter Stemmer",
                "Porter Stemmer",
                "Porter Stemmer",
                "Porter Stemmer",
                "Porter Stemmer"
            ],
            "error": []
        },
        "candidate generation": {
            "translated_key": "Generación de candidatos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context Sensitive Stemming for Web Search Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo!",
                "Inc. 701 First Avenue Sunnyvale, California 94089 {fuchun, nawaaz, xinli, yumaol}@yahoo-inc.com ABSTRACT Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms.",
                "Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation.",
                "In this paper, we propose a context sensitive stemming method that addresses these two issues.",
                "Two unique properties make our approach feasible for Web Search.",
                "First, based on statistical language modeling, we perform context sensitive analysis on the query side.",
                "We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine.",
                "This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time.",
                "Second, our approach performs a context sensitive document matching for those expanded variants.",
                "This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision.",
                "Using word pluralization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Storage and Retrieval-Query formulation General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Web search has now become a major tool in our daily lives for information seeking.",
                "One of the important issues in Web search is that user queries are often not best formulated to get optimal results.",
                "For example, running shoe is a query that occurs frequently in query logs.",
                "However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes.",
                "Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs.",
                "This is difficult even for experienced users, and especially difficult for non-native speakers.",
                "One traditional solution is to use stemming [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term.",
                "Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing.",
                "Stemming can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing.",
                "Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows handling each query differently, and hence is preferred.",
                "Although traditional stemming increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched.",
                "When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6].",
                "In addition, it reduces system performance because the search engine has to match all the word variants.",
                "As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa.",
                "Thus, one needs to be very cautious when using stemming in Web search engines.",
                "One problem of traditional stemming is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word.",
                "For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored.",
                "For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query.",
                "Transforming book store to match book stores is fine, but matching book storing or booking store is not.",
                "A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query.",
                "However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20].",
                "A second problem of traditional stemming is its blind matching of all occurrences in documents.",
                "For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store.",
                "Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected.",
                "Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases.",
                "To alleviate these two problems, we propose a context sensitive stemming approach for Web search.",
                "Our solution consists of two context sensitive analysis, one on the query side and the other on the document side.",
                "On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms.",
                "On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query.",
                "Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines.",
                "We use pluralization handling as a running example for our stemming approach.",
                "The motivation for using pluralization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance.",
                "As far as we know, no previous research has systematically investigated the usage of pluralization in Web search.",
                "As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion.",
                "Experiments on general stemming yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper.",
                "In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2.",
                "We describe the details of the context sensitive stemming approach in Section 3.",
                "We then perform extensive experiments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5.",
                "Finally, we conclude the paper in Section 6. 2.",
                "RELATED WORK Stemming is a long studied technology.",
                "Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18].",
                "The Porter stemmer is widely used due to its simplicity and effectiveness in many applications.",
                "However, the Porter stemming makes many mistakes because its simple rules cannot fully describe English morphology.",
                "Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23].",
                "We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates.",
                "Using stemming in information retrieval is also a well known technique [8, 10].",
                "However, the effectiveness of stemming for English query systems was previously reported to be rather limited.",
                "Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance.",
                "Later, Harman [9] compares three general stemming techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper).",
                "They also proposed selective stemming based on query length and term importance, but no positive results were reported.",
                "On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%).",
                "However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search.",
                "These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2].",
                "We suspect the previous failures were mainly due to the two problems we mentioned in the introduction.",
                "Blind stemming, or a simple query length based selective stemming as used in [9] is not enough.",
                "Stemming has to be decided on case by case basis, not only at the query level but also at the document level.",
                "As we will show, if handled correctly, significant improvement can be achieved.",
                "A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25].",
                "To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21].",
                "This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search.",
                "In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift).",
                "Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query.",
                "Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search.",
                "On the other hand, stemming is much more conservative since most of the time, stemming preserves the original search intent.",
                "While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision.",
                "The increase on recall is obvious.",
                "With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded.",
                "On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the expanded query.",
                "This is similar to the relevance feedback in the sense that it requires multiple passes retrieval.",
                "If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful.",
                "It is expensive to deploy this in production Web search engines.",
                "Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine.",
                "In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search - stemming.",
                "Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision.",
                "Its simple, yet very efficient and effective, making real time stemming feasible for Web search.",
                "Our results will affirm researchers that stemming is indeed very important to large scale information retrieval. 3.",
                "CONTEXT SENSITIVE STEMMING 3.1 Overview Our system has four components as illustrated in Figure 1: <br>candidate generation</br>, query segmentation and head word detection, context sensitive query stemming and context sensitive document matching.",
                "<br>candidate generation</br> (component 1) is performed oﬄine and generated candidates are stored in a dictionary.",
                "For an input query, we first segment query into concepts and detect the head word for each concept (component 2).",
                "We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4).",
                "Below we discuss each of the components in more detail.",
                "Component 4: context sensitive document matching Input Query: and head word detection Component 2: segment Component 1: <br>candidate generation</br> comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: hotel comparisons hotel −> hotels Figure 1: System Components 3.2 Expansion <br>candidate generation</br> One of the ways to generate candidates is using the Porter stemmer [18].",
                "The Porter stemmer simply uses morphological rules to convert a word to its base form.",
                "It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past.",
                "A more conservative way is based on using corpus analysis to improve the Porter stemmer results [26].",
                "The corpus analysis we do is based on word distributional similarity [15].",
                "The rationale of using distributional word similarity is that true variants tend to be used in similar contexts.",
                "In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word.",
                "We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus.",
                "The similarity between two words is the cosine similarity between the two corresponding feature vectors.",
                "The top 20 similar words to develop is shown in the following table. rank candidate score rank candidate score 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word develop.",
                "Column score is the similarity score.",
                "To determine the stemming candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list.",
                "After applying these rules, for the word develop, the stemming candidates are developing, developed, develops, development, developement, developer, developmental.",
                "For the pluralization handling purpose, only the candidate develops is retained.",
                "One thing we note from observing the distributionally similar words is that they are closely related semantically.",
                "These words might serve as candidates for general query expansion, a topic we will investigate in the future. 3.3 Segmentation and headword identification For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts.",
                "We first break a query into segments, each segment representing a concept which normally is a noun phrase.",
                "For each of the noun phrases, we then detect the most important word which we call the head word.",
                "Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity.",
                "To break a query into segments, we have to define a criteria to measure the strength of the relation between words.",
                "One effective method is to use mutual information as an indicator on whether or not to split two words [19].",
                "We use a log of 25M queries and collect the bigram and unigram frequencies from it.",
                "For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word.",
                "We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here.",
                "Table 2 shows some examples of query segmentation.",
                "The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query.",
                "Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: Query segmentation: a segment is bracketed. parsing since many queries are not grammatical and are very short.",
                "Applying a parser trained on sentences from documents to queries will have poor performance.",
                "In our solution, we just use simple heuristics rules, and it works very well in practice for English.",
                "For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of/in/at/from UVW.",
                "In such cases, the head word is typically the last nonstop word of XYZ. 3.4 Context sensitive word expansion After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful.",
                "Our statistics show that about half of the queries can be transformed by pluralization via naive stemming.",
                "Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse.",
                "Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost.",
                "In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful.",
                "Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts.",
                "Head words hotel and comparison can be expanded to hotels and comparisons.",
                "Are both transformations useful?",
                "To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web.",
                "The more likely a query to occur on the Web, the more relevant documents this query is able to return.",
                "Now the whole problem becomes how to calculate the probability of query to occur on the Web.",
                "Calculating the probability of string occurring in a corpus is a well known language modeling problem.",
                "The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur).",
                "The simplest and most successful approach to language modeling is still based on the n-gram model.",
                "By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.",
                "Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus.",
                "Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary.",
                "This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6).",
                "Also, because of the heavy tailed nature of language (i.e.",
                "Zipfs law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling.",
                "One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator.",
                "Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5].",
                "We used absolute smoothing in our experiments.",
                "Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string.",
                "Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3.",
                "We can see that all alternatives are less likely than the input query.",
                "It is therefore not useful to make an expansion for this query.",
                "On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded.",
                "To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score.",
                "Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face. 3.5 Context sensitive document matching Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants.",
                "For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison.",
                "However, not every occurrence of comparison in the document is of interest.",
                "A page which is about comparing customer service can contain all of the words hotel price comparisons comparison.",
                "This page is not a good page for the query.",
                "If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval.",
                "To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document.",
                "A variant match is considered valid only if the variant occurs in the same context as the original word does.",
                "The context is the left or the right non-stop segments 1 of the original word.",
                "Taking the same query as an example, the context of comparisons is price.",
                "The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price.",
                "Thus, we should only match those occurrences of comparison in the document if they occur after the word price.",
                "Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size.",
                "If the expanded word comparison occurs within the context of price within a window, it is considered valid.",
                "The smaller the window size is, the more restrictive the matching.",
                "We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases. 4.",
                "EXPERIMENTAL EVALUATION 4.1 Evaluation metrics We will measure both relevance improvement and the stemming cost required to achieve the relevance. 1 a context segment can not be a single stop word. 4.1.1 Relevance measurement We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11].",
                "Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) . (7) where gk is the weight for the document at rank k. Higher degree of relevance corresponds to a higher weight.",
                "A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights.",
                "We use dcg to represent the average DCG(5) over a set of test queries. 4.1.2 Stemming cost Another metric is to measure the additional cost incurred by stemming.",
                "Given the same level of relevance improvement, we prefer a stemming method that has less additional cost.",
                "We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed. 4.2 Data preparation We randomly sample 870 queries from a three month query log, with 290 from each month.",
                "Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming.",
                "We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words.",
                "In the end, we have 529 correctly spelled queries with at least 2 words. 4.3 Naive stemming for Web search Before explaining the experiments and results in detail, wed like to describe the traditional way of using stemming for Web search, referred as the naive model.",
                "This is to treat every word variant equivalent for all possible words in the query.",
                "The query book store will be transformed into (book OR books)(store OR stores) when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments. 4.4 Experimental setup The baseline model is the model without stemming.",
                "We first run the naive model to see how well it performs over the baseline.",
                "Then we improve the naive stemming model by document sensitive matching, referred as document sensitive matching model.",
                "This model makes the same stemming as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5.",
                "The naive model and document sensitive matching model stem the most queries.",
                "Out of the 529 queries, there are 408 queries that they stem, corresponding to 46.7% query traffic (out of a total of 870).",
                "We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as selective stemming model.",
                "Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model.",
                "We experiment with unigram language model and bigram language model.",
                "Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments.",
                "To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data.",
                "The oracle model only expands a word if the stemming will give better results.",
                "To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries.",
                "Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words. 4.5 Results We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5.",
                "Each row in Table 4 is a stemming strategy described in section 4.4.",
                "The first column is the name of the strategy.",
                "The second column is the number of queries affected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg.",
                "The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy).",
                "The fourth column is the relative improvement over the baseline, and the last column is the p-value of Wilcoxon significance test.",
                "There are several observations about the results.",
                "We can see the naively stemming only obtains a statistically insignificant improvement of 1.5%.",
                "Looking at Table 5, it gives an improvement of 2.7% on short queries.",
                "However, it also hurts long queries by -2.4%.",
                "Overall, the improvement is canceled out.",
                "The reason that it improves short queries is that most short queries only have one word that can be stemmed.",
                "Thus, blindly pluralizing short queries is relatively safe.",
                "However for long queries, most queries can have multiple words that can be pluralized.",
                "Expanding all of them without selection will significantly hurt precision.",
                "Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8%.",
                "The improvement comes from the conservative context sensitive document matching.",
                "An expanded word is valid only if it occurs within the context of original query in the document.",
                "This reduces many spurious matches.",
                "However, we still notice that for long queries, context sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem.",
                "While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches.",
                "It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases.",
                "Selective word pluralization further helps resolving the problem faced by document context sensitive stemming.",
                "It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stemming in the first place.",
                "By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision.",
                "With the unigram language model, we can reduce the stemming cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4%.",
                "In particular, it gives significant improvements on long queries.",
                "The dcg gain is turned from negative to positive, from −1.6% to 1.1%.",
                "This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement.",
                "For short queries too, we observe both dcg improvement and stemming cost reduction with the unigram language model.",
                "The advantages of predictive word expansion with a language model is further boosted with a better bigram language model.",
                "The overall dcg gain is lifted from 3.4% to 3.9%, and stemming cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic.",
                "For short queries, bigram language model improves the dcg gain from 4.4% to 4.7%, and reduces stemming cost from 272/272 to 150/272.",
                "For long queries, bigram language model improves dcg gain from 1.1% to 2.5%, and reduces stemming cost from 136/136 to 100/136.",
                "We observe that the bigram language model gives a larger lift for long queries.",
                "This is because the uncertainty in long queries is larger and a more powerful language model is needed.",
                "We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation.",
                "Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying.",
                "For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram language model.",
                "For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram language model.",
                "We may gain additional benefit with a more powerful language model for long queries.",
                "However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem.",
                "These problems have to be addressed separately.",
                "Looking at the the upper-bound of overhead reduction for oracle stemming, 75% (308/408) of the naive stemmings are wasteful.",
                "We currently capture about half of them.",
                "Further reduction of the overhead requires sacrificing the dcg gain.",
                "Now we can compare the stemming strategies from a different aspect.",
                "Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only.",
                "We can see that the number of affected queries decreases as the stemming strategy becomes more accurate (dcg improvement).",
                "For the bigram language model, over the 250/408 stemmed queries, the dcg improvement is 6.1%.",
                "An interesting observation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more difficult queries (low dcg queries). 5.",
                "DISCUSSIONS 5.1 Language models from query vs. from Web As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web.",
                "The language model should describe the occurrence of the string on the Web.",
                "However, the query log is also a good resource. 2 Note that this upperbound is for pluralization handling only, not for general stemming.",
                "General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics.",
                "Affected Queries dcg dcg Improvement p-value baseline 0/408 7.102 N/A N/A naive model 408/408 7.206 1.5% 0.22 document context sensitive model 408/408 7.302 2.8% 0.014 selective model: unigram LM 300/408 7.321 3.4% 0.001 selective model: bigram LM 250/408 7.381 3.9% 0.001 oracle model 100/408 7.519 5.9% 0.001 Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming Short Query Results Affected Queries dcg Improvement p-value baseline 0/272 N/A N/A naive model 272/272 2.7% 0.48 document context sensitive model 272/272 4.2% 0.002 selective model: unigram LM 185/272 4.4% 0.001 selective model: bigram LM 150/272 4.7% 0.001 oracle model 71/272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p-value baseline 0/136 N/A N/A naive model 136/136 -2.4% 0.25 document context sensitive model 136/136 -1.6% 0.27 selective model: unigram LM 115/136 1.1% 0.001 selective model: bigram LM 100/136 2.5% 0.001 oracle model 29/136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results.",
                "To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a language model from the same query top 25M queries as used to learn segmentation, and use that for prediction.",
                "We observed a slight performance decrease compared to the model trained on Web frequencies.",
                "In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries.",
                "Thus, the query log can serve as a good approximation of the Web frequencies. 5.2 How linguistics helps Some linguistic knowledge is useful in stemming.",
                "For the pluralization handling case, pluralization and de-pluralization is not symmetric.",
                "A plural word used in a query indicates a special intent.",
                "For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California.",
                "A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank.",
                "To capture this intent, we have to make sure the document is a general page about hotels in new york.",
                "We do this by requiring that the plural word hotels appears in the document.",
                "On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information.",
                "We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property. 5.3 Error analysis One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after stemming.",
                "Generally speaking, pluralization or depluralization keeps the original intent.",
                "However, the intent could change in a few cases.",
                "For one example of such a query, job at apple, we pluralize job to jobs.",
                "This stemming makes the original query ambiguous.",
                "The query job OR jobs at apple has two intents.",
                "One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company.",
                "Thus, the results after query stemming returns Steve Jobs as one of the results in top 5.",
                "One solution is performing results set based analysis to check if the intent is changed.",
                "This is similar to relevance feedback and requires second phase ranking.",
                "A second type of mistakes is the entity/concept recognition problem, These include two kinds.",
                "One is that the stemmed word variant now matches part of an entity or concept.",
                "For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco.",
                "The results will match cookie jar in san francisco.",
                "Although cookie still means the same thing as cookies, cookie jar is a different concept.",
                "Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words.",
                "For example, quote ICE is pluralized to quote OR quotes ICE.",
                "The original intent for this query is searching for stock quote for ticker ICE.",
                "However, we noticed that among the top results, one of the results is Food quotes: Ice cream.",
                "This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408/408 7.102 7.206 1.5% document context sensitive model 408/408 7.102 7.302 2.8% selective model: unigram LM 300/408 5.904 6.187 4.8% selective model: bigram LM 250/408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the affected queries before/after applying stemming the pluralized word quotes.",
                "The unchanged word ICE matches part of the noun phrase ice cream here.",
                "To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words.",
                "A third type of mistakes occurs in long queries.",
                "For the query bar code reader software, two words are pluralized. code to codes and reader to readers.",
                "In fact, bar code reader in the original query is a strong concept and the internal words should not be changed.",
                "This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking.",
                "For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept. 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented a simple yet elegant way of stemming for Web search.",
                "It improves naive stemming in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side.",
                "Using pluralization handling as an example, experiments on a major Web search engine data show it significantly improves the Web relevance and reduces the stemming cost.",
                "It also significantly improves Web click through rate (details not reported in the paper).",
                "For the future work, we are investigating the problems we identified in the error analysis section.",
                "These include: entity and noun phrase matching mistakes, and improved segmentation. 7.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving Web Search Ranking by Incorporating User Behavior Information.",
                "In SIGIR, 2006. [2] E. Airio.",
                "Word Normalization and Decompounding in Mono- and Bilingual IR.",
                "Information Retrieval, 9:249-271, 2006. [3] P. Anick.",
                "Using Terminological Feedback for Web Search Refinement: a Log-based Study.",
                "In SIGIR, 2003. [4] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press/Addison Wesley, 1999. [5] S. Chen and J. Goodman.",
                "An Empirical Study of Smoothing Techniques for Language Modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [6] S. Cronen-Townsend, Y. Zhou, and B. Croft.",
                "A Framework for Selective Query Expansion.",
                "In CIKM, 2004. [7] H. Fang and C. Zhai.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In SIGIR, 2006. [8] W. B. Frakes.",
                "Term Conflation for Information Retrieval.",
                "In C. J. Rijsbergen, editor, Research and Development in Information Retrieval, pages 383-389.",
                "Cambridge University Press, 1984. [9] D. Harman.",
                "How Effective is Suffixing?",
                "JASIS, 42(1):7-15, 1991. [10] D. Hull.",
                "Stemming Algorithms - A Case Study for Detailed Evaluation.",
                "JASIS, 47(1):70-84, 1996. [11] K. Jarvelin and J. Kekalainen.",
                "Cumulated Gain-Based Evaluation Evaluation of IR Techniques.",
                "ACM TOIS, 20:422-446, 2002. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating Query Substitutions.",
                "In WWW, 2006. [13] W. Kraaij and R. Pohlmann.",
                "Viewing Stemming as Recall Enhancement.",
                "In SIGIR, 1996. [14] R. Krovetz.",
                "Viewing Morphology as an Inference Process.",
                "In SIGIR, 1993. [15] D. Lin.",
                "Automatic Retrieval and Clustering of Similar Words.",
                "In COLING-ACL, 1998. [16] J.",
                "B. Lovins.",
                "Development of a Stemming Algorithm.",
                "Mechanical Translation and Computational Linguistics, II:22-31, 1968. [17] M. Lennon and D. Peirce and B. Tarry and P. Willett.",
                "An Evaluation of Some Conflation Algorithms for Information Retrieval.",
                "Journal of Information Science, 3:177-188, 1981. [18] M. Porter.",
                "An Algorithm for Suffix Stripping.",
                "Program, 14(3):130-137, 1980. [19] K. M. Risvik, T. Mikolajewski, and P. Boros.",
                "Query Segmentation for Web Search.",
                "In WWW, 2003. [20] S. E. Robertson.",
                "On Term Selection for Query Expansion.",
                "Journal of Documentation, 46(4):359-364, 1990. [21] G. Salton and C. Buckley.",
                "Improving Retrieval Performance by Relevance Feedback.",
                "JASIS, 41(4):288 - 297, 1999. [22] R. Sun, C.-H. Ong, and T.-S. Chua.",
                "Mining Dependency Relations for Query Expansion in Passage Retrieval.",
                "In SIGIR, 2006. [23] C. Van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, second version, 1979. [24] B. V´elez, R. Weiss, M. A. Sheldon, and D. K. Gifford.",
                "Fast and Effective Query Refinement.",
                "In SIGIR, 1997. [25] J. Xu and B. Croft.",
                "Query Expansion using Local and Global Document Analysis.",
                "In SIGIR, 1996. [26] J. Xu and B. Croft.",
                "Corpus-based Stemming using Cooccurrence of Word Variants.",
                "ACM TOIS, 16 (1):61-81, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Context Sentitive Stemming 3.1 Descripción general Nuestro sistema tiene cuatro componentes como se ilustra en la Figura 1: \"Generación de candidatos\", segmentación de consultas y detección de palabras de cabeza, consulta sensible al contexto de consulta y coincidencia de documentos sensibles al contexto.",
                "La \"generación de candidatos\" (componente 1) se realiza o ﬄ ine y los candidatos generados se almacenan en un diccionario.",
                "Componente 4: Consulta de entrada de coincidencia del documento sensible al contexto: y la detección de palabras de la cabeza Componente 2: Componente del segmento 1: Comparación de \"Generación de candidatos\" Comparaciones -> Componente de comparación 3: Decisión selectiva de expansión de palabras: Comparaciones -> Ejemplo de comparación: Comparación de precios del hotel Salida: Hotel ComparacionesHotel -> Hoteles Figura 1: Componentes del sistema 3.2 Expansión \"Generación de candidatos\" Una de las formas de generar candidatos es usar el Porter Stemmer [18]."
            ],
            "translated_text": "",
            "candidates": [
                "generación candidata",
                "Generación de candidatos",
                "generación candidata",
                "generación de candidatos",
                "generación candidata",
                "Generación de candidatos",
                "Generación de candidatos"
            ],
            "error": []
        },
        "query segmentation": {
            "translated_key": "segmentación de consultas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context Sensitive Stemming for Web Search Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo!",
                "Inc. 701 First Avenue Sunnyvale, California 94089 {fuchun, nawaaz, xinli, yumaol}@yahoo-inc.com ABSTRACT Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms.",
                "Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation.",
                "In this paper, we propose a context sensitive stemming method that addresses these two issues.",
                "Two unique properties make our approach feasible for Web Search.",
                "First, based on statistical language modeling, we perform context sensitive analysis on the query side.",
                "We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine.",
                "This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time.",
                "Second, our approach performs a context sensitive document matching for those expanded variants.",
                "This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision.",
                "Using word pluralization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Storage and Retrieval-Query formulation General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Web search has now become a major tool in our daily lives for information seeking.",
                "One of the important issues in Web search is that user queries are often not best formulated to get optimal results.",
                "For example, running shoe is a query that occurs frequently in query logs.",
                "However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes.",
                "Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs.",
                "This is difficult even for experienced users, and especially difficult for non-native speakers.",
                "One traditional solution is to use stemming [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term.",
                "Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing.",
                "Stemming can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing.",
                "Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows handling each query differently, and hence is preferred.",
                "Although traditional stemming increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched.",
                "When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6].",
                "In addition, it reduces system performance because the search engine has to match all the word variants.",
                "As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa.",
                "Thus, one needs to be very cautious when using stemming in Web search engines.",
                "One problem of traditional stemming is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word.",
                "For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored.",
                "For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query.",
                "Transforming book store to match book stores is fine, but matching book storing or booking store is not.",
                "A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query.",
                "However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20].",
                "A second problem of traditional stemming is its blind matching of all occurrences in documents.",
                "For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store.",
                "Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected.",
                "Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases.",
                "To alleviate these two problems, we propose a context sensitive stemming approach for Web search.",
                "Our solution consists of two context sensitive analysis, one on the query side and the other on the document side.",
                "On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms.",
                "On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query.",
                "Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines.",
                "We use pluralization handling as a running example for our stemming approach.",
                "The motivation for using pluralization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance.",
                "As far as we know, no previous research has systematically investigated the usage of pluralization in Web search.",
                "As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion.",
                "Experiments on general stemming yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper.",
                "In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2.",
                "We describe the details of the context sensitive stemming approach in Section 3.",
                "We then perform extensive experiments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5.",
                "Finally, we conclude the paper in Section 6. 2.",
                "RELATED WORK Stemming is a long studied technology.",
                "Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18].",
                "The Porter stemmer is widely used due to its simplicity and effectiveness in many applications.",
                "However, the Porter stemming makes many mistakes because its simple rules cannot fully describe English morphology.",
                "Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23].",
                "We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates.",
                "Using stemming in information retrieval is also a well known technique [8, 10].",
                "However, the effectiveness of stemming for English query systems was previously reported to be rather limited.",
                "Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance.",
                "Later, Harman [9] compares three general stemming techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper).",
                "They also proposed selective stemming based on query length and term importance, but no positive results were reported.",
                "On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%).",
                "However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search.",
                "These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2].",
                "We suspect the previous failures were mainly due to the two problems we mentioned in the introduction.",
                "Blind stemming, or a simple query length based selective stemming as used in [9] is not enough.",
                "Stemming has to be decided on case by case basis, not only at the query level but also at the document level.",
                "As we will show, if handled correctly, significant improvement can be achieved.",
                "A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25].",
                "To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21].",
                "This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search.",
                "In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift).",
                "Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query.",
                "Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search.",
                "On the other hand, stemming is much more conservative since most of the time, stemming preserves the original search intent.",
                "While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision.",
                "The increase on recall is obvious.",
                "With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded.",
                "On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the expanded query.",
                "This is similar to the relevance feedback in the sense that it requires multiple passes retrieval.",
                "If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful.",
                "It is expensive to deploy this in production Web search engines.",
                "Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine.",
                "In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search - stemming.",
                "Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision.",
                "Its simple, yet very efficient and effective, making real time stemming feasible for Web search.",
                "Our results will affirm researchers that stemming is indeed very important to large scale information retrieval. 3.",
                "CONTEXT SENSITIVE STEMMING 3.1 Overview Our system has four components as illustrated in Figure 1: candidate generation, <br>query segmentation</br> and head word detection, context sensitive query stemming and context sensitive document matching.",
                "Candidate generation (component 1) is performed oﬄine and generated candidates are stored in a dictionary.",
                "For an input query, we first segment query into concepts and detect the head word for each concept (component 2).",
                "We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4).",
                "Below we discuss each of the components in more detail.",
                "Component 4: context sensitive document matching Input Query: and head word detection Component 2: segment Component 1: candidate generation comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: hotel comparisons hotel −> hotels Figure 1: System Components 3.2 Expansion candidate generation One of the ways to generate candidates is using the Porter stemmer [18].",
                "The Porter stemmer simply uses morphological rules to convert a word to its base form.",
                "It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past.",
                "A more conservative way is based on using corpus analysis to improve the Porter stemmer results [26].",
                "The corpus analysis we do is based on word distributional similarity [15].",
                "The rationale of using distributional word similarity is that true variants tend to be used in similar contexts.",
                "In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word.",
                "We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus.",
                "The similarity between two words is the cosine similarity between the two corresponding feature vectors.",
                "The top 20 similar words to develop is shown in the following table. rank candidate score rank candidate score 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word develop.",
                "Column score is the similarity score.",
                "To determine the stemming candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list.",
                "After applying these rules, for the word develop, the stemming candidates are developing, developed, develops, development, developement, developer, developmental.",
                "For the pluralization handling purpose, only the candidate develops is retained.",
                "One thing we note from observing the distributionally similar words is that they are closely related semantically.",
                "These words might serve as candidates for general query expansion, a topic we will investigate in the future. 3.3 Segmentation and headword identification For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts.",
                "We first break a query into segments, each segment representing a concept which normally is a noun phrase.",
                "For each of the noun phrases, we then detect the most important word which we call the head word.",
                "Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity.",
                "To break a query into segments, we have to define a criteria to measure the strength of the relation between words.",
                "One effective method is to use mutual information as an indicator on whether or not to split two words [19].",
                "We use a log of 25M queries and collect the bigram and unigram frequencies from it.",
                "For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word.",
                "We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here.",
                "Table 2 shows some examples of <br>query segmentation</br>.",
                "The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query.",
                "Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: <br>query segmentation</br>: a segment is bracketed. parsing since many queries are not grammatical and are very short.",
                "Applying a parser trained on sentences from documents to queries will have poor performance.",
                "In our solution, we just use simple heuristics rules, and it works very well in practice for English.",
                "For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of/in/at/from UVW.",
                "In such cases, the head word is typically the last nonstop word of XYZ. 3.4 Context sensitive word expansion After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful.",
                "Our statistics show that about half of the queries can be transformed by pluralization via naive stemming.",
                "Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse.",
                "Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost.",
                "In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful.",
                "Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts.",
                "Head words hotel and comparison can be expanded to hotels and comparisons.",
                "Are both transformations useful?",
                "To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web.",
                "The more likely a query to occur on the Web, the more relevant documents this query is able to return.",
                "Now the whole problem becomes how to calculate the probability of query to occur on the Web.",
                "Calculating the probability of string occurring in a corpus is a well known language modeling problem.",
                "The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur).",
                "The simplest and most successful approach to language modeling is still based on the n-gram model.",
                "By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.",
                "Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus.",
                "Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary.",
                "This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6).",
                "Also, because of the heavy tailed nature of language (i.e.",
                "Zipfs law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling.",
                "One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator.",
                "Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5].",
                "We used absolute smoothing in our experiments.",
                "Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string.",
                "Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3.",
                "We can see that all alternatives are less likely than the input query.",
                "It is therefore not useful to make an expansion for this query.",
                "On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded.",
                "To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score.",
                "Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face. 3.5 Context sensitive document matching Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants.",
                "For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison.",
                "However, not every occurrence of comparison in the document is of interest.",
                "A page which is about comparing customer service can contain all of the words hotel price comparisons comparison.",
                "This page is not a good page for the query.",
                "If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval.",
                "To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document.",
                "A variant match is considered valid only if the variant occurs in the same context as the original word does.",
                "The context is the left or the right non-stop segments 1 of the original word.",
                "Taking the same query as an example, the context of comparisons is price.",
                "The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price.",
                "Thus, we should only match those occurrences of comparison in the document if they occur after the word price.",
                "Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size.",
                "If the expanded word comparison occurs within the context of price within a window, it is considered valid.",
                "The smaller the window size is, the more restrictive the matching.",
                "We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases. 4.",
                "EXPERIMENTAL EVALUATION 4.1 Evaluation metrics We will measure both relevance improvement and the stemming cost required to achieve the relevance. 1 a context segment can not be a single stop word. 4.1.1 Relevance measurement We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11].",
                "Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) . (7) where gk is the weight for the document at rank k. Higher degree of relevance corresponds to a higher weight.",
                "A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights.",
                "We use dcg to represent the average DCG(5) over a set of test queries. 4.1.2 Stemming cost Another metric is to measure the additional cost incurred by stemming.",
                "Given the same level of relevance improvement, we prefer a stemming method that has less additional cost.",
                "We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed. 4.2 Data preparation We randomly sample 870 queries from a three month query log, with 290 from each month.",
                "Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming.",
                "We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words.",
                "In the end, we have 529 correctly spelled queries with at least 2 words. 4.3 Naive stemming for Web search Before explaining the experiments and results in detail, wed like to describe the traditional way of using stemming for Web search, referred as the naive model.",
                "This is to treat every word variant equivalent for all possible words in the query.",
                "The query book store will be transformed into (book OR books)(store OR stores) when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments. 4.4 Experimental setup The baseline model is the model without stemming.",
                "We first run the naive model to see how well it performs over the baseline.",
                "Then we improve the naive stemming model by document sensitive matching, referred as document sensitive matching model.",
                "This model makes the same stemming as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5.",
                "The naive model and document sensitive matching model stem the most queries.",
                "Out of the 529 queries, there are 408 queries that they stem, corresponding to 46.7% query traffic (out of a total of 870).",
                "We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as selective stemming model.",
                "Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model.",
                "We experiment with unigram language model and bigram language model.",
                "Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments.",
                "To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data.",
                "The oracle model only expands a word if the stemming will give better results.",
                "To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries.",
                "Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words. 4.5 Results We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5.",
                "Each row in Table 4 is a stemming strategy described in section 4.4.",
                "The first column is the name of the strategy.",
                "The second column is the number of queries affected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg.",
                "The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy).",
                "The fourth column is the relative improvement over the baseline, and the last column is the p-value of Wilcoxon significance test.",
                "There are several observations about the results.",
                "We can see the naively stemming only obtains a statistically insignificant improvement of 1.5%.",
                "Looking at Table 5, it gives an improvement of 2.7% on short queries.",
                "However, it also hurts long queries by -2.4%.",
                "Overall, the improvement is canceled out.",
                "The reason that it improves short queries is that most short queries only have one word that can be stemmed.",
                "Thus, blindly pluralizing short queries is relatively safe.",
                "However for long queries, most queries can have multiple words that can be pluralized.",
                "Expanding all of them without selection will significantly hurt precision.",
                "Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8%.",
                "The improvement comes from the conservative context sensitive document matching.",
                "An expanded word is valid only if it occurs within the context of original query in the document.",
                "This reduces many spurious matches.",
                "However, we still notice that for long queries, context sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem.",
                "While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches.",
                "It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases.",
                "Selective word pluralization further helps resolving the problem faced by document context sensitive stemming.",
                "It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stemming in the first place.",
                "By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision.",
                "With the unigram language model, we can reduce the stemming cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4%.",
                "In particular, it gives significant improvements on long queries.",
                "The dcg gain is turned from negative to positive, from −1.6% to 1.1%.",
                "This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement.",
                "For short queries too, we observe both dcg improvement and stemming cost reduction with the unigram language model.",
                "The advantages of predictive word expansion with a language model is further boosted with a better bigram language model.",
                "The overall dcg gain is lifted from 3.4% to 3.9%, and stemming cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic.",
                "For short queries, bigram language model improves the dcg gain from 4.4% to 4.7%, and reduces stemming cost from 272/272 to 150/272.",
                "For long queries, bigram language model improves dcg gain from 1.1% to 2.5%, and reduces stemming cost from 136/136 to 100/136.",
                "We observe that the bigram language model gives a larger lift for long queries.",
                "This is because the uncertainty in long queries is larger and a more powerful language model is needed.",
                "We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation.",
                "Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying.",
                "For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram language model.",
                "For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram language model.",
                "We may gain additional benefit with a more powerful language model for long queries.",
                "However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem.",
                "These problems have to be addressed separately.",
                "Looking at the the upper-bound of overhead reduction for oracle stemming, 75% (308/408) of the naive stemmings are wasteful.",
                "We currently capture about half of them.",
                "Further reduction of the overhead requires sacrificing the dcg gain.",
                "Now we can compare the stemming strategies from a different aspect.",
                "Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only.",
                "We can see that the number of affected queries decreases as the stemming strategy becomes more accurate (dcg improvement).",
                "For the bigram language model, over the 250/408 stemmed queries, the dcg improvement is 6.1%.",
                "An interesting observation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more difficult queries (low dcg queries). 5.",
                "DISCUSSIONS 5.1 Language models from query vs. from Web As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web.",
                "The language model should describe the occurrence of the string on the Web.",
                "However, the query log is also a good resource. 2 Note that this upperbound is for pluralization handling only, not for general stemming.",
                "General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics.",
                "Affected Queries dcg dcg Improvement p-value baseline 0/408 7.102 N/A N/A naive model 408/408 7.206 1.5% 0.22 document context sensitive model 408/408 7.302 2.8% 0.014 selective model: unigram LM 300/408 7.321 3.4% 0.001 selective model: bigram LM 250/408 7.381 3.9% 0.001 oracle model 100/408 7.519 5.9% 0.001 Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming Short Query Results Affected Queries dcg Improvement p-value baseline 0/272 N/A N/A naive model 272/272 2.7% 0.48 document context sensitive model 272/272 4.2% 0.002 selective model: unigram LM 185/272 4.4% 0.001 selective model: bigram LM 150/272 4.7% 0.001 oracle model 71/272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p-value baseline 0/136 N/A N/A naive model 136/136 -2.4% 0.25 document context sensitive model 136/136 -1.6% 0.27 selective model: unigram LM 115/136 1.1% 0.001 selective model: bigram LM 100/136 2.5% 0.001 oracle model 29/136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results.",
                "To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a language model from the same query top 25M queries as used to learn segmentation, and use that for prediction.",
                "We observed a slight performance decrease compared to the model trained on Web frequencies.",
                "In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries.",
                "Thus, the query log can serve as a good approximation of the Web frequencies. 5.2 How linguistics helps Some linguistic knowledge is useful in stemming.",
                "For the pluralization handling case, pluralization and de-pluralization is not symmetric.",
                "A plural word used in a query indicates a special intent.",
                "For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California.",
                "A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank.",
                "To capture this intent, we have to make sure the document is a general page about hotels in new york.",
                "We do this by requiring that the plural word hotels appears in the document.",
                "On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information.",
                "We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property. 5.3 Error analysis One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after stemming.",
                "Generally speaking, pluralization or depluralization keeps the original intent.",
                "However, the intent could change in a few cases.",
                "For one example of such a query, job at apple, we pluralize job to jobs.",
                "This stemming makes the original query ambiguous.",
                "The query job OR jobs at apple has two intents.",
                "One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company.",
                "Thus, the results after query stemming returns Steve Jobs as one of the results in top 5.",
                "One solution is performing results set based analysis to check if the intent is changed.",
                "This is similar to relevance feedback and requires second phase ranking.",
                "A second type of mistakes is the entity/concept recognition problem, These include two kinds.",
                "One is that the stemmed word variant now matches part of an entity or concept.",
                "For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco.",
                "The results will match cookie jar in san francisco.",
                "Although cookie still means the same thing as cookies, cookie jar is a different concept.",
                "Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words.",
                "For example, quote ICE is pluralized to quote OR quotes ICE.",
                "The original intent for this query is searching for stock quote for ticker ICE.",
                "However, we noticed that among the top results, one of the results is Food quotes: Ice cream.",
                "This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408/408 7.102 7.206 1.5% document context sensitive model 408/408 7.102 7.302 2.8% selective model: unigram LM 300/408 5.904 6.187 4.8% selective model: bigram LM 250/408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the affected queries before/after applying stemming the pluralized word quotes.",
                "The unchanged word ICE matches part of the noun phrase ice cream here.",
                "To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words.",
                "A third type of mistakes occurs in long queries.",
                "For the query bar code reader software, two words are pluralized. code to codes and reader to readers.",
                "In fact, bar code reader in the original query is a strong concept and the internal words should not be changed.",
                "This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking.",
                "For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept. 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented a simple yet elegant way of stemming for Web search.",
                "It improves naive stemming in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side.",
                "Using pluralization handling as an example, experiments on a major Web search engine data show it significantly improves the Web relevance and reduces the stemming cost.",
                "It also significantly improves Web click through rate (details not reported in the paper).",
                "For the future work, we are investigating the problems we identified in the error analysis section.",
                "These include: entity and noun phrase matching mistakes, and improved segmentation. 7.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving Web Search Ranking by Incorporating User Behavior Information.",
                "In SIGIR, 2006. [2] E. Airio.",
                "Word Normalization and Decompounding in Mono- and Bilingual IR.",
                "Information Retrieval, 9:249-271, 2006. [3] P. Anick.",
                "Using Terminological Feedback for Web Search Refinement: a Log-based Study.",
                "In SIGIR, 2003. [4] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press/Addison Wesley, 1999. [5] S. Chen and J. Goodman.",
                "An Empirical Study of Smoothing Techniques for Language Modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [6] S. Cronen-Townsend, Y. Zhou, and B. Croft.",
                "A Framework for Selective Query Expansion.",
                "In CIKM, 2004. [7] H. Fang and C. Zhai.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In SIGIR, 2006. [8] W. B. Frakes.",
                "Term Conflation for Information Retrieval.",
                "In C. J. Rijsbergen, editor, Research and Development in Information Retrieval, pages 383-389.",
                "Cambridge University Press, 1984. [9] D. Harman.",
                "How Effective is Suffixing?",
                "JASIS, 42(1):7-15, 1991. [10] D. Hull.",
                "Stemming Algorithms - A Case Study for Detailed Evaluation.",
                "JASIS, 47(1):70-84, 1996. [11] K. Jarvelin and J. Kekalainen.",
                "Cumulated Gain-Based Evaluation Evaluation of IR Techniques.",
                "ACM TOIS, 20:422-446, 2002. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating Query Substitutions.",
                "In WWW, 2006. [13] W. Kraaij and R. Pohlmann.",
                "Viewing Stemming as Recall Enhancement.",
                "In SIGIR, 1996. [14] R. Krovetz.",
                "Viewing Morphology as an Inference Process.",
                "In SIGIR, 1993. [15] D. Lin.",
                "Automatic Retrieval and Clustering of Similar Words.",
                "In COLING-ACL, 1998. [16] J.",
                "B. Lovins.",
                "Development of a Stemming Algorithm.",
                "Mechanical Translation and Computational Linguistics, II:22-31, 1968. [17] M. Lennon and D. Peirce and B. Tarry and P. Willett.",
                "An Evaluation of Some Conflation Algorithms for Information Retrieval.",
                "Journal of Information Science, 3:177-188, 1981. [18] M. Porter.",
                "An Algorithm for Suffix Stripping.",
                "Program, 14(3):130-137, 1980. [19] K. M. Risvik, T. Mikolajewski, and P. Boros.",
                "<br>query segmentation</br> for Web Search.",
                "In WWW, 2003. [20] S. E. Robertson.",
                "On Term Selection for Query Expansion.",
                "Journal of Documentation, 46(4):359-364, 1990. [21] G. Salton and C. Buckley.",
                "Improving Retrieval Performance by Relevance Feedback.",
                "JASIS, 41(4):288 - 297, 1999. [22] R. Sun, C.-H. Ong, and T.-S. Chua.",
                "Mining Dependency Relations for Query Expansion in Passage Retrieval.",
                "In SIGIR, 2006. [23] C. Van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, second version, 1979. [24] B. V´elez, R. Weiss, M. A. Sheldon, and D. K. Gifford.",
                "Fast and Effective Query Refinement.",
                "In SIGIR, 1997. [25] J. Xu and B. Croft.",
                "Query Expansion using Local and Global Document Analysis.",
                "In SIGIR, 1996. [26] J. Xu and B. Croft.",
                "Corpus-based Stemming using Cooccurrence of Word Variants.",
                "ACM TOIS, 16 (1):61-81, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Context Sensitive Stemming 3.1 Descripción general Nuestro sistema tiene cuatro componentes como se ilustra en la Figura 1: Generación de candidatos, \"segmentación de consultas\" y detección de palabras de la cabeza, consulta sensible al contexto de consulta y coincidencia de documentos sensibles al contexto.",
                "La Tabla 2 muestra algunos ejemplos de \"segmentación de consultas\".",
                "El análisis de consultas es más difícil que la oración [zapatilla de carrera] [mejor] [Nueva York] [Escuelas de medicina] [imágenes] [de] [Casa Blanca] [Cookies] [en] [San Francisco] [Hotel] [Comparación de precios] Table Table Table2: \"Segmentación de consulta\": un segmento se interpuso.Analizado ya que muchas consultas no son gramaticales y son muy cortas.",
                "\"Segmentación de consulta\" para la búsqueda web."
            ],
            "translated_text": "",
            "candidates": [
                "segmentación de consultas",
                "segmentación de consultas",
                "segmentación de consultas",
                "segmentación de consultas",
                "segmentación de consultas",
                "Segmentación de consulta",
                "segmentación de consultas",
                "Segmentación de consulta"
            ],
            "error": []
        },
        "head word detection": {
            "translated_key": "detección de palabras de cabeza",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context Sensitive Stemming for Web Search Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo!",
                "Inc. 701 First Avenue Sunnyvale, California 94089 {fuchun, nawaaz, xinli, yumaol}@yahoo-inc.com ABSTRACT Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms.",
                "Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation.",
                "In this paper, we propose a context sensitive stemming method that addresses these two issues.",
                "Two unique properties make our approach feasible for Web Search.",
                "First, based on statistical language modeling, we perform context sensitive analysis on the query side.",
                "We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine.",
                "This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time.",
                "Second, our approach performs a context sensitive document matching for those expanded variants.",
                "This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision.",
                "Using word pluralization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Storage and Retrieval-Query formulation General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Web search has now become a major tool in our daily lives for information seeking.",
                "One of the important issues in Web search is that user queries are often not best formulated to get optimal results.",
                "For example, running shoe is a query that occurs frequently in query logs.",
                "However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes.",
                "Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs.",
                "This is difficult even for experienced users, and especially difficult for non-native speakers.",
                "One traditional solution is to use stemming [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term.",
                "Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing.",
                "Stemming can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing.",
                "Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows handling each query differently, and hence is preferred.",
                "Although traditional stemming increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched.",
                "When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6].",
                "In addition, it reduces system performance because the search engine has to match all the word variants.",
                "As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa.",
                "Thus, one needs to be very cautious when using stemming in Web search engines.",
                "One problem of traditional stemming is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word.",
                "For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored.",
                "For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query.",
                "Transforming book store to match book stores is fine, but matching book storing or booking store is not.",
                "A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query.",
                "However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20].",
                "A second problem of traditional stemming is its blind matching of all occurrences in documents.",
                "For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store.",
                "Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected.",
                "Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases.",
                "To alleviate these two problems, we propose a context sensitive stemming approach for Web search.",
                "Our solution consists of two context sensitive analysis, one on the query side and the other on the document side.",
                "On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms.",
                "On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query.",
                "Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines.",
                "We use pluralization handling as a running example for our stemming approach.",
                "The motivation for using pluralization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance.",
                "As far as we know, no previous research has systematically investigated the usage of pluralization in Web search.",
                "As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion.",
                "Experiments on general stemming yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper.",
                "In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2.",
                "We describe the details of the context sensitive stemming approach in Section 3.",
                "We then perform extensive experiments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5.",
                "Finally, we conclude the paper in Section 6. 2.",
                "RELATED WORK Stemming is a long studied technology.",
                "Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18].",
                "The Porter stemmer is widely used due to its simplicity and effectiveness in many applications.",
                "However, the Porter stemming makes many mistakes because its simple rules cannot fully describe English morphology.",
                "Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23].",
                "We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates.",
                "Using stemming in information retrieval is also a well known technique [8, 10].",
                "However, the effectiveness of stemming for English query systems was previously reported to be rather limited.",
                "Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance.",
                "Later, Harman [9] compares three general stemming techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper).",
                "They also proposed selective stemming based on query length and term importance, but no positive results were reported.",
                "On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%).",
                "However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search.",
                "These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2].",
                "We suspect the previous failures were mainly due to the two problems we mentioned in the introduction.",
                "Blind stemming, or a simple query length based selective stemming as used in [9] is not enough.",
                "Stemming has to be decided on case by case basis, not only at the query level but also at the document level.",
                "As we will show, if handled correctly, significant improvement can be achieved.",
                "A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25].",
                "To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21].",
                "This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search.",
                "In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift).",
                "Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query.",
                "Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search.",
                "On the other hand, stemming is much more conservative since most of the time, stemming preserves the original search intent.",
                "While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision.",
                "The increase on recall is obvious.",
                "With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded.",
                "On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the expanded query.",
                "This is similar to the relevance feedback in the sense that it requires multiple passes retrieval.",
                "If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful.",
                "It is expensive to deploy this in production Web search engines.",
                "Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine.",
                "In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search - stemming.",
                "Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision.",
                "Its simple, yet very efficient and effective, making real time stemming feasible for Web search.",
                "Our results will affirm researchers that stemming is indeed very important to large scale information retrieval. 3.",
                "CONTEXT SENSITIVE STEMMING 3.1 Overview Our system has four components as illustrated in Figure 1: candidate generation, query segmentation and <br>head word detection</br>, context sensitive query stemming and context sensitive document matching.",
                "Candidate generation (component 1) is performed oﬄine and generated candidates are stored in a dictionary.",
                "For an input query, we first segment query into concepts and detect the head word for each concept (component 2).",
                "We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4).",
                "Below we discuss each of the components in more detail.",
                "Component 4: context sensitive document matching Input Query: and <br>head word detection</br> Component 2: segment Component 1: candidate generation comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: hotel comparisons hotel −> hotels Figure 1: System Components 3.2 Expansion candidate generation One of the ways to generate candidates is using the Porter stemmer [18].",
                "The Porter stemmer simply uses morphological rules to convert a word to its base form.",
                "It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past.",
                "A more conservative way is based on using corpus analysis to improve the Porter stemmer results [26].",
                "The corpus analysis we do is based on word distributional similarity [15].",
                "The rationale of using distributional word similarity is that true variants tend to be used in similar contexts.",
                "In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word.",
                "We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus.",
                "The similarity between two words is the cosine similarity between the two corresponding feature vectors.",
                "The top 20 similar words to develop is shown in the following table. rank candidate score rank candidate score 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word develop.",
                "Column score is the similarity score.",
                "To determine the stemming candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list.",
                "After applying these rules, for the word develop, the stemming candidates are developing, developed, develops, development, developement, developer, developmental.",
                "For the pluralization handling purpose, only the candidate develops is retained.",
                "One thing we note from observing the distributionally similar words is that they are closely related semantically.",
                "These words might serve as candidates for general query expansion, a topic we will investigate in the future. 3.3 Segmentation and headword identification For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts.",
                "We first break a query into segments, each segment representing a concept which normally is a noun phrase.",
                "For each of the noun phrases, we then detect the most important word which we call the head word.",
                "Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity.",
                "To break a query into segments, we have to define a criteria to measure the strength of the relation between words.",
                "One effective method is to use mutual information as an indicator on whether or not to split two words [19].",
                "We use a log of 25M queries and collect the bigram and unigram frequencies from it.",
                "For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word.",
                "We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here.",
                "Table 2 shows some examples of query segmentation.",
                "The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query.",
                "Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: Query segmentation: a segment is bracketed. parsing since many queries are not grammatical and are very short.",
                "Applying a parser trained on sentences from documents to queries will have poor performance.",
                "In our solution, we just use simple heuristics rules, and it works very well in practice for English.",
                "For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of/in/at/from UVW.",
                "In such cases, the head word is typically the last nonstop word of XYZ. 3.4 Context sensitive word expansion After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful.",
                "Our statistics show that about half of the queries can be transformed by pluralization via naive stemming.",
                "Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse.",
                "Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost.",
                "In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful.",
                "Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts.",
                "Head words hotel and comparison can be expanded to hotels and comparisons.",
                "Are both transformations useful?",
                "To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web.",
                "The more likely a query to occur on the Web, the more relevant documents this query is able to return.",
                "Now the whole problem becomes how to calculate the probability of query to occur on the Web.",
                "Calculating the probability of string occurring in a corpus is a well known language modeling problem.",
                "The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur).",
                "The simplest and most successful approach to language modeling is still based on the n-gram model.",
                "By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.",
                "Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus.",
                "Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary.",
                "This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6).",
                "Also, because of the heavy tailed nature of language (i.e.",
                "Zipfs law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling.",
                "One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator.",
                "Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5].",
                "We used absolute smoothing in our experiments.",
                "Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string.",
                "Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3.",
                "We can see that all alternatives are less likely than the input query.",
                "It is therefore not useful to make an expansion for this query.",
                "On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded.",
                "To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score.",
                "Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face. 3.5 Context sensitive document matching Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants.",
                "For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison.",
                "However, not every occurrence of comparison in the document is of interest.",
                "A page which is about comparing customer service can contain all of the words hotel price comparisons comparison.",
                "This page is not a good page for the query.",
                "If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval.",
                "To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document.",
                "A variant match is considered valid only if the variant occurs in the same context as the original word does.",
                "The context is the left or the right non-stop segments 1 of the original word.",
                "Taking the same query as an example, the context of comparisons is price.",
                "The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price.",
                "Thus, we should only match those occurrences of comparison in the document if they occur after the word price.",
                "Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size.",
                "If the expanded word comparison occurs within the context of price within a window, it is considered valid.",
                "The smaller the window size is, the more restrictive the matching.",
                "We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases. 4.",
                "EXPERIMENTAL EVALUATION 4.1 Evaluation metrics We will measure both relevance improvement and the stemming cost required to achieve the relevance. 1 a context segment can not be a single stop word. 4.1.1 Relevance measurement We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11].",
                "Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) . (7) where gk is the weight for the document at rank k. Higher degree of relevance corresponds to a higher weight.",
                "A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights.",
                "We use dcg to represent the average DCG(5) over a set of test queries. 4.1.2 Stemming cost Another metric is to measure the additional cost incurred by stemming.",
                "Given the same level of relevance improvement, we prefer a stemming method that has less additional cost.",
                "We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed. 4.2 Data preparation We randomly sample 870 queries from a three month query log, with 290 from each month.",
                "Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming.",
                "We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words.",
                "In the end, we have 529 correctly spelled queries with at least 2 words. 4.3 Naive stemming for Web search Before explaining the experiments and results in detail, wed like to describe the traditional way of using stemming for Web search, referred as the naive model.",
                "This is to treat every word variant equivalent for all possible words in the query.",
                "The query book store will be transformed into (book OR books)(store OR stores) when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments. 4.4 Experimental setup The baseline model is the model without stemming.",
                "We first run the naive model to see how well it performs over the baseline.",
                "Then we improve the naive stemming model by document sensitive matching, referred as document sensitive matching model.",
                "This model makes the same stemming as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5.",
                "The naive model and document sensitive matching model stem the most queries.",
                "Out of the 529 queries, there are 408 queries that they stem, corresponding to 46.7% query traffic (out of a total of 870).",
                "We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as selective stemming model.",
                "Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model.",
                "We experiment with unigram language model and bigram language model.",
                "Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments.",
                "To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data.",
                "The oracle model only expands a word if the stemming will give better results.",
                "To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries.",
                "Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words. 4.5 Results We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5.",
                "Each row in Table 4 is a stemming strategy described in section 4.4.",
                "The first column is the name of the strategy.",
                "The second column is the number of queries affected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg.",
                "The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy).",
                "The fourth column is the relative improvement over the baseline, and the last column is the p-value of Wilcoxon significance test.",
                "There are several observations about the results.",
                "We can see the naively stemming only obtains a statistically insignificant improvement of 1.5%.",
                "Looking at Table 5, it gives an improvement of 2.7% on short queries.",
                "However, it also hurts long queries by -2.4%.",
                "Overall, the improvement is canceled out.",
                "The reason that it improves short queries is that most short queries only have one word that can be stemmed.",
                "Thus, blindly pluralizing short queries is relatively safe.",
                "However for long queries, most queries can have multiple words that can be pluralized.",
                "Expanding all of them without selection will significantly hurt precision.",
                "Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8%.",
                "The improvement comes from the conservative context sensitive document matching.",
                "An expanded word is valid only if it occurs within the context of original query in the document.",
                "This reduces many spurious matches.",
                "However, we still notice that for long queries, context sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem.",
                "While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches.",
                "It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases.",
                "Selective word pluralization further helps resolving the problem faced by document context sensitive stemming.",
                "It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stemming in the first place.",
                "By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision.",
                "With the unigram language model, we can reduce the stemming cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4%.",
                "In particular, it gives significant improvements on long queries.",
                "The dcg gain is turned from negative to positive, from −1.6% to 1.1%.",
                "This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement.",
                "For short queries too, we observe both dcg improvement and stemming cost reduction with the unigram language model.",
                "The advantages of predictive word expansion with a language model is further boosted with a better bigram language model.",
                "The overall dcg gain is lifted from 3.4% to 3.9%, and stemming cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic.",
                "For short queries, bigram language model improves the dcg gain from 4.4% to 4.7%, and reduces stemming cost from 272/272 to 150/272.",
                "For long queries, bigram language model improves dcg gain from 1.1% to 2.5%, and reduces stemming cost from 136/136 to 100/136.",
                "We observe that the bigram language model gives a larger lift for long queries.",
                "This is because the uncertainty in long queries is larger and a more powerful language model is needed.",
                "We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation.",
                "Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying.",
                "For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram language model.",
                "For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram language model.",
                "We may gain additional benefit with a more powerful language model for long queries.",
                "However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem.",
                "These problems have to be addressed separately.",
                "Looking at the the upper-bound of overhead reduction for oracle stemming, 75% (308/408) of the naive stemmings are wasteful.",
                "We currently capture about half of them.",
                "Further reduction of the overhead requires sacrificing the dcg gain.",
                "Now we can compare the stemming strategies from a different aspect.",
                "Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only.",
                "We can see that the number of affected queries decreases as the stemming strategy becomes more accurate (dcg improvement).",
                "For the bigram language model, over the 250/408 stemmed queries, the dcg improvement is 6.1%.",
                "An interesting observation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more difficult queries (low dcg queries). 5.",
                "DISCUSSIONS 5.1 Language models from query vs. from Web As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web.",
                "The language model should describe the occurrence of the string on the Web.",
                "However, the query log is also a good resource. 2 Note that this upperbound is for pluralization handling only, not for general stemming.",
                "General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics.",
                "Affected Queries dcg dcg Improvement p-value baseline 0/408 7.102 N/A N/A naive model 408/408 7.206 1.5% 0.22 document context sensitive model 408/408 7.302 2.8% 0.014 selective model: unigram LM 300/408 7.321 3.4% 0.001 selective model: bigram LM 250/408 7.381 3.9% 0.001 oracle model 100/408 7.519 5.9% 0.001 Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming Short Query Results Affected Queries dcg Improvement p-value baseline 0/272 N/A N/A naive model 272/272 2.7% 0.48 document context sensitive model 272/272 4.2% 0.002 selective model: unigram LM 185/272 4.4% 0.001 selective model: bigram LM 150/272 4.7% 0.001 oracle model 71/272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p-value baseline 0/136 N/A N/A naive model 136/136 -2.4% 0.25 document context sensitive model 136/136 -1.6% 0.27 selective model: unigram LM 115/136 1.1% 0.001 selective model: bigram LM 100/136 2.5% 0.001 oracle model 29/136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results.",
                "To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a language model from the same query top 25M queries as used to learn segmentation, and use that for prediction.",
                "We observed a slight performance decrease compared to the model trained on Web frequencies.",
                "In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries.",
                "Thus, the query log can serve as a good approximation of the Web frequencies. 5.2 How linguistics helps Some linguistic knowledge is useful in stemming.",
                "For the pluralization handling case, pluralization and de-pluralization is not symmetric.",
                "A plural word used in a query indicates a special intent.",
                "For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California.",
                "A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank.",
                "To capture this intent, we have to make sure the document is a general page about hotels in new york.",
                "We do this by requiring that the plural word hotels appears in the document.",
                "On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information.",
                "We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property. 5.3 Error analysis One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after stemming.",
                "Generally speaking, pluralization or depluralization keeps the original intent.",
                "However, the intent could change in a few cases.",
                "For one example of such a query, job at apple, we pluralize job to jobs.",
                "This stemming makes the original query ambiguous.",
                "The query job OR jobs at apple has two intents.",
                "One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company.",
                "Thus, the results after query stemming returns Steve Jobs as one of the results in top 5.",
                "One solution is performing results set based analysis to check if the intent is changed.",
                "This is similar to relevance feedback and requires second phase ranking.",
                "A second type of mistakes is the entity/concept recognition problem, These include two kinds.",
                "One is that the stemmed word variant now matches part of an entity or concept.",
                "For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco.",
                "The results will match cookie jar in san francisco.",
                "Although cookie still means the same thing as cookies, cookie jar is a different concept.",
                "Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words.",
                "For example, quote ICE is pluralized to quote OR quotes ICE.",
                "The original intent for this query is searching for stock quote for ticker ICE.",
                "However, we noticed that among the top results, one of the results is Food quotes: Ice cream.",
                "This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408/408 7.102 7.206 1.5% document context sensitive model 408/408 7.102 7.302 2.8% selective model: unigram LM 300/408 5.904 6.187 4.8% selective model: bigram LM 250/408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the affected queries before/after applying stemming the pluralized word quotes.",
                "The unchanged word ICE matches part of the noun phrase ice cream here.",
                "To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words.",
                "A third type of mistakes occurs in long queries.",
                "For the query bar code reader software, two words are pluralized. code to codes and reader to readers.",
                "In fact, bar code reader in the original query is a strong concept and the internal words should not be changed.",
                "This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking.",
                "For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept. 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented a simple yet elegant way of stemming for Web search.",
                "It improves naive stemming in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side.",
                "Using pluralization handling as an example, experiments on a major Web search engine data show it significantly improves the Web relevance and reduces the stemming cost.",
                "It also significantly improves Web click through rate (details not reported in the paper).",
                "For the future work, we are investigating the problems we identified in the error analysis section.",
                "These include: entity and noun phrase matching mistakes, and improved segmentation. 7.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving Web Search Ranking by Incorporating User Behavior Information.",
                "In SIGIR, 2006. [2] E. Airio.",
                "Word Normalization and Decompounding in Mono- and Bilingual IR.",
                "Information Retrieval, 9:249-271, 2006. [3] P. Anick.",
                "Using Terminological Feedback for Web Search Refinement: a Log-based Study.",
                "In SIGIR, 2003. [4] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press/Addison Wesley, 1999. [5] S. Chen and J. Goodman.",
                "An Empirical Study of Smoothing Techniques for Language Modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [6] S. Cronen-Townsend, Y. Zhou, and B. Croft.",
                "A Framework for Selective Query Expansion.",
                "In CIKM, 2004. [7] H. Fang and C. Zhai.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In SIGIR, 2006. [8] W. B. Frakes.",
                "Term Conflation for Information Retrieval.",
                "In C. J. Rijsbergen, editor, Research and Development in Information Retrieval, pages 383-389.",
                "Cambridge University Press, 1984. [9] D. Harman.",
                "How Effective is Suffixing?",
                "JASIS, 42(1):7-15, 1991. [10] D. Hull.",
                "Stemming Algorithms - A Case Study for Detailed Evaluation.",
                "JASIS, 47(1):70-84, 1996. [11] K. Jarvelin and J. Kekalainen.",
                "Cumulated Gain-Based Evaluation Evaluation of IR Techniques.",
                "ACM TOIS, 20:422-446, 2002. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating Query Substitutions.",
                "In WWW, 2006. [13] W. Kraaij and R. Pohlmann.",
                "Viewing Stemming as Recall Enhancement.",
                "In SIGIR, 1996. [14] R. Krovetz.",
                "Viewing Morphology as an Inference Process.",
                "In SIGIR, 1993. [15] D. Lin.",
                "Automatic Retrieval and Clustering of Similar Words.",
                "In COLING-ACL, 1998. [16] J.",
                "B. Lovins.",
                "Development of a Stemming Algorithm.",
                "Mechanical Translation and Computational Linguistics, II:22-31, 1968. [17] M. Lennon and D. Peirce and B. Tarry and P. Willett.",
                "An Evaluation of Some Conflation Algorithms for Information Retrieval.",
                "Journal of Information Science, 3:177-188, 1981. [18] M. Porter.",
                "An Algorithm for Suffix Stripping.",
                "Program, 14(3):130-137, 1980. [19] K. M. Risvik, T. Mikolajewski, and P. Boros.",
                "Query Segmentation for Web Search.",
                "In WWW, 2003. [20] S. E. Robertson.",
                "On Term Selection for Query Expansion.",
                "Journal of Documentation, 46(4):359-364, 1990. [21] G. Salton and C. Buckley.",
                "Improving Retrieval Performance by Relevance Feedback.",
                "JASIS, 41(4):288 - 297, 1999. [22] R. Sun, C.-H. Ong, and T.-S. Chua.",
                "Mining Dependency Relations for Query Expansion in Passage Retrieval.",
                "In SIGIR, 2006. [23] C. Van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, second version, 1979. [24] B. V´elez, R. Weiss, M. A. Sheldon, and D. K. Gifford.",
                "Fast and Effective Query Refinement.",
                "In SIGIR, 1997. [25] J. Xu and B. Croft.",
                "Query Expansion using Local and Global Document Analysis.",
                "In SIGIR, 1996. [26] J. Xu and B. Croft.",
                "Corpus-based Stemming using Cooccurrence of Word Variants.",
                "ACM TOIS, 16 (1):61-81, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Context Sensitive Stemming 3.1 Descripción general Nuestro sistema tiene cuatro componentes como se ilustra en la Figura 1: Generación de candidatos, segmentación de consultas y \"detección de palabras de cabeza\", consulta sensible al contexto derivada y coincidencia de documentos sensibles al contexto.",
                "Componente 4: Consulta de entrada de coincidencia de documentos confidenciales del contexto: y \"Detección de palabras de cabeza\" Componente 2: Componente del segmento 1: Comparación de generación de candidatos −> Componente de comparación 3: Decisión selectiva de expansión de palabras: Comparaciones -> Ejemplo de comparación: Comparaciones de precios del hotel Salida: Comparaciones de hotelHotel -> Hoteles Figura 1: Componentes del sistema 3.2 Expansión Generación de candidatos Una de las formas de generar candidatos es usar el Porter Stemmer [18]."
            ],
            "translated_text": "",
            "candidates": [
                "detección de palabras de cabeza",
                "detección de palabras de cabeza",
                "detección de palabras de cabeza",
                "Detección de palabras de cabeza"
            ],
            "error": []
        },
        "context sensitive query stemming": {
            "translated_key": "stemming de consultas sensible al contexto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context Sensitive Stemming for Web Search Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo!",
                "Inc. 701 First Avenue Sunnyvale, California 94089 {fuchun, nawaaz, xinli, yumaol}@yahoo-inc.com ABSTRACT Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms.",
                "Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation.",
                "In this paper, we propose a context sensitive stemming method that addresses these two issues.",
                "Two unique properties make our approach feasible for Web Search.",
                "First, based on statistical language modeling, we perform context sensitive analysis on the query side.",
                "We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine.",
                "This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time.",
                "Second, our approach performs a context sensitive document matching for those expanded variants.",
                "This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision.",
                "Using word pluralization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Storage and Retrieval-Query formulation General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Web search has now become a major tool in our daily lives for information seeking.",
                "One of the important issues in Web search is that user queries are often not best formulated to get optimal results.",
                "For example, running shoe is a query that occurs frequently in query logs.",
                "However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes.",
                "Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs.",
                "This is difficult even for experienced users, and especially difficult for non-native speakers.",
                "One traditional solution is to use stemming [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term.",
                "Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing.",
                "Stemming can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing.",
                "Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows handling each query differently, and hence is preferred.",
                "Although traditional stemming increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched.",
                "When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6].",
                "In addition, it reduces system performance because the search engine has to match all the word variants.",
                "As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa.",
                "Thus, one needs to be very cautious when using stemming in Web search engines.",
                "One problem of traditional stemming is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word.",
                "For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored.",
                "For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query.",
                "Transforming book store to match book stores is fine, but matching book storing or booking store is not.",
                "A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query.",
                "However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20].",
                "A second problem of traditional stemming is its blind matching of all occurrences in documents.",
                "For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store.",
                "Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected.",
                "Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases.",
                "To alleviate these two problems, we propose a context sensitive stemming approach for Web search.",
                "Our solution consists of two context sensitive analysis, one on the query side and the other on the document side.",
                "On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms.",
                "On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query.",
                "Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines.",
                "We use pluralization handling as a running example for our stemming approach.",
                "The motivation for using pluralization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance.",
                "As far as we know, no previous research has systematically investigated the usage of pluralization in Web search.",
                "As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion.",
                "Experiments on general stemming yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper.",
                "In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2.",
                "We describe the details of the context sensitive stemming approach in Section 3.",
                "We then perform extensive experiments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5.",
                "Finally, we conclude the paper in Section 6. 2.",
                "RELATED WORK Stemming is a long studied technology.",
                "Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18].",
                "The Porter stemmer is widely used due to its simplicity and effectiveness in many applications.",
                "However, the Porter stemming makes many mistakes because its simple rules cannot fully describe English morphology.",
                "Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23].",
                "We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates.",
                "Using stemming in information retrieval is also a well known technique [8, 10].",
                "However, the effectiveness of stemming for English query systems was previously reported to be rather limited.",
                "Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance.",
                "Later, Harman [9] compares three general stemming techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper).",
                "They also proposed selective stemming based on query length and term importance, but no positive results were reported.",
                "On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%).",
                "However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search.",
                "These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2].",
                "We suspect the previous failures were mainly due to the two problems we mentioned in the introduction.",
                "Blind stemming, or a simple query length based selective stemming as used in [9] is not enough.",
                "Stemming has to be decided on case by case basis, not only at the query level but also at the document level.",
                "As we will show, if handled correctly, significant improvement can be achieved.",
                "A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25].",
                "To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21].",
                "This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search.",
                "In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift).",
                "Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query.",
                "Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search.",
                "On the other hand, stemming is much more conservative since most of the time, stemming preserves the original search intent.",
                "While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision.",
                "The increase on recall is obvious.",
                "With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded.",
                "On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the expanded query.",
                "This is similar to the relevance feedback in the sense that it requires multiple passes retrieval.",
                "If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful.",
                "It is expensive to deploy this in production Web search engines.",
                "Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine.",
                "In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search - stemming.",
                "Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision.",
                "Its simple, yet very efficient and effective, making real time stemming feasible for Web search.",
                "Our results will affirm researchers that stemming is indeed very important to large scale information retrieval. 3.",
                "CONTEXT SENSITIVE STEMMING 3.1 Overview Our system has four components as illustrated in Figure 1: candidate generation, query segmentation and head word detection, <br>context sensitive query stemming</br> and context sensitive document matching.",
                "Candidate generation (component 1) is performed oﬄine and generated candidates are stored in a dictionary.",
                "For an input query, we first segment query into concepts and detect the head word for each concept (component 2).",
                "We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4).",
                "Below we discuss each of the components in more detail.",
                "Component 4: context sensitive document matching Input Query: and head word detection Component 2: segment Component 1: candidate generation comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: hotel comparisons hotel −> hotels Figure 1: System Components 3.2 Expansion candidate generation One of the ways to generate candidates is using the Porter stemmer [18].",
                "The Porter stemmer simply uses morphological rules to convert a word to its base form.",
                "It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past.",
                "A more conservative way is based on using corpus analysis to improve the Porter stemmer results [26].",
                "The corpus analysis we do is based on word distributional similarity [15].",
                "The rationale of using distributional word similarity is that true variants tend to be used in similar contexts.",
                "In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word.",
                "We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus.",
                "The similarity between two words is the cosine similarity between the two corresponding feature vectors.",
                "The top 20 similar words to develop is shown in the following table. rank candidate score rank candidate score 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word develop.",
                "Column score is the similarity score.",
                "To determine the stemming candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list.",
                "After applying these rules, for the word develop, the stemming candidates are developing, developed, develops, development, developement, developer, developmental.",
                "For the pluralization handling purpose, only the candidate develops is retained.",
                "One thing we note from observing the distributionally similar words is that they are closely related semantically.",
                "These words might serve as candidates for general query expansion, a topic we will investigate in the future. 3.3 Segmentation and headword identification For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts.",
                "We first break a query into segments, each segment representing a concept which normally is a noun phrase.",
                "For each of the noun phrases, we then detect the most important word which we call the head word.",
                "Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity.",
                "To break a query into segments, we have to define a criteria to measure the strength of the relation between words.",
                "One effective method is to use mutual information as an indicator on whether or not to split two words [19].",
                "We use a log of 25M queries and collect the bigram and unigram frequencies from it.",
                "For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word.",
                "We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here.",
                "Table 2 shows some examples of query segmentation.",
                "The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query.",
                "Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: Query segmentation: a segment is bracketed. parsing since many queries are not grammatical and are very short.",
                "Applying a parser trained on sentences from documents to queries will have poor performance.",
                "In our solution, we just use simple heuristics rules, and it works very well in practice for English.",
                "For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of/in/at/from UVW.",
                "In such cases, the head word is typically the last nonstop word of XYZ. 3.4 Context sensitive word expansion After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful.",
                "Our statistics show that about half of the queries can be transformed by pluralization via naive stemming.",
                "Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse.",
                "Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost.",
                "In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful.",
                "Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts.",
                "Head words hotel and comparison can be expanded to hotels and comparisons.",
                "Are both transformations useful?",
                "To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web.",
                "The more likely a query to occur on the Web, the more relevant documents this query is able to return.",
                "Now the whole problem becomes how to calculate the probability of query to occur on the Web.",
                "Calculating the probability of string occurring in a corpus is a well known language modeling problem.",
                "The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur).",
                "The simplest and most successful approach to language modeling is still based on the n-gram model.",
                "By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.",
                "Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus.",
                "Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary.",
                "This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6).",
                "Also, because of the heavy tailed nature of language (i.e.",
                "Zipfs law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling.",
                "One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator.",
                "Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5].",
                "We used absolute smoothing in our experiments.",
                "Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string.",
                "Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3.",
                "We can see that all alternatives are less likely than the input query.",
                "It is therefore not useful to make an expansion for this query.",
                "On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded.",
                "To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score.",
                "Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face. 3.5 Context sensitive document matching Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants.",
                "For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison.",
                "However, not every occurrence of comparison in the document is of interest.",
                "A page which is about comparing customer service can contain all of the words hotel price comparisons comparison.",
                "This page is not a good page for the query.",
                "If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval.",
                "To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document.",
                "A variant match is considered valid only if the variant occurs in the same context as the original word does.",
                "The context is the left or the right non-stop segments 1 of the original word.",
                "Taking the same query as an example, the context of comparisons is price.",
                "The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price.",
                "Thus, we should only match those occurrences of comparison in the document if they occur after the word price.",
                "Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size.",
                "If the expanded word comparison occurs within the context of price within a window, it is considered valid.",
                "The smaller the window size is, the more restrictive the matching.",
                "We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases. 4.",
                "EXPERIMENTAL EVALUATION 4.1 Evaluation metrics We will measure both relevance improvement and the stemming cost required to achieve the relevance. 1 a context segment can not be a single stop word. 4.1.1 Relevance measurement We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11].",
                "Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) . (7) where gk is the weight for the document at rank k. Higher degree of relevance corresponds to a higher weight.",
                "A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights.",
                "We use dcg to represent the average DCG(5) over a set of test queries. 4.1.2 Stemming cost Another metric is to measure the additional cost incurred by stemming.",
                "Given the same level of relevance improvement, we prefer a stemming method that has less additional cost.",
                "We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed. 4.2 Data preparation We randomly sample 870 queries from a three month query log, with 290 from each month.",
                "Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming.",
                "We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words.",
                "In the end, we have 529 correctly spelled queries with at least 2 words. 4.3 Naive stemming for Web search Before explaining the experiments and results in detail, wed like to describe the traditional way of using stemming for Web search, referred as the naive model.",
                "This is to treat every word variant equivalent for all possible words in the query.",
                "The query book store will be transformed into (book OR books)(store OR stores) when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments. 4.4 Experimental setup The baseline model is the model without stemming.",
                "We first run the naive model to see how well it performs over the baseline.",
                "Then we improve the naive stemming model by document sensitive matching, referred as document sensitive matching model.",
                "This model makes the same stemming as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5.",
                "The naive model and document sensitive matching model stem the most queries.",
                "Out of the 529 queries, there are 408 queries that they stem, corresponding to 46.7% query traffic (out of a total of 870).",
                "We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as selective stemming model.",
                "Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model.",
                "We experiment with unigram language model and bigram language model.",
                "Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments.",
                "To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data.",
                "The oracle model only expands a word if the stemming will give better results.",
                "To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries.",
                "Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words. 4.5 Results We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5.",
                "Each row in Table 4 is a stemming strategy described in section 4.4.",
                "The first column is the name of the strategy.",
                "The second column is the number of queries affected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg.",
                "The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy).",
                "The fourth column is the relative improvement over the baseline, and the last column is the p-value of Wilcoxon significance test.",
                "There are several observations about the results.",
                "We can see the naively stemming only obtains a statistically insignificant improvement of 1.5%.",
                "Looking at Table 5, it gives an improvement of 2.7% on short queries.",
                "However, it also hurts long queries by -2.4%.",
                "Overall, the improvement is canceled out.",
                "The reason that it improves short queries is that most short queries only have one word that can be stemmed.",
                "Thus, blindly pluralizing short queries is relatively safe.",
                "However for long queries, most queries can have multiple words that can be pluralized.",
                "Expanding all of them without selection will significantly hurt precision.",
                "Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8%.",
                "The improvement comes from the conservative context sensitive document matching.",
                "An expanded word is valid only if it occurs within the context of original query in the document.",
                "This reduces many spurious matches.",
                "However, we still notice that for long queries, context sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem.",
                "While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches.",
                "It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases.",
                "Selective word pluralization further helps resolving the problem faced by document context sensitive stemming.",
                "It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stemming in the first place.",
                "By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision.",
                "With the unigram language model, we can reduce the stemming cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4%.",
                "In particular, it gives significant improvements on long queries.",
                "The dcg gain is turned from negative to positive, from −1.6% to 1.1%.",
                "This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement.",
                "For short queries too, we observe both dcg improvement and stemming cost reduction with the unigram language model.",
                "The advantages of predictive word expansion with a language model is further boosted with a better bigram language model.",
                "The overall dcg gain is lifted from 3.4% to 3.9%, and stemming cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic.",
                "For short queries, bigram language model improves the dcg gain from 4.4% to 4.7%, and reduces stemming cost from 272/272 to 150/272.",
                "For long queries, bigram language model improves dcg gain from 1.1% to 2.5%, and reduces stemming cost from 136/136 to 100/136.",
                "We observe that the bigram language model gives a larger lift for long queries.",
                "This is because the uncertainty in long queries is larger and a more powerful language model is needed.",
                "We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation.",
                "Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying.",
                "For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram language model.",
                "For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram language model.",
                "We may gain additional benefit with a more powerful language model for long queries.",
                "However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem.",
                "These problems have to be addressed separately.",
                "Looking at the the upper-bound of overhead reduction for oracle stemming, 75% (308/408) of the naive stemmings are wasteful.",
                "We currently capture about half of them.",
                "Further reduction of the overhead requires sacrificing the dcg gain.",
                "Now we can compare the stemming strategies from a different aspect.",
                "Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only.",
                "We can see that the number of affected queries decreases as the stemming strategy becomes more accurate (dcg improvement).",
                "For the bigram language model, over the 250/408 stemmed queries, the dcg improvement is 6.1%.",
                "An interesting observation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more difficult queries (low dcg queries). 5.",
                "DISCUSSIONS 5.1 Language models from query vs. from Web As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web.",
                "The language model should describe the occurrence of the string on the Web.",
                "However, the query log is also a good resource. 2 Note that this upperbound is for pluralization handling only, not for general stemming.",
                "General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics.",
                "Affected Queries dcg dcg Improvement p-value baseline 0/408 7.102 N/A N/A naive model 408/408 7.206 1.5% 0.22 document context sensitive model 408/408 7.302 2.8% 0.014 selective model: unigram LM 300/408 7.321 3.4% 0.001 selective model: bigram LM 250/408 7.381 3.9% 0.001 oracle model 100/408 7.519 5.9% 0.001 Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming Short Query Results Affected Queries dcg Improvement p-value baseline 0/272 N/A N/A naive model 272/272 2.7% 0.48 document context sensitive model 272/272 4.2% 0.002 selective model: unigram LM 185/272 4.4% 0.001 selective model: bigram LM 150/272 4.7% 0.001 oracle model 71/272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p-value baseline 0/136 N/A N/A naive model 136/136 -2.4% 0.25 document context sensitive model 136/136 -1.6% 0.27 selective model: unigram LM 115/136 1.1% 0.001 selective model: bigram LM 100/136 2.5% 0.001 oracle model 29/136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results.",
                "To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a language model from the same query top 25M queries as used to learn segmentation, and use that for prediction.",
                "We observed a slight performance decrease compared to the model trained on Web frequencies.",
                "In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries.",
                "Thus, the query log can serve as a good approximation of the Web frequencies. 5.2 How linguistics helps Some linguistic knowledge is useful in stemming.",
                "For the pluralization handling case, pluralization and de-pluralization is not symmetric.",
                "A plural word used in a query indicates a special intent.",
                "For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California.",
                "A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank.",
                "To capture this intent, we have to make sure the document is a general page about hotels in new york.",
                "We do this by requiring that the plural word hotels appears in the document.",
                "On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information.",
                "We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property. 5.3 Error analysis One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after stemming.",
                "Generally speaking, pluralization or depluralization keeps the original intent.",
                "However, the intent could change in a few cases.",
                "For one example of such a query, job at apple, we pluralize job to jobs.",
                "This stemming makes the original query ambiguous.",
                "The query job OR jobs at apple has two intents.",
                "One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company.",
                "Thus, the results after query stemming returns Steve Jobs as one of the results in top 5.",
                "One solution is performing results set based analysis to check if the intent is changed.",
                "This is similar to relevance feedback and requires second phase ranking.",
                "A second type of mistakes is the entity/concept recognition problem, These include two kinds.",
                "One is that the stemmed word variant now matches part of an entity or concept.",
                "For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco.",
                "The results will match cookie jar in san francisco.",
                "Although cookie still means the same thing as cookies, cookie jar is a different concept.",
                "Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words.",
                "For example, quote ICE is pluralized to quote OR quotes ICE.",
                "The original intent for this query is searching for stock quote for ticker ICE.",
                "However, we noticed that among the top results, one of the results is Food quotes: Ice cream.",
                "This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408/408 7.102 7.206 1.5% document context sensitive model 408/408 7.102 7.302 2.8% selective model: unigram LM 300/408 5.904 6.187 4.8% selective model: bigram LM 250/408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the affected queries before/after applying stemming the pluralized word quotes.",
                "The unchanged word ICE matches part of the noun phrase ice cream here.",
                "To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words.",
                "A third type of mistakes occurs in long queries.",
                "For the query bar code reader software, two words are pluralized. code to codes and reader to readers.",
                "In fact, bar code reader in the original query is a strong concept and the internal words should not be changed.",
                "This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking.",
                "For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept. 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented a simple yet elegant way of stemming for Web search.",
                "It improves naive stemming in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side.",
                "Using pluralization handling as an example, experiments on a major Web search engine data show it significantly improves the Web relevance and reduces the stemming cost.",
                "It also significantly improves Web click through rate (details not reported in the paper).",
                "For the future work, we are investigating the problems we identified in the error analysis section.",
                "These include: entity and noun phrase matching mistakes, and improved segmentation. 7.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving Web Search Ranking by Incorporating User Behavior Information.",
                "In SIGIR, 2006. [2] E. Airio.",
                "Word Normalization and Decompounding in Mono- and Bilingual IR.",
                "Information Retrieval, 9:249-271, 2006. [3] P. Anick.",
                "Using Terminological Feedback for Web Search Refinement: a Log-based Study.",
                "In SIGIR, 2003. [4] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press/Addison Wesley, 1999. [5] S. Chen and J. Goodman.",
                "An Empirical Study of Smoothing Techniques for Language Modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [6] S. Cronen-Townsend, Y. Zhou, and B. Croft.",
                "A Framework for Selective Query Expansion.",
                "In CIKM, 2004. [7] H. Fang and C. Zhai.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In SIGIR, 2006. [8] W. B. Frakes.",
                "Term Conflation for Information Retrieval.",
                "In C. J. Rijsbergen, editor, Research and Development in Information Retrieval, pages 383-389.",
                "Cambridge University Press, 1984. [9] D. Harman.",
                "How Effective is Suffixing?",
                "JASIS, 42(1):7-15, 1991. [10] D. Hull.",
                "Stemming Algorithms - A Case Study for Detailed Evaluation.",
                "JASIS, 47(1):70-84, 1996. [11] K. Jarvelin and J. Kekalainen.",
                "Cumulated Gain-Based Evaluation Evaluation of IR Techniques.",
                "ACM TOIS, 20:422-446, 2002. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating Query Substitutions.",
                "In WWW, 2006. [13] W. Kraaij and R. Pohlmann.",
                "Viewing Stemming as Recall Enhancement.",
                "In SIGIR, 1996. [14] R. Krovetz.",
                "Viewing Morphology as an Inference Process.",
                "In SIGIR, 1993. [15] D. Lin.",
                "Automatic Retrieval and Clustering of Similar Words.",
                "In COLING-ACL, 1998. [16] J.",
                "B. Lovins.",
                "Development of a Stemming Algorithm.",
                "Mechanical Translation and Computational Linguistics, II:22-31, 1968. [17] M. Lennon and D. Peirce and B. Tarry and P. Willett.",
                "An Evaluation of Some Conflation Algorithms for Information Retrieval.",
                "Journal of Information Science, 3:177-188, 1981. [18] M. Porter.",
                "An Algorithm for Suffix Stripping.",
                "Program, 14(3):130-137, 1980. [19] K. M. Risvik, T. Mikolajewski, and P. Boros.",
                "Query Segmentation for Web Search.",
                "In WWW, 2003. [20] S. E. Robertson.",
                "On Term Selection for Query Expansion.",
                "Journal of Documentation, 46(4):359-364, 1990. [21] G. Salton and C. Buckley.",
                "Improving Retrieval Performance by Relevance Feedback.",
                "JASIS, 41(4):288 - 297, 1999. [22] R. Sun, C.-H. Ong, and T.-S. Chua.",
                "Mining Dependency Relations for Query Expansion in Passage Retrieval.",
                "In SIGIR, 2006. [23] C. Van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, second version, 1979. [24] B. V´elez, R. Weiss, M. A. Sheldon, and D. K. Gifford.",
                "Fast and Effective Query Refinement.",
                "In SIGIR, 1997. [25] J. Xu and B. Croft.",
                "Query Expansion using Local and Global Document Analysis.",
                "In SIGIR, 1996. [26] J. Xu and B. Croft.",
                "Corpus-based Stemming using Cooccurrence of Word Variants.",
                "ACM TOIS, 16 (1):61-81, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Context Sensitive Stemming 3.1 Descripción general Nuestro sistema tiene cuatro componentes como se ilustra en la Figura 1: Generación de candidatos, segmentación de consultas y detección de palabras de cabeza, \"consulta sensible al contexto derivada\" y coincidencia de documentos sensibles al contexto."
            ],
            "translated_text": "",
            "candidates": [
                "consulta sensible al contexto derivado",
                "consulta sensible al contexto derivada"
            ],
            "error": []
        },
        "context sensitive document matching": {
            "translated_key": "coincidencias de documentos sensibles al contexto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context Sensitive Stemming for Web Search Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo!",
                "Inc. 701 First Avenue Sunnyvale, California 94089 {fuchun, nawaaz, xinli, yumaol}@yahoo-inc.com ABSTRACT Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms.",
                "Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation.",
                "In this paper, we propose a context sensitive stemming method that addresses these two issues.",
                "Two unique properties make our approach feasible for Web Search.",
                "First, based on statistical language modeling, we perform context sensitive analysis on the query side.",
                "We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine.",
                "This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time.",
                "Second, our approach performs a <br>context sensitive document matching</br> for those expanded variants.",
                "This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision.",
                "Using word pluralization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Storage and Retrieval-Query formulation General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Web search has now become a major tool in our daily lives for information seeking.",
                "One of the important issues in Web search is that user queries are often not best formulated to get optimal results.",
                "For example, running shoe is a query that occurs frequently in query logs.",
                "However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes.",
                "Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs.",
                "This is difficult even for experienced users, and especially difficult for non-native speakers.",
                "One traditional solution is to use stemming [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term.",
                "Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing.",
                "Stemming can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing.",
                "Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows handling each query differently, and hence is preferred.",
                "Although traditional stemming increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched.",
                "When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6].",
                "In addition, it reduces system performance because the search engine has to match all the word variants.",
                "As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa.",
                "Thus, one needs to be very cautious when using stemming in Web search engines.",
                "One problem of traditional stemming is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word.",
                "For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored.",
                "For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query.",
                "Transforming book store to match book stores is fine, but matching book storing or booking store is not.",
                "A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query.",
                "However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20].",
                "A second problem of traditional stemming is its blind matching of all occurrences in documents.",
                "For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store.",
                "Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected.",
                "Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases.",
                "To alleviate these two problems, we propose a context sensitive stemming approach for Web search.",
                "Our solution consists of two context sensitive analysis, one on the query side and the other on the document side.",
                "On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms.",
                "On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query.",
                "Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines.",
                "We use pluralization handling as a running example for our stemming approach.",
                "The motivation for using pluralization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance.",
                "As far as we know, no previous research has systematically investigated the usage of pluralization in Web search.",
                "As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion.",
                "Experiments on general stemming yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper.",
                "In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2.",
                "We describe the details of the context sensitive stemming approach in Section 3.",
                "We then perform extensive experiments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5.",
                "Finally, we conclude the paper in Section 6. 2.",
                "RELATED WORK Stemming is a long studied technology.",
                "Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18].",
                "The Porter stemmer is widely used due to its simplicity and effectiveness in many applications.",
                "However, the Porter stemming makes many mistakes because its simple rules cannot fully describe English morphology.",
                "Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23].",
                "We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates.",
                "Using stemming in information retrieval is also a well known technique [8, 10].",
                "However, the effectiveness of stemming for English query systems was previously reported to be rather limited.",
                "Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance.",
                "Later, Harman [9] compares three general stemming techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper).",
                "They also proposed selective stemming based on query length and term importance, but no positive results were reported.",
                "On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%).",
                "However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search.",
                "These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2].",
                "We suspect the previous failures were mainly due to the two problems we mentioned in the introduction.",
                "Blind stemming, or a simple query length based selective stemming as used in [9] is not enough.",
                "Stemming has to be decided on case by case basis, not only at the query level but also at the document level.",
                "As we will show, if handled correctly, significant improvement can be achieved.",
                "A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25].",
                "To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21].",
                "This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search.",
                "In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift).",
                "Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query.",
                "Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search.",
                "On the other hand, stemming is much more conservative since most of the time, stemming preserves the original search intent.",
                "While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision.",
                "The increase on recall is obvious.",
                "With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded.",
                "On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the expanded query.",
                "This is similar to the relevance feedback in the sense that it requires multiple passes retrieval.",
                "If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful.",
                "It is expensive to deploy this in production Web search engines.",
                "Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine.",
                "In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search - stemming.",
                "Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision.",
                "Its simple, yet very efficient and effective, making real time stemming feasible for Web search.",
                "Our results will affirm researchers that stemming is indeed very important to large scale information retrieval. 3.",
                "CONTEXT SENSITIVE STEMMING 3.1 Overview Our system has four components as illustrated in Figure 1: candidate generation, query segmentation and head word detection, context sensitive query stemming and <br>context sensitive document matching</br>.",
                "Candidate generation (component 1) is performed oﬄine and generated candidates are stored in a dictionary.",
                "For an input query, we first segment query into concepts and detect the head word for each concept (component 2).",
                "We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform <br>context sensitive document matching</br> (component 4).",
                "Below we discuss each of the components in more detail.",
                "Component 4: <br>context sensitive document matching</br> Input Query: and head word detection Component 2: segment Component 1: candidate generation comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: hotel comparisons hotel −> hotels Figure 1: System Components 3.2 Expansion candidate generation One of the ways to generate candidates is using the Porter stemmer [18].",
                "The Porter stemmer simply uses morphological rules to convert a word to its base form.",
                "It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past.",
                "A more conservative way is based on using corpus analysis to improve the Porter stemmer results [26].",
                "The corpus analysis we do is based on word distributional similarity [15].",
                "The rationale of using distributional word similarity is that true variants tend to be used in similar contexts.",
                "In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word.",
                "We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus.",
                "The similarity between two words is the cosine similarity between the two corresponding feature vectors.",
                "The top 20 similar words to develop is shown in the following table. rank candidate score rank candidate score 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word develop.",
                "Column score is the similarity score.",
                "To determine the stemming candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list.",
                "After applying these rules, for the word develop, the stemming candidates are developing, developed, develops, development, developement, developer, developmental.",
                "For the pluralization handling purpose, only the candidate develops is retained.",
                "One thing we note from observing the distributionally similar words is that they are closely related semantically.",
                "These words might serve as candidates for general query expansion, a topic we will investigate in the future. 3.3 Segmentation and headword identification For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts.",
                "We first break a query into segments, each segment representing a concept which normally is a noun phrase.",
                "For each of the noun phrases, we then detect the most important word which we call the head word.",
                "Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity.",
                "To break a query into segments, we have to define a criteria to measure the strength of the relation between words.",
                "One effective method is to use mutual information as an indicator on whether or not to split two words [19].",
                "We use a log of 25M queries and collect the bigram and unigram frequencies from it.",
                "For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word.",
                "We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here.",
                "Table 2 shows some examples of query segmentation.",
                "The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query.",
                "Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: Query segmentation: a segment is bracketed. parsing since many queries are not grammatical and are very short.",
                "Applying a parser trained on sentences from documents to queries will have poor performance.",
                "In our solution, we just use simple heuristics rules, and it works very well in practice for English.",
                "For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of/in/at/from UVW.",
                "In such cases, the head word is typically the last nonstop word of XYZ. 3.4 Context sensitive word expansion After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful.",
                "Our statistics show that about half of the queries can be transformed by pluralization via naive stemming.",
                "Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse.",
                "Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost.",
                "In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful.",
                "Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts.",
                "Head words hotel and comparison can be expanded to hotels and comparisons.",
                "Are both transformations useful?",
                "To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web.",
                "The more likely a query to occur on the Web, the more relevant documents this query is able to return.",
                "Now the whole problem becomes how to calculate the probability of query to occur on the Web.",
                "Calculating the probability of string occurring in a corpus is a well known language modeling problem.",
                "The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur).",
                "The simplest and most successful approach to language modeling is still based on the n-gram model.",
                "By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.",
                "Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus.",
                "Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary.",
                "This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6).",
                "Also, because of the heavy tailed nature of language (i.e.",
                "Zipfs law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling.",
                "One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator.",
                "Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5].",
                "We used absolute smoothing in our experiments.",
                "Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string.",
                "Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3.",
                "We can see that all alternatives are less likely than the input query.",
                "It is therefore not useful to make an expansion for this query.",
                "On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded.",
                "To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score.",
                "Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face. 3.5 <br>context sensitive document matching</br> Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants.",
                "For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison.",
                "However, not every occurrence of comparison in the document is of interest.",
                "A page which is about comparing customer service can contain all of the words hotel price comparisons comparison.",
                "This page is not a good page for the query.",
                "If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval.",
                "To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document.",
                "A variant match is considered valid only if the variant occurs in the same context as the original word does.",
                "The context is the left or the right non-stop segments 1 of the original word.",
                "Taking the same query as an example, the context of comparisons is price.",
                "The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price.",
                "Thus, we should only match those occurrences of comparison in the document if they occur after the word price.",
                "Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size.",
                "If the expanded word comparison occurs within the context of price within a window, it is considered valid.",
                "The smaller the window size is, the more restrictive the matching.",
                "We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases. 4.",
                "EXPERIMENTAL EVALUATION 4.1 Evaluation metrics We will measure both relevance improvement and the stemming cost required to achieve the relevance. 1 a context segment can not be a single stop word. 4.1.1 Relevance measurement We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11].",
                "Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) . (7) where gk is the weight for the document at rank k. Higher degree of relevance corresponds to a higher weight.",
                "A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights.",
                "We use dcg to represent the average DCG(5) over a set of test queries. 4.1.2 Stemming cost Another metric is to measure the additional cost incurred by stemming.",
                "Given the same level of relevance improvement, we prefer a stemming method that has less additional cost.",
                "We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed. 4.2 Data preparation We randomly sample 870 queries from a three month query log, with 290 from each month.",
                "Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming.",
                "We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words.",
                "In the end, we have 529 correctly spelled queries with at least 2 words. 4.3 Naive stemming for Web search Before explaining the experiments and results in detail, wed like to describe the traditional way of using stemming for Web search, referred as the naive model.",
                "This is to treat every word variant equivalent for all possible words in the query.",
                "The query book store will be transformed into (book OR books)(store OR stores) when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments. 4.4 Experimental setup The baseline model is the model without stemming.",
                "We first run the naive model to see how well it performs over the baseline.",
                "Then we improve the naive stemming model by document sensitive matching, referred as document sensitive matching model.",
                "This model makes the same stemming as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5.",
                "The naive model and document sensitive matching model stem the most queries.",
                "Out of the 529 queries, there are 408 queries that they stem, corresponding to 46.7% query traffic (out of a total of 870).",
                "We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as selective stemming model.",
                "Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model.",
                "We experiment with unigram language model and bigram language model.",
                "Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments.",
                "To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data.",
                "The oracle model only expands a word if the stemming will give better results.",
                "To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries.",
                "Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words. 4.5 Results We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5.",
                "Each row in Table 4 is a stemming strategy described in section 4.4.",
                "The first column is the name of the strategy.",
                "The second column is the number of queries affected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg.",
                "The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy).",
                "The fourth column is the relative improvement over the baseline, and the last column is the p-value of Wilcoxon significance test.",
                "There are several observations about the results.",
                "We can see the naively stemming only obtains a statistically insignificant improvement of 1.5%.",
                "Looking at Table 5, it gives an improvement of 2.7% on short queries.",
                "However, it also hurts long queries by -2.4%.",
                "Overall, the improvement is canceled out.",
                "The reason that it improves short queries is that most short queries only have one word that can be stemmed.",
                "Thus, blindly pluralizing short queries is relatively safe.",
                "However for long queries, most queries can have multiple words that can be pluralized.",
                "Expanding all of them without selection will significantly hurt precision.",
                "Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8%.",
                "The improvement comes from the conservative <br>context sensitive document matching</br>.",
                "An expanded word is valid only if it occurs within the context of original query in the document.",
                "This reduces many spurious matches.",
                "However, we still notice that for long queries, context sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem.",
                "While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches.",
                "It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases.",
                "Selective word pluralization further helps resolving the problem faced by document context sensitive stemming.",
                "It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stemming in the first place.",
                "By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision.",
                "With the unigram language model, we can reduce the stemming cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4%.",
                "In particular, it gives significant improvements on long queries.",
                "The dcg gain is turned from negative to positive, from −1.6% to 1.1%.",
                "This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement.",
                "For short queries too, we observe both dcg improvement and stemming cost reduction with the unigram language model.",
                "The advantages of predictive word expansion with a language model is further boosted with a better bigram language model.",
                "The overall dcg gain is lifted from 3.4% to 3.9%, and stemming cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic.",
                "For short queries, bigram language model improves the dcg gain from 4.4% to 4.7%, and reduces stemming cost from 272/272 to 150/272.",
                "For long queries, bigram language model improves dcg gain from 1.1% to 2.5%, and reduces stemming cost from 136/136 to 100/136.",
                "We observe that the bigram language model gives a larger lift for long queries.",
                "This is because the uncertainty in long queries is larger and a more powerful language model is needed.",
                "We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation.",
                "Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying.",
                "For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram language model.",
                "For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram language model.",
                "We may gain additional benefit with a more powerful language model for long queries.",
                "However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem.",
                "These problems have to be addressed separately.",
                "Looking at the the upper-bound of overhead reduction for oracle stemming, 75% (308/408) of the naive stemmings are wasteful.",
                "We currently capture about half of them.",
                "Further reduction of the overhead requires sacrificing the dcg gain.",
                "Now we can compare the stemming strategies from a different aspect.",
                "Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only.",
                "We can see that the number of affected queries decreases as the stemming strategy becomes more accurate (dcg improvement).",
                "For the bigram language model, over the 250/408 stemmed queries, the dcg improvement is 6.1%.",
                "An interesting observation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more difficult queries (low dcg queries). 5.",
                "DISCUSSIONS 5.1 Language models from query vs. from Web As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web.",
                "The language model should describe the occurrence of the string on the Web.",
                "However, the query log is also a good resource. 2 Note that this upperbound is for pluralization handling only, not for general stemming.",
                "General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics.",
                "Affected Queries dcg dcg Improvement p-value baseline 0/408 7.102 N/A N/A naive model 408/408 7.206 1.5% 0.22 document context sensitive model 408/408 7.302 2.8% 0.014 selective model: unigram LM 300/408 7.321 3.4% 0.001 selective model: bigram LM 250/408 7.381 3.9% 0.001 oracle model 100/408 7.519 5.9% 0.001 Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming Short Query Results Affected Queries dcg Improvement p-value baseline 0/272 N/A N/A naive model 272/272 2.7% 0.48 document context sensitive model 272/272 4.2% 0.002 selective model: unigram LM 185/272 4.4% 0.001 selective model: bigram LM 150/272 4.7% 0.001 oracle model 71/272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p-value baseline 0/136 N/A N/A naive model 136/136 -2.4% 0.25 document context sensitive model 136/136 -1.6% 0.27 selective model: unigram LM 115/136 1.1% 0.001 selective model: bigram LM 100/136 2.5% 0.001 oracle model 29/136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results.",
                "To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a language model from the same query top 25M queries as used to learn segmentation, and use that for prediction.",
                "We observed a slight performance decrease compared to the model trained on Web frequencies.",
                "In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries.",
                "Thus, the query log can serve as a good approximation of the Web frequencies. 5.2 How linguistics helps Some linguistic knowledge is useful in stemming.",
                "For the pluralization handling case, pluralization and de-pluralization is not symmetric.",
                "A plural word used in a query indicates a special intent.",
                "For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California.",
                "A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank.",
                "To capture this intent, we have to make sure the document is a general page about hotels in new york.",
                "We do this by requiring that the plural word hotels appears in the document.",
                "On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information.",
                "We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property. 5.3 Error analysis One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after stemming.",
                "Generally speaking, pluralization or depluralization keeps the original intent.",
                "However, the intent could change in a few cases.",
                "For one example of such a query, job at apple, we pluralize job to jobs.",
                "This stemming makes the original query ambiguous.",
                "The query job OR jobs at apple has two intents.",
                "One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company.",
                "Thus, the results after query stemming returns Steve Jobs as one of the results in top 5.",
                "One solution is performing results set based analysis to check if the intent is changed.",
                "This is similar to relevance feedback and requires second phase ranking.",
                "A second type of mistakes is the entity/concept recognition problem, These include two kinds.",
                "One is that the stemmed word variant now matches part of an entity or concept.",
                "For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco.",
                "The results will match cookie jar in san francisco.",
                "Although cookie still means the same thing as cookies, cookie jar is a different concept.",
                "Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words.",
                "For example, quote ICE is pluralized to quote OR quotes ICE.",
                "The original intent for this query is searching for stock quote for ticker ICE.",
                "However, we noticed that among the top results, one of the results is Food quotes: Ice cream.",
                "This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408/408 7.102 7.206 1.5% document context sensitive model 408/408 7.102 7.302 2.8% selective model: unigram LM 300/408 5.904 6.187 4.8% selective model: bigram LM 250/408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the affected queries before/after applying stemming the pluralized word quotes.",
                "The unchanged word ICE matches part of the noun phrase ice cream here.",
                "To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words.",
                "A third type of mistakes occurs in long queries.",
                "For the query bar code reader software, two words are pluralized. code to codes and reader to readers.",
                "In fact, bar code reader in the original query is a strong concept and the internal words should not be changed.",
                "This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking.",
                "For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept. 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented a simple yet elegant way of stemming for Web search.",
                "It improves naive stemming in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side.",
                "Using pluralization handling as an example, experiments on a major Web search engine data show it significantly improves the Web relevance and reduces the stemming cost.",
                "It also significantly improves Web click through rate (details not reported in the paper).",
                "For the future work, we are investigating the problems we identified in the error analysis section.",
                "These include: entity and noun phrase matching mistakes, and improved segmentation. 7.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving Web Search Ranking by Incorporating User Behavior Information.",
                "In SIGIR, 2006. [2] E. Airio.",
                "Word Normalization and Decompounding in Mono- and Bilingual IR.",
                "Information Retrieval, 9:249-271, 2006. [3] P. Anick.",
                "Using Terminological Feedback for Web Search Refinement: a Log-based Study.",
                "In SIGIR, 2003. [4] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press/Addison Wesley, 1999. [5] S. Chen and J. Goodman.",
                "An Empirical Study of Smoothing Techniques for Language Modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [6] S. Cronen-Townsend, Y. Zhou, and B. Croft.",
                "A Framework for Selective Query Expansion.",
                "In CIKM, 2004. [7] H. Fang and C. Zhai.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In SIGIR, 2006. [8] W. B. Frakes.",
                "Term Conflation for Information Retrieval.",
                "In C. J. Rijsbergen, editor, Research and Development in Information Retrieval, pages 383-389.",
                "Cambridge University Press, 1984. [9] D. Harman.",
                "How Effective is Suffixing?",
                "JASIS, 42(1):7-15, 1991. [10] D. Hull.",
                "Stemming Algorithms - A Case Study for Detailed Evaluation.",
                "JASIS, 47(1):70-84, 1996. [11] K. Jarvelin and J. Kekalainen.",
                "Cumulated Gain-Based Evaluation Evaluation of IR Techniques.",
                "ACM TOIS, 20:422-446, 2002. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating Query Substitutions.",
                "In WWW, 2006. [13] W. Kraaij and R. Pohlmann.",
                "Viewing Stemming as Recall Enhancement.",
                "In SIGIR, 1996. [14] R. Krovetz.",
                "Viewing Morphology as an Inference Process.",
                "In SIGIR, 1993. [15] D. Lin.",
                "Automatic Retrieval and Clustering of Similar Words.",
                "In COLING-ACL, 1998. [16] J.",
                "B. Lovins.",
                "Development of a Stemming Algorithm.",
                "Mechanical Translation and Computational Linguistics, II:22-31, 1968. [17] M. Lennon and D. Peirce and B. Tarry and P. Willett.",
                "An Evaluation of Some Conflation Algorithms for Information Retrieval.",
                "Journal of Information Science, 3:177-188, 1981. [18] M. Porter.",
                "An Algorithm for Suffix Stripping.",
                "Program, 14(3):130-137, 1980. [19] K. M. Risvik, T. Mikolajewski, and P. Boros.",
                "Query Segmentation for Web Search.",
                "In WWW, 2003. [20] S. E. Robertson.",
                "On Term Selection for Query Expansion.",
                "Journal of Documentation, 46(4):359-364, 1990. [21] G. Salton and C. Buckley.",
                "Improving Retrieval Performance by Relevance Feedback.",
                "JASIS, 41(4):288 - 297, 1999. [22] R. Sun, C.-H. Ong, and T.-S. Chua.",
                "Mining Dependency Relations for Query Expansion in Passage Retrieval.",
                "In SIGIR, 2006. [23] C. Van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, second version, 1979. [24] B. V´elez, R. Weiss, M. A. Sheldon, and D. K. Gifford.",
                "Fast and Effective Query Refinement.",
                "In SIGIR, 1997. [25] J. Xu and B. Croft.",
                "Query Expansion using Local and Global Document Analysis.",
                "In SIGIR, 1996. [26] J. Xu and B. Croft.",
                "Corpus-based Stemming using Cooccurrence of Word Variants.",
                "ACM TOIS, 16 (1):61-81, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En segundo lugar, nuestro enfoque realiza una \"coincidencia de documentos sensibles al contexto\" para esas variantes expandidas.",
                "Context Sensitive Stemming 3.1 Descripción general Nuestro sistema tiene cuatro componentes como se ilustra en la Figura 1: Generación de candidatos, segmentación de consultas y detección de palabras de cabeza, consulta sensible al contexto derivada y \"coincidencia de documentos sensibles al contexto\".",
                "Luego usamos el modelado de lenguaje estadístico para decidir si una variante particular es útil (componente 3) y, finalmente, para las variantes expandidas, realizamos una \"coincidencia de documentos sensibles al contexto\" (componente 4).",
                "Componente 4: Consulta de entrada \"Contexto Sensitive Documing\": y Componente de detección de palabras de la cabeza 2: Componente del segmento 1: Comparación de generación de candidatos −> Comparación de comparación 3: Decisión selectiva de expansión de palabras: comparaciones -> Ejemplo de comparación: Comparaciones de precios del hotel Salida: Comparaciones de hotelHotel -> Hoteles Figura 1: Componentes del sistema 3.2 Expansión Generación de candidatos Una de las formas de generar candidatos es usar el Porter Stemmer [18].",
                "Variaciones de consulta Entropía Comparación de precios del hotel 6.177 Comparaciones de precios del hotel 6.597 Hoteles Comparación de precios 6.937 Comparaciones de precios de hoteles 7.360 Tabla 3: Variaciones de la consulta Comparación de precios del hotel clasificado por puntaje de entropía, con la consulta original en negrita.3.5 \"Matriota de documento sensible al contexto\", incluso después de saber qué variantes de palabras, es probable que sean útiles, debemos ser conservadores en la coincidencia de documentos para las variantes expandidas.",
                "La mejora proviene de la \"coincidencia de documentos sensible al contexto\" conservador."
            ],
            "translated_text": "",
            "candidates": [
                "coincidencia de documentos sensibles al contexto",
                "coincidencia de documentos sensibles al contexto",
                "coincidencia de documentos sensibles al contexto",
                "coincidencia de documentos sensibles al contexto",
                "coincidencia de documentos sensibles al contexto",
                "coincidencia de documentos sensibles al contexto",
                "coincidencia de documentos sensibles al contexto",
                "Contexto Sensitive Documing",
                "coincidencia de documentos sensibles al contexto",
                "Matriota de documento sensible al contexto",
                "coincidencia de documentos sensibles al contexto",
                "coincidencia de documentos sensible al contexto"
            ],
            "error": []
        },
        "unigram language model": {
            "translated_key": "modelo de lenguaje unigrama",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context Sensitive Stemming for Web Search Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo!",
                "Inc. 701 First Avenue Sunnyvale, California 94089 {fuchun, nawaaz, xinli, yumaol}@yahoo-inc.com ABSTRACT Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms.",
                "Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation.",
                "In this paper, we propose a context sensitive stemming method that addresses these two issues.",
                "Two unique properties make our approach feasible for Web Search.",
                "First, based on statistical language modeling, we perform context sensitive analysis on the query side.",
                "We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine.",
                "This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time.",
                "Second, our approach performs a context sensitive document matching for those expanded variants.",
                "This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision.",
                "Using word pluralization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Storage and Retrieval-Query formulation General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Web search has now become a major tool in our daily lives for information seeking.",
                "One of the important issues in Web search is that user queries are often not best formulated to get optimal results.",
                "For example, running shoe is a query that occurs frequently in query logs.",
                "However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes.",
                "Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs.",
                "This is difficult even for experienced users, and especially difficult for non-native speakers.",
                "One traditional solution is to use stemming [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term.",
                "Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing.",
                "Stemming can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing.",
                "Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows handling each query differently, and hence is preferred.",
                "Although traditional stemming increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched.",
                "When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6].",
                "In addition, it reduces system performance because the search engine has to match all the word variants.",
                "As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa.",
                "Thus, one needs to be very cautious when using stemming in Web search engines.",
                "One problem of traditional stemming is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word.",
                "For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored.",
                "For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query.",
                "Transforming book store to match book stores is fine, but matching book storing or booking store is not.",
                "A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query.",
                "However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20].",
                "A second problem of traditional stemming is its blind matching of all occurrences in documents.",
                "For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store.",
                "Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected.",
                "Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases.",
                "To alleviate these two problems, we propose a context sensitive stemming approach for Web search.",
                "Our solution consists of two context sensitive analysis, one on the query side and the other on the document side.",
                "On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms.",
                "On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query.",
                "Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines.",
                "We use pluralization handling as a running example for our stemming approach.",
                "The motivation for using pluralization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance.",
                "As far as we know, no previous research has systematically investigated the usage of pluralization in Web search.",
                "As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion.",
                "Experiments on general stemming yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper.",
                "In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2.",
                "We describe the details of the context sensitive stemming approach in Section 3.",
                "We then perform extensive experiments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5.",
                "Finally, we conclude the paper in Section 6. 2.",
                "RELATED WORK Stemming is a long studied technology.",
                "Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18].",
                "The Porter stemmer is widely used due to its simplicity and effectiveness in many applications.",
                "However, the Porter stemming makes many mistakes because its simple rules cannot fully describe English morphology.",
                "Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23].",
                "We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates.",
                "Using stemming in information retrieval is also a well known technique [8, 10].",
                "However, the effectiveness of stemming for English query systems was previously reported to be rather limited.",
                "Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance.",
                "Later, Harman [9] compares three general stemming techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper).",
                "They also proposed selective stemming based on query length and term importance, but no positive results were reported.",
                "On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%).",
                "However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search.",
                "These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2].",
                "We suspect the previous failures were mainly due to the two problems we mentioned in the introduction.",
                "Blind stemming, or a simple query length based selective stemming as used in [9] is not enough.",
                "Stemming has to be decided on case by case basis, not only at the query level but also at the document level.",
                "As we will show, if handled correctly, significant improvement can be achieved.",
                "A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25].",
                "To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21].",
                "This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search.",
                "In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift).",
                "Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query.",
                "Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search.",
                "On the other hand, stemming is much more conservative since most of the time, stemming preserves the original search intent.",
                "While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision.",
                "The increase on recall is obvious.",
                "With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded.",
                "On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the expanded query.",
                "This is similar to the relevance feedback in the sense that it requires multiple passes retrieval.",
                "If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful.",
                "It is expensive to deploy this in production Web search engines.",
                "Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine.",
                "In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search - stemming.",
                "Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision.",
                "Its simple, yet very efficient and effective, making real time stemming feasible for Web search.",
                "Our results will affirm researchers that stemming is indeed very important to large scale information retrieval. 3.",
                "CONTEXT SENSITIVE STEMMING 3.1 Overview Our system has four components as illustrated in Figure 1: candidate generation, query segmentation and head word detection, context sensitive query stemming and context sensitive document matching.",
                "Candidate generation (component 1) is performed oﬄine and generated candidates are stored in a dictionary.",
                "For an input query, we first segment query into concepts and detect the head word for each concept (component 2).",
                "We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4).",
                "Below we discuss each of the components in more detail.",
                "Component 4: context sensitive document matching Input Query: and head word detection Component 2: segment Component 1: candidate generation comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: hotel comparisons hotel −> hotels Figure 1: System Components 3.2 Expansion candidate generation One of the ways to generate candidates is using the Porter stemmer [18].",
                "The Porter stemmer simply uses morphological rules to convert a word to its base form.",
                "It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past.",
                "A more conservative way is based on using corpus analysis to improve the Porter stemmer results [26].",
                "The corpus analysis we do is based on word distributional similarity [15].",
                "The rationale of using distributional word similarity is that true variants tend to be used in similar contexts.",
                "In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word.",
                "We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus.",
                "The similarity between two words is the cosine similarity between the two corresponding feature vectors.",
                "The top 20 similar words to develop is shown in the following table. rank candidate score rank candidate score 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word develop.",
                "Column score is the similarity score.",
                "To determine the stemming candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list.",
                "After applying these rules, for the word develop, the stemming candidates are developing, developed, develops, development, developement, developer, developmental.",
                "For the pluralization handling purpose, only the candidate develops is retained.",
                "One thing we note from observing the distributionally similar words is that they are closely related semantically.",
                "These words might serve as candidates for general query expansion, a topic we will investigate in the future. 3.3 Segmentation and headword identification For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts.",
                "We first break a query into segments, each segment representing a concept which normally is a noun phrase.",
                "For each of the noun phrases, we then detect the most important word which we call the head word.",
                "Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity.",
                "To break a query into segments, we have to define a criteria to measure the strength of the relation between words.",
                "One effective method is to use mutual information as an indicator on whether or not to split two words [19].",
                "We use a log of 25M queries and collect the bigram and unigram frequencies from it.",
                "For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word.",
                "We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here.",
                "Table 2 shows some examples of query segmentation.",
                "The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query.",
                "Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: Query segmentation: a segment is bracketed. parsing since many queries are not grammatical and are very short.",
                "Applying a parser trained on sentences from documents to queries will have poor performance.",
                "In our solution, we just use simple heuristics rules, and it works very well in practice for English.",
                "For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of/in/at/from UVW.",
                "In such cases, the head word is typically the last nonstop word of XYZ. 3.4 Context sensitive word expansion After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful.",
                "Our statistics show that about half of the queries can be transformed by pluralization via naive stemming.",
                "Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse.",
                "Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost.",
                "In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful.",
                "Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts.",
                "Head words hotel and comparison can be expanded to hotels and comparisons.",
                "Are both transformations useful?",
                "To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web.",
                "The more likely a query to occur on the Web, the more relevant documents this query is able to return.",
                "Now the whole problem becomes how to calculate the probability of query to occur on the Web.",
                "Calculating the probability of string occurring in a corpus is a well known language modeling problem.",
                "The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur).",
                "The simplest and most successful approach to language modeling is still based on the n-gram model.",
                "By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.",
                "Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus.",
                "Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary.",
                "This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6).",
                "Also, because of the heavy tailed nature of language (i.e.",
                "Zipfs law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling.",
                "One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator.",
                "Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5].",
                "We used absolute smoothing in our experiments.",
                "Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string.",
                "Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3.",
                "We can see that all alternatives are less likely than the input query.",
                "It is therefore not useful to make an expansion for this query.",
                "On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded.",
                "To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score.",
                "Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face. 3.5 Context sensitive document matching Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants.",
                "For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison.",
                "However, not every occurrence of comparison in the document is of interest.",
                "A page which is about comparing customer service can contain all of the words hotel price comparisons comparison.",
                "This page is not a good page for the query.",
                "If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval.",
                "To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document.",
                "A variant match is considered valid only if the variant occurs in the same context as the original word does.",
                "The context is the left or the right non-stop segments 1 of the original word.",
                "Taking the same query as an example, the context of comparisons is price.",
                "The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price.",
                "Thus, we should only match those occurrences of comparison in the document if they occur after the word price.",
                "Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size.",
                "If the expanded word comparison occurs within the context of price within a window, it is considered valid.",
                "The smaller the window size is, the more restrictive the matching.",
                "We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases. 4.",
                "EXPERIMENTAL EVALUATION 4.1 Evaluation metrics We will measure both relevance improvement and the stemming cost required to achieve the relevance. 1 a context segment can not be a single stop word. 4.1.1 Relevance measurement We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11].",
                "Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) . (7) where gk is the weight for the document at rank k. Higher degree of relevance corresponds to a higher weight.",
                "A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights.",
                "We use dcg to represent the average DCG(5) over a set of test queries. 4.1.2 Stemming cost Another metric is to measure the additional cost incurred by stemming.",
                "Given the same level of relevance improvement, we prefer a stemming method that has less additional cost.",
                "We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed. 4.2 Data preparation We randomly sample 870 queries from a three month query log, with 290 from each month.",
                "Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming.",
                "We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words.",
                "In the end, we have 529 correctly spelled queries with at least 2 words. 4.3 Naive stemming for Web search Before explaining the experiments and results in detail, wed like to describe the traditional way of using stemming for Web search, referred as the naive model.",
                "This is to treat every word variant equivalent for all possible words in the query.",
                "The query book store will be transformed into (book OR books)(store OR stores) when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments. 4.4 Experimental setup The baseline model is the model without stemming.",
                "We first run the naive model to see how well it performs over the baseline.",
                "Then we improve the naive stemming model by document sensitive matching, referred as document sensitive matching model.",
                "This model makes the same stemming as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5.",
                "The naive model and document sensitive matching model stem the most queries.",
                "Out of the 529 queries, there are 408 queries that they stem, corresponding to 46.7% query traffic (out of a total of 870).",
                "We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as selective stemming model.",
                "Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model.",
                "We experiment with <br>unigram language model</br> and bigram language model.",
                "Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments.",
                "To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data.",
                "The oracle model only expands a word if the stemming will give better results.",
                "To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries.",
                "Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words. 4.5 Results We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5.",
                "Each row in Table 4 is a stemming strategy described in section 4.4.",
                "The first column is the name of the strategy.",
                "The second column is the number of queries affected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg.",
                "The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy).",
                "The fourth column is the relative improvement over the baseline, and the last column is the p-value of Wilcoxon significance test.",
                "There are several observations about the results.",
                "We can see the naively stemming only obtains a statistically insignificant improvement of 1.5%.",
                "Looking at Table 5, it gives an improvement of 2.7% on short queries.",
                "However, it also hurts long queries by -2.4%.",
                "Overall, the improvement is canceled out.",
                "The reason that it improves short queries is that most short queries only have one word that can be stemmed.",
                "Thus, blindly pluralizing short queries is relatively safe.",
                "However for long queries, most queries can have multiple words that can be pluralized.",
                "Expanding all of them without selection will significantly hurt precision.",
                "Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8%.",
                "The improvement comes from the conservative context sensitive document matching.",
                "An expanded word is valid only if it occurs within the context of original query in the document.",
                "This reduces many spurious matches.",
                "However, we still notice that for long queries, context sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem.",
                "While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches.",
                "It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases.",
                "Selective word pluralization further helps resolving the problem faced by document context sensitive stemming.",
                "It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stemming in the first place.",
                "By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision.",
                "With the <br>unigram language model</br>, we can reduce the stemming cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4%.",
                "In particular, it gives significant improvements on long queries.",
                "The dcg gain is turned from negative to positive, from −1.6% to 1.1%.",
                "This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement.",
                "For short queries too, we observe both dcg improvement and stemming cost reduction with the <br>unigram language model</br>.",
                "The advantages of predictive word expansion with a language model is further boosted with a better bigram language model.",
                "The overall dcg gain is lifted from 3.4% to 3.9%, and stemming cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic.",
                "For short queries, bigram language model improves the dcg gain from 4.4% to 4.7%, and reduces stemming cost from 272/272 to 150/272.",
                "For long queries, bigram language model improves dcg gain from 1.1% to 2.5%, and reduces stemming cost from 136/136 to 100/136.",
                "We observe that the bigram language model gives a larger lift for long queries.",
                "This is because the uncertainty in long queries is larger and a more powerful language model is needed.",
                "We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation.",
                "Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying.",
                "For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram language model.",
                "For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram language model.",
                "We may gain additional benefit with a more powerful language model for long queries.",
                "However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem.",
                "These problems have to be addressed separately.",
                "Looking at the the upper-bound of overhead reduction for oracle stemming, 75% (308/408) of the naive stemmings are wasteful.",
                "We currently capture about half of them.",
                "Further reduction of the overhead requires sacrificing the dcg gain.",
                "Now we can compare the stemming strategies from a different aspect.",
                "Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only.",
                "We can see that the number of affected queries decreases as the stemming strategy becomes more accurate (dcg improvement).",
                "For the bigram language model, over the 250/408 stemmed queries, the dcg improvement is 6.1%.",
                "An interesting observation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more difficult queries (low dcg queries). 5.",
                "DISCUSSIONS 5.1 Language models from query vs. from Web As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web.",
                "The language model should describe the occurrence of the string on the Web.",
                "However, the query log is also a good resource. 2 Note that this upperbound is for pluralization handling only, not for general stemming.",
                "General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics.",
                "Affected Queries dcg dcg Improvement p-value baseline 0/408 7.102 N/A N/A naive model 408/408 7.206 1.5% 0.22 document context sensitive model 408/408 7.302 2.8% 0.014 selective model: unigram LM 300/408 7.321 3.4% 0.001 selective model: bigram LM 250/408 7.381 3.9% 0.001 oracle model 100/408 7.519 5.9% 0.001 Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming Short Query Results Affected Queries dcg Improvement p-value baseline 0/272 N/A N/A naive model 272/272 2.7% 0.48 document context sensitive model 272/272 4.2% 0.002 selective model: unigram LM 185/272 4.4% 0.001 selective model: bigram LM 150/272 4.7% 0.001 oracle model 71/272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p-value baseline 0/136 N/A N/A naive model 136/136 -2.4% 0.25 document context sensitive model 136/136 -1.6% 0.27 selective model: unigram LM 115/136 1.1% 0.001 selective model: bigram LM 100/136 2.5% 0.001 oracle model 29/136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results.",
                "To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a language model from the same query top 25M queries as used to learn segmentation, and use that for prediction.",
                "We observed a slight performance decrease compared to the model trained on Web frequencies.",
                "In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries.",
                "Thus, the query log can serve as a good approximation of the Web frequencies. 5.2 How linguistics helps Some linguistic knowledge is useful in stemming.",
                "For the pluralization handling case, pluralization and de-pluralization is not symmetric.",
                "A plural word used in a query indicates a special intent.",
                "For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California.",
                "A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank.",
                "To capture this intent, we have to make sure the document is a general page about hotels in new york.",
                "We do this by requiring that the plural word hotels appears in the document.",
                "On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information.",
                "We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property. 5.3 Error analysis One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after stemming.",
                "Generally speaking, pluralization or depluralization keeps the original intent.",
                "However, the intent could change in a few cases.",
                "For one example of such a query, job at apple, we pluralize job to jobs.",
                "This stemming makes the original query ambiguous.",
                "The query job OR jobs at apple has two intents.",
                "One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company.",
                "Thus, the results after query stemming returns Steve Jobs as one of the results in top 5.",
                "One solution is performing results set based analysis to check if the intent is changed.",
                "This is similar to relevance feedback and requires second phase ranking.",
                "A second type of mistakes is the entity/concept recognition problem, These include two kinds.",
                "One is that the stemmed word variant now matches part of an entity or concept.",
                "For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco.",
                "The results will match cookie jar in san francisco.",
                "Although cookie still means the same thing as cookies, cookie jar is a different concept.",
                "Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words.",
                "For example, quote ICE is pluralized to quote OR quotes ICE.",
                "The original intent for this query is searching for stock quote for ticker ICE.",
                "However, we noticed that among the top results, one of the results is Food quotes: Ice cream.",
                "This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408/408 7.102 7.206 1.5% document context sensitive model 408/408 7.102 7.302 2.8% selective model: unigram LM 300/408 5.904 6.187 4.8% selective model: bigram LM 250/408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the affected queries before/after applying stemming the pluralized word quotes.",
                "The unchanged word ICE matches part of the noun phrase ice cream here.",
                "To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words.",
                "A third type of mistakes occurs in long queries.",
                "For the query bar code reader software, two words are pluralized. code to codes and reader to readers.",
                "In fact, bar code reader in the original query is a strong concept and the internal words should not be changed.",
                "This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking.",
                "For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept. 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented a simple yet elegant way of stemming for Web search.",
                "It improves naive stemming in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side.",
                "Using pluralization handling as an example, experiments on a major Web search engine data show it significantly improves the Web relevance and reduces the stemming cost.",
                "It also significantly improves Web click through rate (details not reported in the paper).",
                "For the future work, we are investigating the problems we identified in the error analysis section.",
                "These include: entity and noun phrase matching mistakes, and improved segmentation. 7.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving Web Search Ranking by Incorporating User Behavior Information.",
                "In SIGIR, 2006. [2] E. Airio.",
                "Word Normalization and Decompounding in Mono- and Bilingual IR.",
                "Information Retrieval, 9:249-271, 2006. [3] P. Anick.",
                "Using Terminological Feedback for Web Search Refinement: a Log-based Study.",
                "In SIGIR, 2003. [4] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press/Addison Wesley, 1999. [5] S. Chen and J. Goodman.",
                "An Empirical Study of Smoothing Techniques for Language Modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [6] S. Cronen-Townsend, Y. Zhou, and B. Croft.",
                "A Framework for Selective Query Expansion.",
                "In CIKM, 2004. [7] H. Fang and C. Zhai.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In SIGIR, 2006. [8] W. B. Frakes.",
                "Term Conflation for Information Retrieval.",
                "In C. J. Rijsbergen, editor, Research and Development in Information Retrieval, pages 383-389.",
                "Cambridge University Press, 1984. [9] D. Harman.",
                "How Effective is Suffixing?",
                "JASIS, 42(1):7-15, 1991. [10] D. Hull.",
                "Stemming Algorithms - A Case Study for Detailed Evaluation.",
                "JASIS, 47(1):70-84, 1996. [11] K. Jarvelin and J. Kekalainen.",
                "Cumulated Gain-Based Evaluation Evaluation of IR Techniques.",
                "ACM TOIS, 20:422-446, 2002. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating Query Substitutions.",
                "In WWW, 2006. [13] W. Kraaij and R. Pohlmann.",
                "Viewing Stemming as Recall Enhancement.",
                "In SIGIR, 1996. [14] R. Krovetz.",
                "Viewing Morphology as an Inference Process.",
                "In SIGIR, 1993. [15] D. Lin.",
                "Automatic Retrieval and Clustering of Similar Words.",
                "In COLING-ACL, 1998. [16] J.",
                "B. Lovins.",
                "Development of a Stemming Algorithm.",
                "Mechanical Translation and Computational Linguistics, II:22-31, 1968. [17] M. Lennon and D. Peirce and B. Tarry and P. Willett.",
                "An Evaluation of Some Conflation Algorithms for Information Retrieval.",
                "Journal of Information Science, 3:177-188, 1981. [18] M. Porter.",
                "An Algorithm for Suffix Stripping.",
                "Program, 14(3):130-137, 1980. [19] K. M. Risvik, T. Mikolajewski, and P. Boros.",
                "Query Segmentation for Web Search.",
                "In WWW, 2003. [20] S. E. Robertson.",
                "On Term Selection for Query Expansion.",
                "Journal of Documentation, 46(4):359-364, 1990. [21] G. Salton and C. Buckley.",
                "Improving Retrieval Performance by Relevance Feedback.",
                "JASIS, 41(4):288 - 297, 1999. [22] R. Sun, C.-H. Ong, and T.-S. Chua.",
                "Mining Dependency Relations for Query Expansion in Passage Retrieval.",
                "In SIGIR, 2006. [23] C. Van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, second version, 1979. [24] B. V´elez, R. Weiss, M. A. Sheldon, and D. K. Gifford.",
                "Fast and Effective Query Refinement.",
                "In SIGIR, 1997. [25] J. Xu and B. Croft.",
                "Query Expansion using Local and Global Document Analysis.",
                "In SIGIR, 1996. [26] J. Xu and B. Croft.",
                "Corpus-based Stemming using Cooccurrence of Word Variants.",
                "ACM TOIS, 16 (1):61-81, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Experimentamos con \"modelo de lenguaje unigram\" y modelo de lenguaje BigRam.",
                "Con el \"modelo de lenguaje unigram\", podemos reducir el costo de la derivación en un 26.7% (de 408/408 a 300/408) y levantar la mejora general de DCG de 2.8% a 3.4%.",
                "Para consultas cortas también, observamos tanto la mejora de DCG como la reducción de costos con el \"modelo de idioma unigram\"."
            ],
            "translated_text": "",
            "candidates": [
                "modelo de idioma unigram",
                "modelo de lenguaje unigram",
                "modelo de idioma unigram",
                "modelo de lenguaje unigram",
                "modelo de idioma unigram",
                "modelo de idioma unigram"
            ],
            "error": []
        },
        "bigram language model": {
            "translated_key": "modelo de lenguaje bigrama",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context Sensitive Stemming for Web Search Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo!",
                "Inc. 701 First Avenue Sunnyvale, California 94089 {fuchun, nawaaz, xinli, yumaol}@yahoo-inc.com ABSTRACT Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms.",
                "Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation.",
                "In this paper, we propose a context sensitive stemming method that addresses these two issues.",
                "Two unique properties make our approach feasible for Web Search.",
                "First, based on statistical language modeling, we perform context sensitive analysis on the query side.",
                "We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine.",
                "This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time.",
                "Second, our approach performs a context sensitive document matching for those expanded variants.",
                "This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision.",
                "Using word pluralization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Storage and Retrieval-Query formulation General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Web search has now become a major tool in our daily lives for information seeking.",
                "One of the important issues in Web search is that user queries are often not best formulated to get optimal results.",
                "For example, running shoe is a query that occurs frequently in query logs.",
                "However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes.",
                "Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs.",
                "This is difficult even for experienced users, and especially difficult for non-native speakers.",
                "One traditional solution is to use stemming [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term.",
                "Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing.",
                "Stemming can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing.",
                "Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows handling each query differently, and hence is preferred.",
                "Although traditional stemming increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched.",
                "When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6].",
                "In addition, it reduces system performance because the search engine has to match all the word variants.",
                "As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa.",
                "Thus, one needs to be very cautious when using stemming in Web search engines.",
                "One problem of traditional stemming is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word.",
                "For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored.",
                "For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query.",
                "Transforming book store to match book stores is fine, but matching book storing or booking store is not.",
                "A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query.",
                "However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20].",
                "A second problem of traditional stemming is its blind matching of all occurrences in documents.",
                "For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store.",
                "Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected.",
                "Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases.",
                "To alleviate these two problems, we propose a context sensitive stemming approach for Web search.",
                "Our solution consists of two context sensitive analysis, one on the query side and the other on the document side.",
                "On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms.",
                "On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query.",
                "Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines.",
                "We use pluralization handling as a running example for our stemming approach.",
                "The motivation for using pluralization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance.",
                "As far as we know, no previous research has systematically investigated the usage of pluralization in Web search.",
                "As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion.",
                "Experiments on general stemming yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper.",
                "In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2.",
                "We describe the details of the context sensitive stemming approach in Section 3.",
                "We then perform extensive experiments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5.",
                "Finally, we conclude the paper in Section 6. 2.",
                "RELATED WORK Stemming is a long studied technology.",
                "Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18].",
                "The Porter stemmer is widely used due to its simplicity and effectiveness in many applications.",
                "However, the Porter stemming makes many mistakes because its simple rules cannot fully describe English morphology.",
                "Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23].",
                "We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates.",
                "Using stemming in information retrieval is also a well known technique [8, 10].",
                "However, the effectiveness of stemming for English query systems was previously reported to be rather limited.",
                "Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance.",
                "Later, Harman [9] compares three general stemming techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper).",
                "They also proposed selective stemming based on query length and term importance, but no positive results were reported.",
                "On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%).",
                "However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search.",
                "These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2].",
                "We suspect the previous failures were mainly due to the two problems we mentioned in the introduction.",
                "Blind stemming, or a simple query length based selective stemming as used in [9] is not enough.",
                "Stemming has to be decided on case by case basis, not only at the query level but also at the document level.",
                "As we will show, if handled correctly, significant improvement can be achieved.",
                "A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25].",
                "To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21].",
                "This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search.",
                "In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift).",
                "Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query.",
                "Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search.",
                "On the other hand, stemming is much more conservative since most of the time, stemming preserves the original search intent.",
                "While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision.",
                "The increase on recall is obvious.",
                "With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded.",
                "On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the expanded query.",
                "This is similar to the relevance feedback in the sense that it requires multiple passes retrieval.",
                "If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful.",
                "It is expensive to deploy this in production Web search engines.",
                "Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine.",
                "In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search - stemming.",
                "Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision.",
                "Its simple, yet very efficient and effective, making real time stemming feasible for Web search.",
                "Our results will affirm researchers that stemming is indeed very important to large scale information retrieval. 3.",
                "CONTEXT SENSITIVE STEMMING 3.1 Overview Our system has four components as illustrated in Figure 1: candidate generation, query segmentation and head word detection, context sensitive query stemming and context sensitive document matching.",
                "Candidate generation (component 1) is performed oﬄine and generated candidates are stored in a dictionary.",
                "For an input query, we first segment query into concepts and detect the head word for each concept (component 2).",
                "We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4).",
                "Below we discuss each of the components in more detail.",
                "Component 4: context sensitive document matching Input Query: and head word detection Component 2: segment Component 1: candidate generation comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: hotel comparisons hotel −> hotels Figure 1: System Components 3.2 Expansion candidate generation One of the ways to generate candidates is using the Porter stemmer [18].",
                "The Porter stemmer simply uses morphological rules to convert a word to its base form.",
                "It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past.",
                "A more conservative way is based on using corpus analysis to improve the Porter stemmer results [26].",
                "The corpus analysis we do is based on word distributional similarity [15].",
                "The rationale of using distributional word similarity is that true variants tend to be used in similar contexts.",
                "In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word.",
                "We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus.",
                "The similarity between two words is the cosine similarity between the two corresponding feature vectors.",
                "The top 20 similar words to develop is shown in the following table. rank candidate score rank candidate score 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word develop.",
                "Column score is the similarity score.",
                "To determine the stemming candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list.",
                "After applying these rules, for the word develop, the stemming candidates are developing, developed, develops, development, developement, developer, developmental.",
                "For the pluralization handling purpose, only the candidate develops is retained.",
                "One thing we note from observing the distributionally similar words is that they are closely related semantically.",
                "These words might serve as candidates for general query expansion, a topic we will investigate in the future. 3.3 Segmentation and headword identification For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts.",
                "We first break a query into segments, each segment representing a concept which normally is a noun phrase.",
                "For each of the noun phrases, we then detect the most important word which we call the head word.",
                "Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity.",
                "To break a query into segments, we have to define a criteria to measure the strength of the relation between words.",
                "One effective method is to use mutual information as an indicator on whether or not to split two words [19].",
                "We use a log of 25M queries and collect the bigram and unigram frequencies from it.",
                "For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word.",
                "We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here.",
                "Table 2 shows some examples of query segmentation.",
                "The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query.",
                "Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: Query segmentation: a segment is bracketed. parsing since many queries are not grammatical and are very short.",
                "Applying a parser trained on sentences from documents to queries will have poor performance.",
                "In our solution, we just use simple heuristics rules, and it works very well in practice for English.",
                "For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of/in/at/from UVW.",
                "In such cases, the head word is typically the last nonstop word of XYZ. 3.4 Context sensitive word expansion After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful.",
                "Our statistics show that about half of the queries can be transformed by pluralization via naive stemming.",
                "Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse.",
                "Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost.",
                "In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful.",
                "Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts.",
                "Head words hotel and comparison can be expanded to hotels and comparisons.",
                "Are both transformations useful?",
                "To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web.",
                "The more likely a query to occur on the Web, the more relevant documents this query is able to return.",
                "Now the whole problem becomes how to calculate the probability of query to occur on the Web.",
                "Calculating the probability of string occurring in a corpus is a well known language modeling problem.",
                "The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur).",
                "The simplest and most successful approach to language modeling is still based on the n-gram model.",
                "By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.",
                "Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus.",
                "Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary.",
                "This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6).",
                "Also, because of the heavy tailed nature of language (i.e.",
                "Zipfs law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling.",
                "One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator.",
                "Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5].",
                "We used absolute smoothing in our experiments.",
                "Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string.",
                "Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3.",
                "We can see that all alternatives are less likely than the input query.",
                "It is therefore not useful to make an expansion for this query.",
                "On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded.",
                "To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score.",
                "Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face. 3.5 Context sensitive document matching Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants.",
                "For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison.",
                "However, not every occurrence of comparison in the document is of interest.",
                "A page which is about comparing customer service can contain all of the words hotel price comparisons comparison.",
                "This page is not a good page for the query.",
                "If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval.",
                "To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document.",
                "A variant match is considered valid only if the variant occurs in the same context as the original word does.",
                "The context is the left or the right non-stop segments 1 of the original word.",
                "Taking the same query as an example, the context of comparisons is price.",
                "The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price.",
                "Thus, we should only match those occurrences of comparison in the document if they occur after the word price.",
                "Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size.",
                "If the expanded word comparison occurs within the context of price within a window, it is considered valid.",
                "The smaller the window size is, the more restrictive the matching.",
                "We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases. 4.",
                "EXPERIMENTAL EVALUATION 4.1 Evaluation metrics We will measure both relevance improvement and the stemming cost required to achieve the relevance. 1 a context segment can not be a single stop word. 4.1.1 Relevance measurement We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11].",
                "Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) . (7) where gk is the weight for the document at rank k. Higher degree of relevance corresponds to a higher weight.",
                "A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights.",
                "We use dcg to represent the average DCG(5) over a set of test queries. 4.1.2 Stemming cost Another metric is to measure the additional cost incurred by stemming.",
                "Given the same level of relevance improvement, we prefer a stemming method that has less additional cost.",
                "We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed. 4.2 Data preparation We randomly sample 870 queries from a three month query log, with 290 from each month.",
                "Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming.",
                "We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words.",
                "In the end, we have 529 correctly spelled queries with at least 2 words. 4.3 Naive stemming for Web search Before explaining the experiments and results in detail, wed like to describe the traditional way of using stemming for Web search, referred as the naive model.",
                "This is to treat every word variant equivalent for all possible words in the query.",
                "The query book store will be transformed into (book OR books)(store OR stores) when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments. 4.4 Experimental setup The baseline model is the model without stemming.",
                "We first run the naive model to see how well it performs over the baseline.",
                "Then we improve the naive stemming model by document sensitive matching, referred as document sensitive matching model.",
                "This model makes the same stemming as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5.",
                "The naive model and document sensitive matching model stem the most queries.",
                "Out of the 529 queries, there are 408 queries that they stem, corresponding to 46.7% query traffic (out of a total of 870).",
                "We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as selective stemming model.",
                "Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model.",
                "We experiment with unigram language model and <br>bigram language model</br>.",
                "Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments.",
                "To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data.",
                "The oracle model only expands a word if the stemming will give better results.",
                "To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries.",
                "Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words. 4.5 Results We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5.",
                "Each row in Table 4 is a stemming strategy described in section 4.4.",
                "The first column is the name of the strategy.",
                "The second column is the number of queries affected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg.",
                "The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy).",
                "The fourth column is the relative improvement over the baseline, and the last column is the p-value of Wilcoxon significance test.",
                "There are several observations about the results.",
                "We can see the naively stemming only obtains a statistically insignificant improvement of 1.5%.",
                "Looking at Table 5, it gives an improvement of 2.7% on short queries.",
                "However, it also hurts long queries by -2.4%.",
                "Overall, the improvement is canceled out.",
                "The reason that it improves short queries is that most short queries only have one word that can be stemmed.",
                "Thus, blindly pluralizing short queries is relatively safe.",
                "However for long queries, most queries can have multiple words that can be pluralized.",
                "Expanding all of them without selection will significantly hurt precision.",
                "Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8%.",
                "The improvement comes from the conservative context sensitive document matching.",
                "An expanded word is valid only if it occurs within the context of original query in the document.",
                "This reduces many spurious matches.",
                "However, we still notice that for long queries, context sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem.",
                "While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches.",
                "It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases.",
                "Selective word pluralization further helps resolving the problem faced by document context sensitive stemming.",
                "It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stemming in the first place.",
                "By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision.",
                "With the unigram language model, we can reduce the stemming cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4%.",
                "In particular, it gives significant improvements on long queries.",
                "The dcg gain is turned from negative to positive, from −1.6% to 1.1%.",
                "This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement.",
                "For short queries too, we observe both dcg improvement and stemming cost reduction with the unigram language model.",
                "The advantages of predictive word expansion with a language model is further boosted with a better <br>bigram language model</br>.",
                "The overall dcg gain is lifted from 3.4% to 3.9%, and stemming cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic.",
                "For short queries, <br>bigram language model</br> improves the dcg gain from 4.4% to 4.7%, and reduces stemming cost from 272/272 to 150/272.",
                "For long queries, <br>bigram language model</br> improves dcg gain from 1.1% to 2.5%, and reduces stemming cost from 136/136 to 100/136.",
                "We observe that the <br>bigram language model</br> gives a larger lift for long queries.",
                "This is because the uncertainty in long queries is larger and a more powerful language model is needed.",
                "We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation.",
                "Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying.",
                "For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a <br>bigram language model</br>.",
                "For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a <br>bigram language model</br>.",
                "We may gain additional benefit with a more powerful language model for long queries.",
                "However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem.",
                "These problems have to be addressed separately.",
                "Looking at the the upper-bound of overhead reduction for oracle stemming, 75% (308/408) of the naive stemmings are wasteful.",
                "We currently capture about half of them.",
                "Further reduction of the overhead requires sacrificing the dcg gain.",
                "Now we can compare the stemming strategies from a different aspect.",
                "Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only.",
                "We can see that the number of affected queries decreases as the stemming strategy becomes more accurate (dcg improvement).",
                "For the <br>bigram language model</br>, over the 250/408 stemmed queries, the dcg improvement is 6.1%.",
                "An interesting observation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more difficult queries (low dcg queries). 5.",
                "DISCUSSIONS 5.1 Language models from query vs. from Web As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web.",
                "The language model should describe the occurrence of the string on the Web.",
                "However, the query log is also a good resource. 2 Note that this upperbound is for pluralization handling only, not for general stemming.",
                "General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics.",
                "Affected Queries dcg dcg Improvement p-value baseline 0/408 7.102 N/A N/A naive model 408/408 7.206 1.5% 0.22 document context sensitive model 408/408 7.302 2.8% 0.014 selective model: unigram LM 300/408 7.321 3.4% 0.001 selective model: bigram LM 250/408 7.381 3.9% 0.001 oracle model 100/408 7.519 5.9% 0.001 Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming Short Query Results Affected Queries dcg Improvement p-value baseline 0/272 N/A N/A naive model 272/272 2.7% 0.48 document context sensitive model 272/272 4.2% 0.002 selective model: unigram LM 185/272 4.4% 0.001 selective model: bigram LM 150/272 4.7% 0.001 oracle model 71/272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p-value baseline 0/136 N/A N/A naive model 136/136 -2.4% 0.25 document context sensitive model 136/136 -1.6% 0.27 selective model: unigram LM 115/136 1.1% 0.001 selective model: bigram LM 100/136 2.5% 0.001 oracle model 29/136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results.",
                "To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a language model from the same query top 25M queries as used to learn segmentation, and use that for prediction.",
                "We observed a slight performance decrease compared to the model trained on Web frequencies.",
                "In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries.",
                "Thus, the query log can serve as a good approximation of the Web frequencies. 5.2 How linguistics helps Some linguistic knowledge is useful in stemming.",
                "For the pluralization handling case, pluralization and de-pluralization is not symmetric.",
                "A plural word used in a query indicates a special intent.",
                "For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California.",
                "A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank.",
                "To capture this intent, we have to make sure the document is a general page about hotels in new york.",
                "We do this by requiring that the plural word hotels appears in the document.",
                "On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information.",
                "We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property. 5.3 Error analysis One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after stemming.",
                "Generally speaking, pluralization or depluralization keeps the original intent.",
                "However, the intent could change in a few cases.",
                "For one example of such a query, job at apple, we pluralize job to jobs.",
                "This stemming makes the original query ambiguous.",
                "The query job OR jobs at apple has two intents.",
                "One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company.",
                "Thus, the results after query stemming returns Steve Jobs as one of the results in top 5.",
                "One solution is performing results set based analysis to check if the intent is changed.",
                "This is similar to relevance feedback and requires second phase ranking.",
                "A second type of mistakes is the entity/concept recognition problem, These include two kinds.",
                "One is that the stemmed word variant now matches part of an entity or concept.",
                "For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco.",
                "The results will match cookie jar in san francisco.",
                "Although cookie still means the same thing as cookies, cookie jar is a different concept.",
                "Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words.",
                "For example, quote ICE is pluralized to quote OR quotes ICE.",
                "The original intent for this query is searching for stock quote for ticker ICE.",
                "However, we noticed that among the top results, one of the results is Food quotes: Ice cream.",
                "This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408/408 7.102 7.206 1.5% document context sensitive model 408/408 7.102 7.302 2.8% selective model: unigram LM 300/408 5.904 6.187 4.8% selective model: bigram LM 250/408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the affected queries before/after applying stemming the pluralized word quotes.",
                "The unchanged word ICE matches part of the noun phrase ice cream here.",
                "To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words.",
                "A third type of mistakes occurs in long queries.",
                "For the query bar code reader software, two words are pluralized. code to codes and reader to readers.",
                "In fact, bar code reader in the original query is a strong concept and the internal words should not be changed.",
                "This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking.",
                "For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept. 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented a simple yet elegant way of stemming for Web search.",
                "It improves naive stemming in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side.",
                "Using pluralization handling as an example, experiments on a major Web search engine data show it significantly improves the Web relevance and reduces the stemming cost.",
                "It also significantly improves Web click through rate (details not reported in the paper).",
                "For the future work, we are investigating the problems we identified in the error analysis section.",
                "These include: entity and noun phrase matching mistakes, and improved segmentation. 7.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving Web Search Ranking by Incorporating User Behavior Information.",
                "In SIGIR, 2006. [2] E. Airio.",
                "Word Normalization and Decompounding in Mono- and Bilingual IR.",
                "Information Retrieval, 9:249-271, 2006. [3] P. Anick.",
                "Using Terminological Feedback for Web Search Refinement: a Log-based Study.",
                "In SIGIR, 2003. [4] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press/Addison Wesley, 1999. [5] S. Chen and J. Goodman.",
                "An Empirical Study of Smoothing Techniques for Language Modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [6] S. Cronen-Townsend, Y. Zhou, and B. Croft.",
                "A Framework for Selective Query Expansion.",
                "In CIKM, 2004. [7] H. Fang and C. Zhai.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In SIGIR, 2006. [8] W. B. Frakes.",
                "Term Conflation for Information Retrieval.",
                "In C. J. Rijsbergen, editor, Research and Development in Information Retrieval, pages 383-389.",
                "Cambridge University Press, 1984. [9] D. Harman.",
                "How Effective is Suffixing?",
                "JASIS, 42(1):7-15, 1991. [10] D. Hull.",
                "Stemming Algorithms - A Case Study for Detailed Evaluation.",
                "JASIS, 47(1):70-84, 1996. [11] K. Jarvelin and J. Kekalainen.",
                "Cumulated Gain-Based Evaluation Evaluation of IR Techniques.",
                "ACM TOIS, 20:422-446, 2002. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating Query Substitutions.",
                "In WWW, 2006. [13] W. Kraaij and R. Pohlmann.",
                "Viewing Stemming as Recall Enhancement.",
                "In SIGIR, 1996. [14] R. Krovetz.",
                "Viewing Morphology as an Inference Process.",
                "In SIGIR, 1993. [15] D. Lin.",
                "Automatic Retrieval and Clustering of Similar Words.",
                "In COLING-ACL, 1998. [16] J.",
                "B. Lovins.",
                "Development of a Stemming Algorithm.",
                "Mechanical Translation and Computational Linguistics, II:22-31, 1968. [17] M. Lennon and D. Peirce and B. Tarry and P. Willett.",
                "An Evaluation of Some Conflation Algorithms for Information Retrieval.",
                "Journal of Information Science, 3:177-188, 1981. [18] M. Porter.",
                "An Algorithm for Suffix Stripping.",
                "Program, 14(3):130-137, 1980. [19] K. M. Risvik, T. Mikolajewski, and P. Boros.",
                "Query Segmentation for Web Search.",
                "In WWW, 2003. [20] S. E. Robertson.",
                "On Term Selection for Query Expansion.",
                "Journal of Documentation, 46(4):359-364, 1990. [21] G. Salton and C. Buckley.",
                "Improving Retrieval Performance by Relevance Feedback.",
                "JASIS, 41(4):288 - 297, 1999. [22] R. Sun, C.-H. Ong, and T.-S. Chua.",
                "Mining Dependency Relations for Query Expansion in Passage Retrieval.",
                "In SIGIR, 2006. [23] C. Van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, second version, 1979. [24] B. V´elez, R. Weiss, M. A. Sheldon, and D. K. Gifford.",
                "Fast and Effective Query Refinement.",
                "In SIGIR, 1997. [25] J. Xu and B. Croft.",
                "Query Expansion using Local and Global Document Analysis.",
                "In SIGIR, 1996. [26] J. Xu and B. Croft.",
                "Corpus-based Stemming using Cooccurrence of Word Variants.",
                "ACM TOIS, 16 (1):61-81, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Experimentamos con un modelo de lenguaje unigram y \"modelo de lenguaje BigRam\".",
                "Las ventajas de la expansión de palabras predictivas con un modelo de lenguaje se impulsan aún más con un mejor \"modelo de lenguaje BigRam\".",
                "Para consultas cortas, \"BigRam Language Model\" mejora la ganancia de DCG de 4.4% a 4.7%, y reduce el costo de 272/272 a 150/272.",
                "Para consultas largas, \"BigRam Language Model\" mejora la ganancia de DCG de 1.1% a 2.5%, y reduce el costo de edad de 136/136 a 100/136.",
                "Observamos que el \"modelo de lenguaje BigRam\" ofrece un ascensor más grande para consultas largas.",
                "Para consultas cortas, la ganancia DCG superior es del 6.3% para el manejo perfecto de la pluralización, nuestra ganancia actual es del 4.7% con un \"modelo de idioma BigRam\".",
                "Para consultas largas, la ganancia de DCG es del 4,6% para el manejo perfecto de la pluralización, nuestra ganancia actual es del 2.5% con un \"modelo de lenguaje BigRam\".",
                "Para el \"Modelo de lenguaje BigRam\", durante las consultas de STEMMed 250/408, la mejora de DCG es 6.1%."
            ],
            "translated_text": "",
            "candidates": [
                "modelo de idioma bigram",
                "modelo de lenguaje BigRam",
                "modelo de idioma bigram",
                "modelo de lenguaje BigRam",
                "modelo de idioma bigram",
                "BigRam Language Model",
                "modelo de idioma bigram",
                "BigRam Language Model",
                "modelo de idioma bigram",
                "modelo de lenguaje BigRam",
                "modelo de idioma bigram",
                "modelo de idioma BigRam",
                "modelo de idioma bigram",
                "modelo de lenguaje BigRam",
                "modelo de idioma bigram",
                "Modelo de lenguaje BigRam"
            ],
            "error": []
        },
        "stem": {
            "translated_key": "generar",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context Sensitive Stemming for Web Search Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo!",
                "Inc. 701 First Avenue Sunnyvale, California 94089 {fuchun, nawaaz, xinli, yumaol}@yahoo-inc.com ABSTRACT Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms.",
                "Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation.",
                "In this paper, we propose a context sensitive stemming method that addresses these two issues.",
                "Two unique properties make our approach feasible for Web Search.",
                "First, based on statistical language modeling, we perform context sensitive analysis on the query side.",
                "We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine.",
                "This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time.",
                "Second, our approach performs a context sensitive document matching for those expanded variants.",
                "This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision.",
                "Using word pluralization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Storage and Retrieval-Query formulation General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Web search has now become a major tool in our daily lives for information seeking.",
                "One of the important issues in Web search is that user queries are often not best formulated to get optimal results.",
                "For example, running shoe is a query that occurs frequently in query logs.",
                "However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes.",
                "Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs.",
                "This is difficult even for experienced users, and especially difficult for non-native speakers.",
                "One traditional solution is to use stemming [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term.",
                "Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing.",
                "Stemming can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing.",
                "Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows handling each query differently, and hence is preferred.",
                "Although traditional stemming increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched.",
                "When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6].",
                "In addition, it reduces system performance because the search engine has to match all the word variants.",
                "As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa.",
                "Thus, one needs to be very cautious when using stemming in Web search engines.",
                "One problem of traditional stemming is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word.",
                "For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored.",
                "For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query.",
                "Transforming book store to match book stores is fine, but matching book storing or booking store is not.",
                "A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query.",
                "However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20].",
                "A second problem of traditional stemming is its blind matching of all occurrences in documents.",
                "For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store.",
                "Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected.",
                "Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases.",
                "To alleviate these two problems, we propose a context sensitive stemming approach for Web search.",
                "Our solution consists of two context sensitive analysis, one on the query side and the other on the document side.",
                "On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms.",
                "On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query.",
                "Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines.",
                "We use pluralization handling as a running example for our stemming approach.",
                "The motivation for using pluralization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance.",
                "As far as we know, no previous research has systematically investigated the usage of pluralization in Web search.",
                "As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion.",
                "Experiments on general stemming yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper.",
                "In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2.",
                "We describe the details of the context sensitive stemming approach in Section 3.",
                "We then perform extensive experiments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5.",
                "Finally, we conclude the paper in Section 6. 2.",
                "RELATED WORK Stemming is a long studied technology.",
                "Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18].",
                "The Porter stemmer is widely used due to its simplicity and effectiveness in many applications.",
                "However, the Porter stemming makes many mistakes because its simple rules cannot fully describe English morphology.",
                "Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23].",
                "We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates.",
                "Using stemming in information retrieval is also a well known technique [8, 10].",
                "However, the effectiveness of stemming for English query systems was previously reported to be rather limited.",
                "Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance.",
                "Later, Harman [9] compares three general stemming techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper).",
                "They also proposed selective stemming based on query length and term importance, but no positive results were reported.",
                "On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%).",
                "However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search.",
                "These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2].",
                "We suspect the previous failures were mainly due to the two problems we mentioned in the introduction.",
                "Blind stemming, or a simple query length based selective stemming as used in [9] is not enough.",
                "Stemming has to be decided on case by case basis, not only at the query level but also at the document level.",
                "As we will show, if handled correctly, significant improvement can be achieved.",
                "A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25].",
                "To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21].",
                "This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search.",
                "In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift).",
                "Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query.",
                "Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search.",
                "On the other hand, stemming is much more conservative since most of the time, stemming preserves the original search intent.",
                "While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision.",
                "The increase on recall is obvious.",
                "With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded.",
                "On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the expanded query.",
                "This is similar to the relevance feedback in the sense that it requires multiple passes retrieval.",
                "If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful.",
                "It is expensive to deploy this in production Web search engines.",
                "Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine.",
                "In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search - stemming.",
                "Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision.",
                "Its simple, yet very efficient and effective, making real time stemming feasible for Web search.",
                "Our results will affirm researchers that stemming is indeed very important to large scale information retrieval. 3.",
                "CONTEXT SENSITIVE STEMMING 3.1 Overview Our system has four components as illustrated in Figure 1: candidate generation, query segmentation and head word detection, context sensitive query stemming and context sensitive document matching.",
                "Candidate generation (component 1) is performed oﬄine and generated candidates are stored in a dictionary.",
                "For an input query, we first segment query into concepts and detect the head word for each concept (component 2).",
                "We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4).",
                "Below we discuss each of the components in more detail.",
                "Component 4: context sensitive document matching Input Query: and head word detection Component 2: segment Component 1: candidate generation comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: hotel comparisons hotel −> hotels Figure 1: System Components 3.2 Expansion candidate generation One of the ways to generate candidates is using the Porter stemmer [18].",
                "The Porter stemmer simply uses morphological rules to convert a word to its base form.",
                "It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past.",
                "A more conservative way is based on using corpus analysis to improve the Porter stemmer results [26].",
                "The corpus analysis we do is based on word distributional similarity [15].",
                "The rationale of using distributional word similarity is that true variants tend to be used in similar contexts.",
                "In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word.",
                "We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus.",
                "The similarity between two words is the cosine similarity between the two corresponding feature vectors.",
                "The top 20 similar words to develop is shown in the following table. rank candidate score rank candidate score 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word develop.",
                "Column score is the similarity score.",
                "To determine the stemming candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list.",
                "After applying these rules, for the word develop, the stemming candidates are developing, developed, develops, development, developement, developer, developmental.",
                "For the pluralization handling purpose, only the candidate develops is retained.",
                "One thing we note from observing the distributionally similar words is that they are closely related semantically.",
                "These words might serve as candidates for general query expansion, a topic we will investigate in the future. 3.3 Segmentation and headword identification For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts.",
                "We first break a query into segments, each segment representing a concept which normally is a noun phrase.",
                "For each of the noun phrases, we then detect the most important word which we call the head word.",
                "Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity.",
                "To break a query into segments, we have to define a criteria to measure the strength of the relation between words.",
                "One effective method is to use mutual information as an indicator on whether or not to split two words [19].",
                "We use a log of 25M queries and collect the bigram and unigram frequencies from it.",
                "For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word.",
                "We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here.",
                "Table 2 shows some examples of query segmentation.",
                "The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query.",
                "Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: Query segmentation: a segment is bracketed. parsing since many queries are not grammatical and are very short.",
                "Applying a parser trained on sentences from documents to queries will have poor performance.",
                "In our solution, we just use simple heuristics rules, and it works very well in practice for English.",
                "For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of/in/at/from UVW.",
                "In such cases, the head word is typically the last nonstop word of XYZ. 3.4 Context sensitive word expansion After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful.",
                "Our statistics show that about half of the queries can be transformed by pluralization via naive stemming.",
                "Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse.",
                "Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost.",
                "In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful.",
                "Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts.",
                "Head words hotel and comparison can be expanded to hotels and comparisons.",
                "Are both transformations useful?",
                "To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web.",
                "The more likely a query to occur on the Web, the more relevant documents this query is able to return.",
                "Now the whole problem becomes how to calculate the probability of query to occur on the Web.",
                "Calculating the probability of string occurring in a corpus is a well known language modeling problem.",
                "The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur).",
                "The simplest and most successful approach to language modeling is still based on the n-gram model.",
                "By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.",
                "Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus.",
                "Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary.",
                "This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6).",
                "Also, because of the heavy tailed nature of language (i.e.",
                "Zipfs law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling.",
                "One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator.",
                "Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5].",
                "We used absolute smoothing in our experiments.",
                "Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string.",
                "Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3.",
                "We can see that all alternatives are less likely than the input query.",
                "It is therefore not useful to make an expansion for this query.",
                "On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded.",
                "To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score.",
                "Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face. 3.5 Context sensitive document matching Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants.",
                "For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison.",
                "However, not every occurrence of comparison in the document is of interest.",
                "A page which is about comparing customer service can contain all of the words hotel price comparisons comparison.",
                "This page is not a good page for the query.",
                "If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval.",
                "To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document.",
                "A variant match is considered valid only if the variant occurs in the same context as the original word does.",
                "The context is the left or the right non-stop segments 1 of the original word.",
                "Taking the same query as an example, the context of comparisons is price.",
                "The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price.",
                "Thus, we should only match those occurrences of comparison in the document if they occur after the word price.",
                "Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size.",
                "If the expanded word comparison occurs within the context of price within a window, it is considered valid.",
                "The smaller the window size is, the more restrictive the matching.",
                "We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases. 4.",
                "EXPERIMENTAL EVALUATION 4.1 Evaluation metrics We will measure both relevance improvement and the stemming cost required to achieve the relevance. 1 a context segment can not be a single stop word. 4.1.1 Relevance measurement We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11].",
                "Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) . (7) where gk is the weight for the document at rank k. Higher degree of relevance corresponds to a higher weight.",
                "A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights.",
                "We use dcg to represent the average DCG(5) over a set of test queries. 4.1.2 Stemming cost Another metric is to measure the additional cost incurred by stemming.",
                "Given the same level of relevance improvement, we prefer a stemming method that has less additional cost.",
                "We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed. 4.2 Data preparation We randomly sample 870 queries from a three month query log, with 290 from each month.",
                "Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming.",
                "We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words.",
                "In the end, we have 529 correctly spelled queries with at least 2 words. 4.3 Naive stemming for Web search Before explaining the experiments and results in detail, wed like to describe the traditional way of using stemming for Web search, referred as the naive model.",
                "This is to treat every word variant equivalent for all possible words in the query.",
                "The query book store will be transformed into (book OR books)(store OR stores) when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments. 4.4 Experimental setup The baseline model is the model without stemming.",
                "We first run the naive model to see how well it performs over the baseline.",
                "Then we improve the naive stemming model by document sensitive matching, referred as document sensitive matching model.",
                "This model makes the same stemming as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5.",
                "The naive model and document sensitive matching model <br>stem</br> the most queries.",
                "Out of the 529 queries, there are 408 queries that they <br>stem</br>, corresponding to 46.7% query traffic (out of a total of 870).",
                "We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as selective stemming model.",
                "Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model.",
                "We experiment with unigram language model and bigram language model.",
                "Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments.",
                "To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data.",
                "The oracle model only expands a word if the stemming will give better results.",
                "To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries.",
                "Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words. 4.5 Results We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5.",
                "Each row in Table 4 is a stemming strategy described in section 4.4.",
                "The first column is the name of the strategy.",
                "The second column is the number of queries affected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg.",
                "The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy).",
                "The fourth column is the relative improvement over the baseline, and the last column is the p-value of Wilcoxon significance test.",
                "There are several observations about the results.",
                "We can see the naively stemming only obtains a statistically insignificant improvement of 1.5%.",
                "Looking at Table 5, it gives an improvement of 2.7% on short queries.",
                "However, it also hurts long queries by -2.4%.",
                "Overall, the improvement is canceled out.",
                "The reason that it improves short queries is that most short queries only have one word that can be stemmed.",
                "Thus, blindly pluralizing short queries is relatively safe.",
                "However for long queries, most queries can have multiple words that can be pluralized.",
                "Expanding all of them without selection will significantly hurt precision.",
                "Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8%.",
                "The improvement comes from the conservative context sensitive document matching.",
                "An expanded word is valid only if it occurs within the context of original query in the document.",
                "This reduces many spurious matches.",
                "However, we still notice that for long queries, context sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem.",
                "While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches.",
                "It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases.",
                "Selective word pluralization further helps resolving the problem faced by document context sensitive stemming.",
                "It does not <br>stem</br> every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stemming in the first place.",
                "By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision.",
                "With the unigram language model, we can reduce the stemming cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4%.",
                "In particular, it gives significant improvements on long queries.",
                "The dcg gain is turned from negative to positive, from −1.6% to 1.1%.",
                "This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement.",
                "For short queries too, we observe both dcg improvement and stemming cost reduction with the unigram language model.",
                "The advantages of predictive word expansion with a language model is further boosted with a better bigram language model.",
                "The overall dcg gain is lifted from 3.4% to 3.9%, and stemming cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic.",
                "For short queries, bigram language model improves the dcg gain from 4.4% to 4.7%, and reduces stemming cost from 272/272 to 150/272.",
                "For long queries, bigram language model improves dcg gain from 1.1% to 2.5%, and reduces stemming cost from 136/136 to 100/136.",
                "We observe that the bigram language model gives a larger lift for long queries.",
                "This is because the uncertainty in long queries is larger and a more powerful language model is needed.",
                "We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation.",
                "Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying.",
                "For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram language model.",
                "For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram language model.",
                "We may gain additional benefit with a more powerful language model for long queries.",
                "However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem.",
                "These problems have to be addressed separately.",
                "Looking at the the upper-bound of overhead reduction for oracle stemming, 75% (308/408) of the naive stemmings are wasteful.",
                "We currently capture about half of them.",
                "Further reduction of the overhead requires sacrificing the dcg gain.",
                "Now we can compare the stemming strategies from a different aspect.",
                "Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only.",
                "We can see that the number of affected queries decreases as the stemming strategy becomes more accurate (dcg improvement).",
                "For the bigram language model, over the 250/408 stemmed queries, the dcg improvement is 6.1%.",
                "An interesting observation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more difficult queries (low dcg queries). 5.",
                "DISCUSSIONS 5.1 Language models from query vs. from Web As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web.",
                "The language model should describe the occurrence of the string on the Web.",
                "However, the query log is also a good resource. 2 Note that this upperbound is for pluralization handling only, not for general stemming.",
                "General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics.",
                "Affected Queries dcg dcg Improvement p-value baseline 0/408 7.102 N/A N/A naive model 408/408 7.206 1.5% 0.22 document context sensitive model 408/408 7.302 2.8% 0.014 selective model: unigram LM 300/408 7.321 3.4% 0.001 selective model: bigram LM 250/408 7.381 3.9% 0.001 oracle model 100/408 7.519 5.9% 0.001 Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming Short Query Results Affected Queries dcg Improvement p-value baseline 0/272 N/A N/A naive model 272/272 2.7% 0.48 document context sensitive model 272/272 4.2% 0.002 selective model: unigram LM 185/272 4.4% 0.001 selective model: bigram LM 150/272 4.7% 0.001 oracle model 71/272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p-value baseline 0/136 N/A N/A naive model 136/136 -2.4% 0.25 document context sensitive model 136/136 -1.6% 0.27 selective model: unigram LM 115/136 1.1% 0.001 selective model: bigram LM 100/136 2.5% 0.001 oracle model 29/136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results.",
                "To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a language model from the same query top 25M queries as used to learn segmentation, and use that for prediction.",
                "We observed a slight performance decrease compared to the model trained on Web frequencies.",
                "In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries.",
                "Thus, the query log can serve as a good approximation of the Web frequencies. 5.2 How linguistics helps Some linguistic knowledge is useful in stemming.",
                "For the pluralization handling case, pluralization and de-pluralization is not symmetric.",
                "A plural word used in a query indicates a special intent.",
                "For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California.",
                "A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank.",
                "To capture this intent, we have to make sure the document is a general page about hotels in new york.",
                "We do this by requiring that the plural word hotels appears in the document.",
                "On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information.",
                "We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property. 5.3 Error analysis One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after stemming.",
                "Generally speaking, pluralization or depluralization keeps the original intent.",
                "However, the intent could change in a few cases.",
                "For one example of such a query, job at apple, we pluralize job to jobs.",
                "This stemming makes the original query ambiguous.",
                "The query job OR jobs at apple has two intents.",
                "One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company.",
                "Thus, the results after query stemming returns Steve Jobs as one of the results in top 5.",
                "One solution is performing results set based analysis to check if the intent is changed.",
                "This is similar to relevance feedback and requires second phase ranking.",
                "A second type of mistakes is the entity/concept recognition problem, These include two kinds.",
                "One is that the stemmed word variant now matches part of an entity or concept.",
                "For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco.",
                "The results will match cookie jar in san francisco.",
                "Although cookie still means the same thing as cookies, cookie jar is a different concept.",
                "Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words.",
                "For example, quote ICE is pluralized to quote OR quotes ICE.",
                "The original intent for this query is searching for stock quote for ticker ICE.",
                "However, we noticed that among the top results, one of the results is Food quotes: Ice cream.",
                "This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408/408 7.102 7.206 1.5% document context sensitive model 408/408 7.102 7.302 2.8% selective model: unigram LM 300/408 5.904 6.187 4.8% selective model: bigram LM 250/408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the affected queries before/after applying stemming the pluralized word quotes.",
                "The unchanged word ICE matches part of the noun phrase ice cream here.",
                "To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words.",
                "A third type of mistakes occurs in long queries.",
                "For the query bar code reader software, two words are pluralized. code to codes and reader to readers.",
                "In fact, bar code reader in the original query is a strong concept and the internal words should not be changed.",
                "This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking.",
                "For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept. 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented a simple yet elegant way of stemming for Web search.",
                "It improves naive stemming in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side.",
                "Using pluralization handling as an example, experiments on a major Web search engine data show it significantly improves the Web relevance and reduces the stemming cost.",
                "It also significantly improves Web click through rate (details not reported in the paper).",
                "For the future work, we are investigating the problems we identified in the error analysis section.",
                "These include: entity and noun phrase matching mistakes, and improved segmentation. 7.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving Web Search Ranking by Incorporating User Behavior Information.",
                "In SIGIR, 2006. [2] E. Airio.",
                "Word Normalization and Decompounding in Mono- and Bilingual IR.",
                "Information Retrieval, 9:249-271, 2006. [3] P. Anick.",
                "Using Terminological Feedback for Web Search Refinement: a Log-based Study.",
                "In SIGIR, 2003. [4] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press/Addison Wesley, 1999. [5] S. Chen and J. Goodman.",
                "An Empirical Study of Smoothing Techniques for Language Modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [6] S. Cronen-Townsend, Y. Zhou, and B. Croft.",
                "A Framework for Selective Query Expansion.",
                "In CIKM, 2004. [7] H. Fang and C. Zhai.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In SIGIR, 2006. [8] W. B. Frakes.",
                "Term Conflation for Information Retrieval.",
                "In C. J. Rijsbergen, editor, Research and Development in Information Retrieval, pages 383-389.",
                "Cambridge University Press, 1984. [9] D. Harman.",
                "How Effective is Suffixing?",
                "JASIS, 42(1):7-15, 1991. [10] D. Hull.",
                "Stemming Algorithms - A Case Study for Detailed Evaluation.",
                "JASIS, 47(1):70-84, 1996. [11] K. Jarvelin and J. Kekalainen.",
                "Cumulated Gain-Based Evaluation Evaluation of IR Techniques.",
                "ACM TOIS, 20:422-446, 2002. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating Query Substitutions.",
                "In WWW, 2006. [13] W. Kraaij and R. Pohlmann.",
                "Viewing Stemming as Recall Enhancement.",
                "In SIGIR, 1996. [14] R. Krovetz.",
                "Viewing Morphology as an Inference Process.",
                "In SIGIR, 1993. [15] D. Lin.",
                "Automatic Retrieval and Clustering of Similar Words.",
                "In COLING-ACL, 1998. [16] J.",
                "B. Lovins.",
                "Development of a Stemming Algorithm.",
                "Mechanical Translation and Computational Linguistics, II:22-31, 1968. [17] M. Lennon and D. Peirce and B. Tarry and P. Willett.",
                "An Evaluation of Some Conflation Algorithms for Information Retrieval.",
                "Journal of Information Science, 3:177-188, 1981. [18] M. Porter.",
                "An Algorithm for Suffix Stripping.",
                "Program, 14(3):130-137, 1980. [19] K. M. Risvik, T. Mikolajewski, and P. Boros.",
                "Query Segmentation for Web Search.",
                "In WWW, 2003. [20] S. E. Robertson.",
                "On Term Selection for Query Expansion.",
                "Journal of Documentation, 46(4):359-364, 1990. [21] G. Salton and C. Buckley.",
                "Improving Retrieval Performance by Relevance Feedback.",
                "JASIS, 41(4):288 - 297, 1999. [22] R. Sun, C.-H. Ong, and T.-S. Chua.",
                "Mining Dependency Relations for Query Expansion in Passage Retrieval.",
                "In SIGIR, 2006. [23] C. Van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, second version, 1979. [24] B. V´elez, R. Weiss, M. A. Sheldon, and D. K. Gifford.",
                "Fast and Effective Query Refinement.",
                "In SIGIR, 1997. [25] J. Xu and B. Croft.",
                "Query Expansion using Local and Global Document Analysis.",
                "In SIGIR, 1996. [26] J. Xu and B. Croft.",
                "Corpus-based Stemming using Cooccurrence of Word Variants.",
                "ACM TOIS, 16 (1):61-81, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El modelo ingenuo y el modelo de coincidencia sensible al documento \"STEM\" la mayoría de las consultas.",
                "De las 529 consultas, hay 408 consultas que \"se basan\", correspondientes al 46.7% de tráfico de consultas (de un total de 870).",
                "No se \"detiene\" cada palabra que coloca toda la carga en el algoritmo de clasificación, pero trata de eliminar las medidas innecesarias en primer lugar."
            ],
            "translated_text": "",
            "candidates": [
                "provenir",
                "STEM",
                "provenir",
                "se basan",
                "provenir",
                "detiene"
            ],
            "error": []
        },
        "language model": {
            "translated_key": "modelo de lenguaje",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context Sensitive Stemming for Web Search Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo!",
                "Inc. 701 First Avenue Sunnyvale, California 94089 {fuchun, nawaaz, xinli, yumaol}@yahoo-inc.com ABSTRACT Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms.",
                "Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation.",
                "In this paper, we propose a context sensitive stemming method that addresses these two issues.",
                "Two unique properties make our approach feasible for Web Search.",
                "First, based on statistical language modeling, we perform context sensitive analysis on the query side.",
                "We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine.",
                "This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time.",
                "Second, our approach performs a context sensitive document matching for those expanded variants.",
                "This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision.",
                "Using word pluralization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic.",
                "Categories and Subject Descriptors H.3.3 [Information Systems]: Information Storage and Retrieval-Query formulation General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Web search has now become a major tool in our daily lives for information seeking.",
                "One of the important issues in Web search is that user queries are often not best formulated to get optimal results.",
                "For example, running shoe is a query that occurs frequently in query logs.",
                "However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes.",
                "Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs.",
                "This is difficult even for experienced users, and especially difficult for non-native speakers.",
                "One traditional solution is to use stemming [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term.",
                "Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing.",
                "Stemming can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing.",
                "Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows handling each query differently, and hence is preferred.",
                "Although traditional stemming increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched.",
                "When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6].",
                "In addition, it reduces system performance because the search engine has to match all the word variants.",
                "As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa.",
                "Thus, one needs to be very cautious when using stemming in Web search engines.",
                "One problem of traditional stemming is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word.",
                "For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored.",
                "For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query.",
                "Transforming book store to match book stores is fine, but matching book storing or booking store is not.",
                "A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query.",
                "However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20].",
                "A second problem of traditional stemming is its blind matching of all occurrences in documents.",
                "For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store.",
                "Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected.",
                "Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases.",
                "To alleviate these two problems, we propose a context sensitive stemming approach for Web search.",
                "Our solution consists of two context sensitive analysis, one on the query side and the other on the document side.",
                "On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms.",
                "On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query.",
                "Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines.",
                "We use pluralization handling as a running example for our stemming approach.",
                "The motivation for using pluralization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance.",
                "As far as we know, no previous research has systematically investigated the usage of pluralization in Web search.",
                "As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion.",
                "Experiments on general stemming yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper.",
                "In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2.",
                "We describe the details of the context sensitive stemming approach in Section 3.",
                "We then perform extensive experiments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5.",
                "Finally, we conclude the paper in Section 6. 2.",
                "RELATED WORK Stemming is a long studied technology.",
                "Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18].",
                "The Porter stemmer is widely used due to its simplicity and effectiveness in many applications.",
                "However, the Porter stemming makes many mistakes because its simple rules cannot fully describe English morphology.",
                "Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23].",
                "We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates.",
                "Using stemming in information retrieval is also a well known technique [8, 10].",
                "However, the effectiveness of stemming for English query systems was previously reported to be rather limited.",
                "Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance.",
                "Later, Harman [9] compares three general stemming techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper).",
                "They also proposed selective stemming based on query length and term importance, but no positive results were reported.",
                "On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%).",
                "However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search.",
                "These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2].",
                "We suspect the previous failures were mainly due to the two problems we mentioned in the introduction.",
                "Blind stemming, or a simple query length based selective stemming as used in [9] is not enough.",
                "Stemming has to be decided on case by case basis, not only at the query level but also at the document level.",
                "As we will show, if handled correctly, significant improvement can be achieved.",
                "A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25].",
                "To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21].",
                "This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search.",
                "In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift).",
                "Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query.",
                "Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search.",
                "On the other hand, stemming is much more conservative since most of the time, stemming preserves the original search intent.",
                "While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision.",
                "The increase on recall is obvious.",
                "With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded.",
                "On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the expanded query.",
                "This is similar to the relevance feedback in the sense that it requires multiple passes retrieval.",
                "If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful.",
                "It is expensive to deploy this in production Web search engines.",
                "Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine.",
                "In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search - stemming.",
                "Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision.",
                "Its simple, yet very efficient and effective, making real time stemming feasible for Web search.",
                "Our results will affirm researchers that stemming is indeed very important to large scale information retrieval. 3.",
                "CONTEXT SENSITIVE STEMMING 3.1 Overview Our system has four components as illustrated in Figure 1: candidate generation, query segmentation and head word detection, context sensitive query stemming and context sensitive document matching.",
                "Candidate generation (component 1) is performed oﬄine and generated candidates are stored in a dictionary.",
                "For an input query, we first segment query into concepts and detect the head word for each concept (component 2).",
                "We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4).",
                "Below we discuss each of the components in more detail.",
                "Component 4: context sensitive document matching Input Query: and head word detection Component 2: segment Component 1: candidate generation comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: hotel comparisons hotel −> hotels Figure 1: System Components 3.2 Expansion candidate generation One of the ways to generate candidates is using the Porter stemmer [18].",
                "The Porter stemmer simply uses morphological rules to convert a word to its base form.",
                "It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past.",
                "A more conservative way is based on using corpus analysis to improve the Porter stemmer results [26].",
                "The corpus analysis we do is based on word distributional similarity [15].",
                "The rationale of using distributional word similarity is that true variants tend to be used in similar contexts.",
                "In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word.",
                "We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus.",
                "The similarity between two words is the cosine similarity between the two corresponding feature vectors.",
                "The top 20 similar words to develop is shown in the following table. rank candidate score rank candidate score 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word develop.",
                "Column score is the similarity score.",
                "To determine the stemming candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list.",
                "After applying these rules, for the word develop, the stemming candidates are developing, developed, develops, development, developement, developer, developmental.",
                "For the pluralization handling purpose, only the candidate develops is retained.",
                "One thing we note from observing the distributionally similar words is that they are closely related semantically.",
                "These words might serve as candidates for general query expansion, a topic we will investigate in the future. 3.3 Segmentation and headword identification For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts.",
                "We first break a query into segments, each segment representing a concept which normally is a noun phrase.",
                "For each of the noun phrases, we then detect the most important word which we call the head word.",
                "Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity.",
                "To break a query into segments, we have to define a criteria to measure the strength of the relation between words.",
                "One effective method is to use mutual information as an indicator on whether or not to split two words [19].",
                "We use a log of 25M queries and collect the bigram and unigram frequencies from it.",
                "For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word.",
                "We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here.",
                "Table 2 shows some examples of query segmentation.",
                "The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query.",
                "Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: Query segmentation: a segment is bracketed. parsing since many queries are not grammatical and are very short.",
                "Applying a parser trained on sentences from documents to queries will have poor performance.",
                "In our solution, we just use simple heuristics rules, and it works very well in practice for English.",
                "For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of/in/at/from UVW.",
                "In such cases, the head word is typically the last nonstop word of XYZ. 3.4 Context sensitive word expansion After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful.",
                "Our statistics show that about half of the queries can be transformed by pluralization via naive stemming.",
                "Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse.",
                "Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost.",
                "In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful.",
                "Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts.",
                "Head words hotel and comparison can be expanded to hotels and comparisons.",
                "Are both transformations useful?",
                "To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web.",
                "The more likely a query to occur on the Web, the more relevant documents this query is able to return.",
                "Now the whole problem becomes how to calculate the probability of query to occur on the Web.",
                "Calculating the probability of string occurring in a corpus is a well known language modeling problem.",
                "The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur).",
                "The simplest and most successful approach to language modeling is still based on the n-gram model.",
                "By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.",
                "Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus.",
                "Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary.",
                "This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6).",
                "Also, because of the heavy tailed nature of language (i.e.",
                "Zipfs law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling.",
                "One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator.",
                "Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5].",
                "We used absolute smoothing in our experiments.",
                "Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string.",
                "Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3.",
                "We can see that all alternatives are less likely than the input query.",
                "It is therefore not useful to make an expansion for this query.",
                "On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded.",
                "To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score.",
                "Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face. 3.5 Context sensitive document matching Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants.",
                "For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison.",
                "However, not every occurrence of comparison in the document is of interest.",
                "A page which is about comparing customer service can contain all of the words hotel price comparisons comparison.",
                "This page is not a good page for the query.",
                "If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval.",
                "To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document.",
                "A variant match is considered valid only if the variant occurs in the same context as the original word does.",
                "The context is the left or the right non-stop segments 1 of the original word.",
                "Taking the same query as an example, the context of comparisons is price.",
                "The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price.",
                "Thus, we should only match those occurrences of comparison in the document if they occur after the word price.",
                "Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size.",
                "If the expanded word comparison occurs within the context of price within a window, it is considered valid.",
                "The smaller the window size is, the more restrictive the matching.",
                "We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases. 4.",
                "EXPERIMENTAL EVALUATION 4.1 Evaluation metrics We will measure both relevance improvement and the stemming cost required to achieve the relevance. 1 a context segment can not be a single stop word. 4.1.1 Relevance measurement We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11].",
                "Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) . (7) where gk is the weight for the document at rank k. Higher degree of relevance corresponds to a higher weight.",
                "A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights.",
                "We use dcg to represent the average DCG(5) over a set of test queries. 4.1.2 Stemming cost Another metric is to measure the additional cost incurred by stemming.",
                "Given the same level of relevance improvement, we prefer a stemming method that has less additional cost.",
                "We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed. 4.2 Data preparation We randomly sample 870 queries from a three month query log, with 290 from each month.",
                "Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming.",
                "We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words.",
                "In the end, we have 529 correctly spelled queries with at least 2 words. 4.3 Naive stemming for Web search Before explaining the experiments and results in detail, wed like to describe the traditional way of using stemming for Web search, referred as the naive model.",
                "This is to treat every word variant equivalent for all possible words in the query.",
                "The query book store will be transformed into (book OR books)(store OR stores) when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments. 4.4 Experimental setup The baseline model is the model without stemming.",
                "We first run the naive model to see how well it performs over the baseline.",
                "Then we improve the naive stemming model by document sensitive matching, referred as document sensitive matching model.",
                "This model makes the same stemming as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5.",
                "The naive model and document sensitive matching model stem the most queries.",
                "Out of the 529 queries, there are 408 queries that they stem, corresponding to 46.7% query traffic (out of a total of 870).",
                "We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as selective stemming model.",
                "Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model.",
                "We experiment with unigram <br>language model</br> and bigram <br>language model</br>.",
                "Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments.",
                "To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data.",
                "The oracle model only expands a word if the stemming will give better results.",
                "To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries.",
                "Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words. 4.5 Results We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5.",
                "Each row in Table 4 is a stemming strategy described in section 4.4.",
                "The first column is the name of the strategy.",
                "The second column is the number of queries affected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg.",
                "The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy).",
                "The fourth column is the relative improvement over the baseline, and the last column is the p-value of Wilcoxon significance test.",
                "There are several observations about the results.",
                "We can see the naively stemming only obtains a statistically insignificant improvement of 1.5%.",
                "Looking at Table 5, it gives an improvement of 2.7% on short queries.",
                "However, it also hurts long queries by -2.4%.",
                "Overall, the improvement is canceled out.",
                "The reason that it improves short queries is that most short queries only have one word that can be stemmed.",
                "Thus, blindly pluralizing short queries is relatively safe.",
                "However for long queries, most queries can have multiple words that can be pluralized.",
                "Expanding all of them without selection will significantly hurt precision.",
                "Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8%.",
                "The improvement comes from the conservative context sensitive document matching.",
                "An expanded word is valid only if it occurs within the context of original query in the document.",
                "This reduces many spurious matches.",
                "However, we still notice that for long queries, context sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem.",
                "While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches.",
                "It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases.",
                "Selective word pluralization further helps resolving the problem faced by document context sensitive stemming.",
                "It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stemming in the first place.",
                "By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision.",
                "With the unigram <br>language model</br>, we can reduce the stemming cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4%.",
                "In particular, it gives significant improvements on long queries.",
                "The dcg gain is turned from negative to positive, from −1.6% to 1.1%.",
                "This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement.",
                "For short queries too, we observe both dcg improvement and stemming cost reduction with the unigram <br>language model</br>.",
                "The advantages of predictive word expansion with a <br>language model</br> is further boosted with a better bigram <br>language model</br>.",
                "The overall dcg gain is lifted from 3.4% to 3.9%, and stemming cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic.",
                "For short queries, bigram <br>language model</br> improves the dcg gain from 4.4% to 4.7%, and reduces stemming cost from 272/272 to 150/272.",
                "For long queries, bigram <br>language model</br> improves dcg gain from 1.1% to 2.5%, and reduces stemming cost from 136/136 to 100/136.",
                "We observe that the bigram <br>language model</br> gives a larger lift for long queries.",
                "This is because the uncertainty in long queries is larger and a more powerful <br>language model</br> is needed.",
                "We hypothesize that a trigram <br>language model</br> would give a further lift for long queries and leave this for future investigation.",
                "Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying.",
                "For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram <br>language model</br>.",
                "For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram <br>language model</br>.",
                "We may gain additional benefit with a more powerful <br>language model</br> for long queries.",
                "However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem.",
                "These problems have to be addressed separately.",
                "Looking at the the upper-bound of overhead reduction for oracle stemming, 75% (308/408) of the naive stemmings are wasteful.",
                "We currently capture about half of them.",
                "Further reduction of the overhead requires sacrificing the dcg gain.",
                "Now we can compare the stemming strategies from a different aspect.",
                "Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only.",
                "We can see that the number of affected queries decreases as the stemming strategy becomes more accurate (dcg improvement).",
                "For the bigram <br>language model</br>, over the 250/408 stemmed queries, the dcg improvement is 6.1%.",
                "An interesting observation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more difficult queries (low dcg queries). 5.",
                "DISCUSSIONS 5.1 Language models from query vs. from Web As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web.",
                "The <br>language model</br> should describe the occurrence of the string on the Web.",
                "However, the query log is also a good resource. 2 Note that this upperbound is for pluralization handling only, not for general stemming.",
                "General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics.",
                "Affected Queries dcg dcg Improvement p-value baseline 0/408 7.102 N/A N/A naive model 408/408 7.206 1.5% 0.22 document context sensitive model 408/408 7.302 2.8% 0.014 selective model: unigram LM 300/408 7.321 3.4% 0.001 selective model: bigram LM 250/408 7.381 3.9% 0.001 oracle model 100/408 7.519 5.9% 0.001 Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming Short Query Results Affected Queries dcg Improvement p-value baseline 0/272 N/A N/A naive model 272/272 2.7% 0.48 document context sensitive model 272/272 4.2% 0.002 selective model: unigram LM 185/272 4.4% 0.001 selective model: bigram LM 150/272 4.7% 0.001 oracle model 71/272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p-value baseline 0/136 N/A N/A naive model 136/136 -2.4% 0.25 document context sensitive model 136/136 -1.6% 0.27 selective model: unigram LM 115/136 1.1% 0.001 selective model: bigram LM 100/136 2.5% 0.001 oracle model 29/136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results.",
                "To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a <br>language model</br> from the same query top 25M queries as used to learn segmentation, and use that for prediction.",
                "We observed a slight performance decrease compared to the model trained on Web frequencies.",
                "In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries.",
                "Thus, the query log can serve as a good approximation of the Web frequencies. 5.2 How linguistics helps Some linguistic knowledge is useful in stemming.",
                "For the pluralization handling case, pluralization and de-pluralization is not symmetric.",
                "A plural word used in a query indicates a special intent.",
                "For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California.",
                "A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank.",
                "To capture this intent, we have to make sure the document is a general page about hotels in new york.",
                "We do this by requiring that the plural word hotels appears in the document.",
                "On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information.",
                "We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property. 5.3 Error analysis One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after stemming.",
                "Generally speaking, pluralization or depluralization keeps the original intent.",
                "However, the intent could change in a few cases.",
                "For one example of such a query, job at apple, we pluralize job to jobs.",
                "This stemming makes the original query ambiguous.",
                "The query job OR jobs at apple has two intents.",
                "One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company.",
                "Thus, the results after query stemming returns Steve Jobs as one of the results in top 5.",
                "One solution is performing results set based analysis to check if the intent is changed.",
                "This is similar to relevance feedback and requires second phase ranking.",
                "A second type of mistakes is the entity/concept recognition problem, These include two kinds.",
                "One is that the stemmed word variant now matches part of an entity or concept.",
                "For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco.",
                "The results will match cookie jar in san francisco.",
                "Although cookie still means the same thing as cookies, cookie jar is a different concept.",
                "Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words.",
                "For example, quote ICE is pluralized to quote OR quotes ICE.",
                "The original intent for this query is searching for stock quote for ticker ICE.",
                "However, we noticed that among the top results, one of the results is Food quotes: Ice cream.",
                "This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408/408 7.102 7.206 1.5% document context sensitive model 408/408 7.102 7.302 2.8% selective model: unigram LM 300/408 5.904 6.187 4.8% selective model: bigram LM 250/408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the affected queries before/after applying stemming the pluralized word quotes.",
                "The unchanged word ICE matches part of the noun phrase ice cream here.",
                "To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words.",
                "A third type of mistakes occurs in long queries.",
                "For the query bar code reader software, two words are pluralized. code to codes and reader to readers.",
                "In fact, bar code reader in the original query is a strong concept and the internal words should not be changed.",
                "This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking.",
                "For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept. 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented a simple yet elegant way of stemming for Web search.",
                "It improves naive stemming in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side.",
                "Using pluralization handling as an example, experiments on a major Web search engine data show it significantly improves the Web relevance and reduces the stemming cost.",
                "It also significantly improves Web click through rate (details not reported in the paper).",
                "For the future work, we are investigating the problems we identified in the error analysis section.",
                "These include: entity and noun phrase matching mistakes, and improved segmentation. 7.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving Web Search Ranking by Incorporating User Behavior Information.",
                "In SIGIR, 2006. [2] E. Airio.",
                "Word Normalization and Decompounding in Mono- and Bilingual IR.",
                "Information Retrieval, 9:249-271, 2006. [3] P. Anick.",
                "Using Terminological Feedback for Web Search Refinement: a Log-based Study.",
                "In SIGIR, 2003. [4] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press/Addison Wesley, 1999. [5] S. Chen and J. Goodman.",
                "An Empirical Study of Smoothing Techniques for Language Modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [6] S. Cronen-Townsend, Y. Zhou, and B. Croft.",
                "A Framework for Selective Query Expansion.",
                "In CIKM, 2004. [7] H. Fang and C. Zhai.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In SIGIR, 2006. [8] W. B. Frakes.",
                "Term Conflation for Information Retrieval.",
                "In C. J. Rijsbergen, editor, Research and Development in Information Retrieval, pages 383-389.",
                "Cambridge University Press, 1984. [9] D. Harman.",
                "How Effective is Suffixing?",
                "JASIS, 42(1):7-15, 1991. [10] D. Hull.",
                "Stemming Algorithms - A Case Study for Detailed Evaluation.",
                "JASIS, 47(1):70-84, 1996. [11] K. Jarvelin and J. Kekalainen.",
                "Cumulated Gain-Based Evaluation Evaluation of IR Techniques.",
                "ACM TOIS, 20:422-446, 2002. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating Query Substitutions.",
                "In WWW, 2006. [13] W. Kraaij and R. Pohlmann.",
                "Viewing Stemming as Recall Enhancement.",
                "In SIGIR, 1996. [14] R. Krovetz.",
                "Viewing Morphology as an Inference Process.",
                "In SIGIR, 1993. [15] D. Lin.",
                "Automatic Retrieval and Clustering of Similar Words.",
                "In COLING-ACL, 1998. [16] J.",
                "B. Lovins.",
                "Development of a Stemming Algorithm.",
                "Mechanical Translation and Computational Linguistics, II:22-31, 1968. [17] M. Lennon and D. Peirce and B. Tarry and P. Willett.",
                "An Evaluation of Some Conflation Algorithms for Information Retrieval.",
                "Journal of Information Science, 3:177-188, 1981. [18] M. Porter.",
                "An Algorithm for Suffix Stripping.",
                "Program, 14(3):130-137, 1980. [19] K. M. Risvik, T. Mikolajewski, and P. Boros.",
                "Query Segmentation for Web Search.",
                "In WWW, 2003. [20] S. E. Robertson.",
                "On Term Selection for Query Expansion.",
                "Journal of Documentation, 46(4):359-364, 1990. [21] G. Salton and C. Buckley.",
                "Improving Retrieval Performance by Relevance Feedback.",
                "JASIS, 41(4):288 - 297, 1999. [22] R. Sun, C.-H. Ong, and T.-S. Chua.",
                "Mining Dependency Relations for Query Expansion in Passage Retrieval.",
                "In SIGIR, 2006. [23] C. Van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, second version, 1979. [24] B. V´elez, R. Weiss, M. A. Sheldon, and D. K. Gifford.",
                "Fast and Effective Query Refinement.",
                "In SIGIR, 1997. [25] J. Xu and B. Croft.",
                "Query Expansion using Local and Global Document Analysis.",
                "In SIGIR, 1996. [26] J. Xu and B. Croft.",
                "Corpus-based Stemming using Cooccurrence of Word Variants.",
                "ACM TOIS, 16 (1):61-81, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Experimentamos con un \"modelo de lenguaje\" unigram y un \"modelo de lenguaje\" de BigRam.",
                "Con el \"modelo de idioma\" de Unigram, podemos reducir el costo de la derivación en un 26.7% (de 408/408 a 300/408) y levantar la mejora general de DCG de 2.8% a 3.4%.",
                "Para consultas cortas también, observamos tanto la mejora de DCG como la reducción de costos con el \"modelo de idioma\" unigram.",
                "Las ventajas de la expansión de palabras predictivas con un \"modelo de idioma\" se impulsan aún más con un mejor \"modelo de idioma\" de Bigram.",
                "Para consultas cortas, el \"modelo de idioma\" de BigRam mejora la ganancia de DCG del 4.4% al 4.7%, y reduce el costo de 272/272 a 150/272.",
                "Para consultas largas, el \"modelo de idioma\" de BigRam mejora la ganancia de DCG de 1.1% a 2.5%, y reduce el costo de edad de 136/136 a 100/136.",
                "Observamos que el \"modelo de idioma\" Bigram ofrece un ascensor más grande para consultas largas.",
                "Esto se debe a que la incertidumbre en consultas largas es más grande y se necesita un \"modelo de idioma\" más poderoso.",
                "Presumimos que un \"modelo de idioma\" de Trigram daría un mayor ascensor para consultas largas y dejaría esto para futuras investigaciones.",
                "Para consultas cortas, la ganancia DCG superior es del 6.3% para el manejo perfecto de la pluralización, nuestra ganancia actual es del 4.7% con un \"modelo de idioma\" BigRam.",
                "Para consultas largas, la ganancia de DCG es del 4,6% para el manejo perfecto de la pluralización, nuestra ganancia actual es del 2.5% con un \"modelo de idioma\" BigRam.",
                "Podemos obtener un beneficio adicional con un \"modelo de idioma\" más poderoso para consultas largas.",
                "Para el \"modelo de idioma\" de BigRam, durante las consultas de STEMMed 250/408, la mejora de DCG es 6.1%.",
                "El \"modelo de idioma\" debe describir la aparición de la cadena en la web.",
                "Para probar la hipótesis de que podemos aprender probabilidades de transformación confiables del registro de consultas, capacitamos un \"modelo de lenguaje\" de las mismas consultas de la misma consulta Top 25m que se usa para aprender segmentación, y lo usamos para predicción."
            ],
            "translated_text": "",
            "candidates": [
                "modelo",
                "modelo de lenguaje",
                "modelo de lenguaje",
                "modelo",
                "modelo de idioma",
                "modelo",
                "modelo de idioma",
                "modelo",
                "modelo de idioma",
                "modelo de idioma",
                "modelo",
                "modelo de idioma",
                "modelo",
                "modelo de idioma",
                "modelo",
                "modelo de idioma",
                "modelo",
                "modelo de idioma",
                "modelo",
                "modelo de idioma",
                "modelo",
                "modelo de idioma",
                "modelo",
                "modelo de idioma",
                "modelo",
                "modelo de idioma",
                "modelo",
                "modelo de idioma",
                "modelo",
                "modelo de idioma",
                "modelo",
                "modelo de lenguaje"
            ],
            "error": []
        }
    }
}