{
    "id": "H-35",
    "original_text": "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval. In the task, a model is automatically created with some training data and then is utilized for ranking of documents. The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain). Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data. Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures. For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs. To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures. Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions. We prove that the training process of AdaRank is exactly that of enhancing the performance measure used. Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1. INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning. When applied to document retrieval, learning to rank becomes a task as follows. In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans. In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model. In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15]. Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized. Several methods for learning to rank have been developed and applied to document retrieval. For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM. Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost. All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures. For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs. In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval. Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank. AdaRank utilizes a linear combination of weak rankers as its model. In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker. We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures. A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process. AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking. Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov. Tuning ranking models using certain training data and a performance measure is a common practice in IR [1]. As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder. From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning. Recently, direct optimization of performance measures in learning has become a hot research topic. Several methods for classification [17] and ranking [5, 19] have been proposed. AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach. The rest of the paper is organized as follows. After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3. Experimental results and discussions are given in Section 4. Section 5 concludes this paper and gives future work. 2. RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query. It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1]. For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune. As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue. Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained. For example, Joachims [16] applies Ranking SVM to document retrieval. He utilizes click-through data to deduce training data for the model creation. Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR. Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents. Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval. The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work. They are learning to rank, boosting, and direct optimization of performance measures. Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores. Several approaches have been proposed to tackle the problem. One major approach to learning to rank is that of transforming it into binary classification on instance pairs. This pair-wise approach fits well with information retrieval and thus is widely used in IR. Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3]. For other approaches to learning to rank, refer to [2, 11, 31]. In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked). Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16]. In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures. Boosting is a general technique for improving the accuracies of machine learning algorithms. The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted. Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction). Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26]. Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8]. In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26]. Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR. Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning. For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification. Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15]. Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning. AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach. AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3. OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval. In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores. The relevance scores are calculated with a ranking function (model). In learning (training), a number of queries and their corresponding retrieved documents are given. Furthermore, the relevance levels of the documents with respect to the queries are also provided. The relevance levels are represented as ranks (i.e., categories in a total order). The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function. Ideally the loss function is defined on the basis of the performance measure used in testing. Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks. There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship. In training, a set of queries Q = {q1, q2, · · · , qm} is given. Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j. A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi). Thus, the training set can be represented as S = {(qi, di, yi)}m i=1. The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores. Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself. We use π( j) to denote the position of item j (i.e., di j). The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries. Table 1: Notations and explanations. Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers. In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function. By query based measure, we mean a measure defined over a ranking list of documents with respect to a query. These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15]. We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures. The first argument of E is the permutation π created using the ranking function f on di. The second argument is the list of ranks yi given by humans. E measures the agreement between π and yi. Table 1 gives a summary of notations described above. Next, as examples of performance measures, we present the definitions of MAP and NDCG. Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j. Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures. The algorithm is referred to as AdaRank and is shown in Figure 1. AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters. AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T). Finally, it outputs a ranking model f by linearly combining the weak rankers. At each round, AdaRank maintains a distribution of weights over the queries in the training data. We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m. For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} . End For Output ranking model: f(x) = fT (x). Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i). Initially, AdaRank sets equal weights to the queries. At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far. As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries. At each round, a weak ranker ht is constructed based on training data with weight distribution Pt. The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi). Several methods for weak ranker construction can be considered. For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt. In this paper, we use single features as weak rankers, as will be explained in Section 3.6. Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker. Intuitively, αt measures the importance of ht. A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs). In contrast, AdaRank tries to optimize a loss function based on queries. Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures. The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1]. We next explain why this is the case. Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions. This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle. We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ . We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x). Several ways of computing coefficients αt and weak rankers ht may be considered. Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1. It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1. T 1. The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix. The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method. More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above. In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet. First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1]. Notice that the major IR measures meet this requirement. In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16]. Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms. The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data. The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8]. Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods. Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs. As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed. In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval. The existing methods cannot focus on the training on the tops, as indicated in [4]. Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem. In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4]. AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm. In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost. First, the types of instances are different. AdaRank makes use of queries and their corresponding document lists as instances. The labels in training data are lists of ranks (relevance levels). AdaBoost makes use of feature vectors as instances. The labels in training data are simply +1 and −1. Second, the performance measures are different. In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query. In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25]. Third, the ways of updating weights are also different. In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner. In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1. Note that AdaBoost can also adopt the weight updating method used in AdaRank. For AdaBoost they are equivalent (cf., [12] page 305). However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments. In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi). Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features. Note that features which are not selected in the training phase will have a weight of zero. 4. EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov. Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets. C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods. Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ). For AdaRank, the parameter T was determined automatically during each experiment. Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined). As the measure E, MAP and NDCG@5 were utilized. The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank. The OHSUMED dataset consists of 348,566 documents and 106 queries. There are in total 16,140 query-document pairs upon which relevance judgments are made. The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant). The data have been used in many experiments in IR, for example [4, 29]. As features, we adopted those used in document retrieval [4]. Table 2 shows the features. For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features. BM25 score itself is also a feature. Stop words were removed and stemming was conducted in the data. We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments. We tuned the parameters for BM25 during one of the trials and applied them to the other trials. The results reported in Figure 2 are those averaged over four trials. In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets. Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant. From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures. We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP. The results indicate that all the improvements are statistically significant (p-value < 0.05). We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5. The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank. WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300). Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query). Following the practice in [28], the queries that have less than 10 relevant documents were discarded. Table 3 shows the statistics on the two datasets. In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking. We also conducted 4-fold cross-validation experiments. The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively. From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP. We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP. The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05). However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval. The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002. There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset. Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data. The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used. The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant. The number of relevant pages vary from query to query (from 1 to 86). We extracted 14 features from each query-document pair. Table 4 gives a list of the features. They are the outputs of some well-known algorithms (systems). These features are different from those in Table 2, because the task is different. Again, we conducted 4-fold cross-validation experiments. The results averaged over four trials are reported in Figure 5. From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures. We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost. Some of the improvements are not statistically significant. This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples. First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost. Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data. The results averaged over four trials in the 4-fold cross validation are shown in Figure 6. We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant. From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important. This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively. We also made statistics on the number of document pairs per query in the training data (for trial 1). The queries are clustered into different groups based on the the number of their associated document pairs. Figure 7 shows the distribution of the query groups. In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999. We can see that the numbers of document pairs really vary from query to query. Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group. The results are reported in Figure 8. We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost. Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k). The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs. For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5. We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training. Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5. The experiment was conducted for each trial. Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively. We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5. The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training. Finally, we tried to verify the correctness of Theorem 1. That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation. From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak. The result agrees well with Theorem 1. 5. CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank. In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures. It employs a boosting technique in ranking model learning. AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking. Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank. Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6. ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7. REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q. Le. Learning to rank with nonsmooth cost functions. In Advances in Neural Information Processing Systems 18, pages 395-402. MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. Hon. Adapting ranking SVM to document retrieval. In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang. Subset ranking using regression. In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu. Overview of the TREC 2003 web track. In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold. Boosting methods for regression. Mach. Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer. An efficient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. J. Comput. Syst. Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: A statistical view of boosting. The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram. Learning rankings via convex hull separation. In Advances in Neural Information Processing Systems 18, pages 395-402. MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of Statistical Learning. Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer. Large Margin rank boundaries for ordinal regression. MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam. Ohsumed: an interactive retrieval evaluation and new large test collection for research. In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen. IR evaluation methods for retrieving highly relevant documents. In SIGIR 23, pages 41-48, 2000. [16] T. Joachims. Optimizing search engines using clickthrough data. In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims. A support vector method for multivariate performance measures. In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum. Direct maximization of rank-based metrics for information retrieval. Technical report, CIIR, 2005. [20] R. Nallapati. Discriminative models for information retrieval. In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma. A study of relevance propagation for web search. In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A. Hull. The TREC-9 filtering track final report. In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y. Singer. Improved boosting algorithms using confidence-rated predictions. Mach. Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y. Ma. Microsoft Research Asia at web track and terabyte track of TREC 2004. In TREC, 2004. [28] A. Trotman. Learning to rank. Inf. Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang. Cost-sensitive learning of SVM for ranking. In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J. Zeng, Y. Yu, and Z. Chen. Exploiting the hierarchical structure for link analysis. In SIGIR 28, pages 186-193, 2005. [31] H. Yu. SVM selective sampling for ranking with application to data retrieval. In SIGKDD 11, pages 354-363, 2005. APPENDIX Here we give the proof of Theorem 1. P. Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)). According to the definition of αt, we know that eαt = φ(t) 1−φ(t) . ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}. Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2.",
    "original_translation": "Adarank: un algoritmo de impulso para la recuperación de información Jun Xu Microsoft Research Asia No. 49 Zhichun Road, Haidian Distints Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distint Distints Beijing, China 100080 Hangli@microsoft.com Resumen En este documento Nos dirigimos el problema de aprender a clasificar para clasificarRecuperación de documentos. En la tarea, un modelo se crea automáticamente con algunos datos de capacitación y luego se utiliza para la clasificación de documentos. La bondad de un modelo generalmente se evalúa con medidas de rendimiento como MAP (precisión promedio media) y NDCG (ganancia acumulativa con descuento normalizada). Idealmente, un algoritmo de aprendizaje entrenaría un modelo de clasificación que podría optimizar directamente las medidas de rendimiento con respecto a los datos de capacitación. Sin embargo, los métodos existentes solo pueden entrenar modelos de clasificación minimizando las funciones de pérdida relacionadas con las medidas de rendimiento. Por ejemplo, clasificación de modelos de clasificación SVM y RankBoost Train minimizando los errores de clasificación en los pares de instancias. Para lidiar con el problema, proponemos un nuevo algoritmo de aprendizaje dentro del marco de impulso, que puede minimizar una función de pérdida directamente definida en las medidas de rendimiento. Nuestro algoritmo, denominado Adarank, construye repetidamente a los rango débil sobre la base de los datos de entrenamiento re-ponderados y finalmente combina linealmente a los rankers débiles para hacer predicciones de clasificación. Probamos que el proceso de entrenamiento de Adarank es exactamente el de mejorar la medida de rendimiento utilizada. Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que Adarank supera significativamente los métodos de referencia de BM25, clasificación de SVM y RankBoost. Categorías y descriptores de sujetos H.3.3 [Búsqueda y recuperación de información]: modelos de recuperación Algoritmos de términos generales, experimentación, teoría 1. La introducción recientemente el aprendizaje para clasificar ha ganado una atención cada vez mayor tanto en los campos de recuperación de información como en el aprendizaje automático. Cuando se aplica a la recuperación de documentos, el aprendizaje para clasificar se convierte en una tarea de la siguiente manera. En la capacitación, se construye un modelo de clasificación con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia dados por los humanos. En la clasificación, dada una nueva consulta, los documentos recuperados correspondientes se clasifican utilizando el modelo de clasificación capacitado. En la recuperación de documentos, los resultados de clasificación generalmente se evalúan en términos de medidas de rendimiento como MAP (precisión promedio media) [1] y NDCG (ganancia acumulativa con descuento normalizada) [15]. Idealmente, se crea la función de clasificación para que se maximice la precisión de la clasificación en términos de una de las medidas con respecto a los datos de entrenamiento. Varios métodos para aprender a clasificar se han desarrollado y aplicado para la recuperación de documentos. Por ejemplo, Herbrich et al.[13] propone un algoritmo de aprendizaje para la clasificación sobre la base de las máquinas de vectores de soporte, llamado Ranking SVM. Freund et al.[8] adopta un enfoque similar y realiza el aprendizaje utilizando el impulso, denominado RankBoost. Todos los métodos existentes utilizados para la recuperación de documentos [2, 3, 8, 13, 16, 20] están diseñados para optimizar las funciones de pérdida relacionadas con las medidas de rendimiento IR, no las funciones de pérdida directamente en función de las medidas. Por ejemplo, clasificación de modelos de clasificación SVM y RankBoost Train minimizando los errores de clasificación en los pares de instancias. En este documento, nuestro objetivo es desarrollar un nuevo algoritmo de aprendizaje que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos. Inspirados en el trabajo de Adaboost para la clasificación [9], proponemos desarrollar un algoritmo de impulso para la recuperación de información, denominado Adarank. Adarank utiliza una combinación lineal de rankers débiles como su modelo. En el aprendizaje, repite el proceso de volver a alojar la muestra de entrenamiento, crear un ranker débil y calcular un peso para el ranker. Mostramos que el algoritmo Adarank puede optimizar iterativamente una función de pérdida exponencial basada en cualquiera de las medidas de rendimiento IR. Se proporciona un límite inferior del rendimiento en los datos de capacitación, lo que indica que la precisión de la clasificación en términos de la medida de rendimiento puede mejorarse continuamente durante el proceso de capacitación. Adarank ofrece varias ventajas: facilidad en la implementación, solidez teórica, eficiencia en la capacitación y alta precisión en la clasificación. Los resultados experimentales indican que Adarank puede superar los métodos de referencia de BM25, clasificar SVM y RankBoost, en cuatro conjuntos de datos de referencia, incluidos Ohsumed, WSJ, AP y .gov. Los modelos de clasificación de ajuste utilizando ciertos datos de entrenamiento y una medida de rendimiento es una práctica común en IR [1]. A medida que el número de características en el modelo de clasificación se hace mayor y la cantidad de datos de entrenamiento se hace más grande, la afinación se vuelve más difícil. Desde el punto de vista de IR, Adarank se puede ver como un método de aprendizaje automático para el ajuste del modelo de clasificación. Recientemente, la optimización directa de las medidas de rendimiento en el aprendizaje se ha convertido en un tema de investigación candente. Se han propuesto varios métodos para la clasificación [17] y la clasificación [5, 19]. Adarank se puede ver como un método de aprendizaje automático para la optimización directa de las medidas de rendimiento, basado en un enfoque diferente. El resto del documento está organizado de la siguiente manera. Después de un resumen del trabajo relacionado en la Sección 2, describimos el algoritmo Adarank propuesto en detalles en la Sección 3. Los resultados experimentales y las discusiones se dan en la Sección 4. La Sección 5 concluye este documento y ofrece trabajo futuro.2. Trabajo relacionado 2.1 Recuperación de información El problema clave para la recuperación de documentos es la clasificación, específicamente, cómo crear el modelo de clasificación (función) que puede ordenar documentos en función de su relevancia para la consulta dada. Es una práctica común en IR ajustar los parámetros de un modelo de clasificación utilizando algunos datos etiquetados y una medida de rendimiento [1]. Por ejemplo, los métodos de vanguardia de BM25 [24] y LMIR (modelos de lenguaje para la recuperación de información) [18, 22] tienen parámetros que sintonizar. A medida que los modelos de clasificación se vuelven más sofisticados (se utilizan más características) y se están disponibles más datos etiquetados, cómo sintonizar o entrenar modelos de clasificación resulta ser un problema desafiante. Recientemente se han aplicado métodos de aprendizaje para clasificar a la construcción del modelo de clasificación y se han obtenido algunos resultados prometedores. Por ejemplo, Joachims [16] aplica SVM de clasificación para documentar la recuperación. Utiliza datos de clic para deducir datos de capacitación para la creación del modelo. Cao et al.[4] Adapte la clasificación de SVM para documentar la recuperación modificando la función de pérdida de bisagra para cumplir mejor los requisitos de IR. Específicamente, introducen una función de pérdida de bisagra que penaliza fuertemente los errores en la parte superior de las listas de clasificación y los errores de consultas con menos documentos recuperados. Burges et al.[3] Emplea la entropía relativa como función de pérdida y descenso de gradiente como un algoritmo para capacitar a un modelo de red neuronal para clasificar en la recuperación de documentos. El método se conoce como RankNet.2.2 Aprendizaje automático Hay tres temas en el aprendizaje automático que están relacionados con nuestro trabajo actual. Están aprendiendo a clasificar, impulsar y optimización directa de las medidas de rendimiento. Aprender a clasificarse es crear automáticamente una función de clasificación que asigne puntajes a las instancias y luego clasificar las instancias utilizando los puntajes. Se han propuesto varios enfoques para abordar el problema. Un enfoque importante para aprender a clasificar es el de transformarlo en clasificación binaria en pares de instancias. Este enfoque pareal se ajusta bien a la recuperación de información y, por lo tanto, se usa ampliamente en IR. Los métodos típicos del enfoque incluyen clasificar SVM [13], RankBoost [8] y RankNet [3]. Para otros enfoques para aprender a clasificarse, consulte [2, 11, 31]. En el enfoque de pareja para la clasificación, la tarea de aprendizaje se formaliza como un problema de clasificar los pares de instancias en dos categorías (clasificada correctamente y clasificada incorrectamente). En realidad, se sabe que reducir los errores de clasificación en los pares de instancias es equivalente a maximizar un límite inferior del mapa [16]. En ese sentido, los métodos existentes para clasificar SVM, RankBoost y RankNet solo pueden minimizar las funciones de pérdida que están libremente relacionadas con las medidas de rendimiento IR. El impulso es una técnica general para mejorar las precisiones de los algoritmos de aprendizaje automático. La idea básica de aumentar es construir repetidamente a los alumnos débiles al volver a alojar los datos de capacitación y formar un conjunto de estudiantes débiles de modo que se aumente el rendimiento total del conjunto. Freund y Schapire han propuesto el primer algoritmo de impulso bien conocido llamado Adaboost (impulso adaptativo) [9], que está diseñado para la clasificación binaria (predicción 0-1). Más tarde, Schapire & Singer ha introducido una versión generalizada de Adaboost en la que los alumnos débiles pueden dar puntajes de confianza en sus predicciones en lugar de tomar decisiones 0-1 [26]. Se han hecho extensiones para tratar los problemas de clasificación de múltiples clases [10, 26], regresión [7] y clasificación [8]. De hecho, Adaboost es un algoritmo que construye ingeniosamente un modelo lineal minimizando la función de pérdida exponencial con respecto a los datos de entrenamiento [26]. Nuestro trabajo en este documento se puede ver como un método de impulso desarrollado para la clasificación, particularmente para la clasificación en IR. Recientemente, varios autores han propuesto realizar una optimización directa de medidas de rendimiento multivariadas en el aprendizaje. Por ejemplo, Joachims [17] presenta un método SVM para optimizar directamente las medidas de rendimiento multivariadas no lineales como la medida F1 para la clasificación. Cossock y Zhang [5] encuentran una manera de optimizar aproximadamente la medida de rendimiento de clasificación DCG [15]. Metzler et al.[19] también propone un método para maximizar directamente las métricas basadas en rango para la clasificación sobre la base del aprendizaje múltiple. Adarank también es uno que intenta optimizar directamente las medidas de rendimiento multivariadas, pero se basa en un enfoque diferente. Adarank es único en el sentido de que emplea una función de pérdida exponencial basada en medidas de rendimiento de IR y una técnica de impulso.3. Nuestro método: ADARANK 3.1 Marco general, primero describimos el marco general de aprender a clasificarse para la recuperación de documentos. En la recuperación (prueba), dada una consulta, el sistema devuelve una lista de clasificación de documentos en orden descendente de los puntajes de relevancia. Los puntajes de relevancia se calculan con una función de clasificación (modelo). En el aprendizaje (capacitación), se dan una serie de consultas y sus documentos recuperados correspondientes. Además, también se proporcionan los niveles de relevancia de los documentos con respecto a las consultas. Los niveles de relevancia se representan como rangos (es decir, categorías en un orden total). El objetivo del aprendizaje es construir una función de clasificación que logre los mejores resultados en la clasificación de los datos de entrenamiento en el sentido de minimización de una función de pérdida. Idealmente, la función de pérdida se define sobre la base de la medida de rendimiento utilizada en las pruebas. Supongamos que y = {r1, r2, · · ·, r} es un conjunto de rangos, donde denota el número de rangos. Existe un orden total entre los rangos R R −1 · · · R1, donde denota una relación de preferencia. En el entrenamiento, se da un conjunto de consultas q = {Q1, Q2, · · ·, QM}. Cada consulta Qi está asociada con una lista de documentos recuperados di = {Di1, Di2, · · ·, di, n (qi)} y una lista de etiquetas yi = {yi1, yi2, · · ·, yi, n (Qi)}, donde n (qi) denota los tamaños de las listas di e yi, dij denota el documento jth en di, y yij ∈ Y denota el rango de documento di j. Se crea un vector de características xij = ψ (qi, di j) ∈ X a partir de cada par de documentos de consulta (qi, di j), i = 1, 2, · · ·, m;j = 1, 2, · · ·, n (qi). Por lo tanto, el conjunto de entrenamiento se puede representar como S = {(qi, di, yi)} m i = 1. El objetivo del aprendizaje es crear una función de clasificación F: x →, de modo que para cada consulta a los elementos en su lista de documentos correspondientes se les pueda asignar puntajes de relevancia utilizando la función y luego clasificarse de acuerdo con los puntajes. Específicamente, creamos una permutación de enteros π (qi, di, f) para consulta qi, la lista correspondiente de documentos di y la función de clasificación f.Deje que di = {di1, di2, · · ·, di, n (qi)} se identifique por la lista de enteros {1, 2, · · ·, n (qi)}, luego permutación π (qi, di, f) se define como una biyección de {1, 2, · · ·, n (qi)} a sí misma. Usamos π (j) para denotar la posición del ítem j (es decir, di j). El proceso de aprendizaje resulta ser el de minimizar la función de pérdida que representa el desacuerdo entre la permutación π (Qi, Di, F) y la lista de rangos Yi, para todas las consultas. Tabla 1: Notaciones y explicaciones. Explicaciones de notaciones qi ∈ Q ith consulta di = {di1, di2, · · ·, di, n (qi)} Lista de documentos para qi yi j ∈ {r1, r2, · · ·, r} rango de di j w.r.t.qi yi = {yi1, yi2, · · ·, yi, n (qi)} Lista de rangos para qi s = {(qi, di, yi)} m i = 1 Conjunto de entrenamiento xij = ψ (qi, dij) ∈ XVector de características para (qi, di j) f (xij) ∈ Modelo de clasificación π (qi, di, f) permutación para qi, di y f ht (xi j) ∈ Tth Ranker débil e (π (qi, di, f), yi) ∈ [−1, +1] función de medida de rendimiento En el documento, definimos el modelo de rango como una combinación lineal de rankers débiles: f (x) = t t = 1 αTHT (x), donde ht (x) es un ranker débil, αT es su peso y t es el número de rankers débiles. En la recuperación de la información, las medidas de rendimiento basadas en consultas se utilizan para evaluar la bondad de una función de clasificación. Por medida basada en la consulta, nos referimos a una medida definida en una lista de clasificación de documentos con respecto a una consulta. Estas medidas incluyen MAP, NDCG, MRR (rango recíproco medio), WTA (los ganadores toman todo) y Precision@N [1, 15]. Utilizamos una función general E (π (Qi, Di, F), yi) ∈ [−1, +1] para representar las medidas de rendimiento. El primer argumento de E es la permutación π creada usando la función de clasificación F en DI. El segundo argumento es la lista de rangos yi dada por humanos. E mide el acuerdo entre π y yi. La Tabla 1 proporciona un resumen de las notaciones descritas anteriormente. A continuación, como ejemplos de medidas de rendimiento, presentamos las definiciones de MAP y NDCG. Dada una consulta Qi, la lista correspondiente de rangos yi, y una permutación πi en di, la precisión promedio para qi se define como: avgpi = n (qi) j = 1 pi (j) · yij n (qi) j = 1 yij, (1) donde YIJ toma 1 y 0 como valores, que representan ser relevantes o irrelevantes y Pi (j) se define como precisión en la posición de dij: pi (j) = k: πi (k) ≤πi (j)yik πi (j), (2) donde πi (j) denota la posición de di j. Dada una consulta Qi, la lista de rangos yi y una permutación πi en di, ndcg en la posición m para qi se define como: ni = ni · j: πi (j) ≤m 2yi j - 1 log (1 + πi (j)), (3) donde Yij asume rangos como valores y Ni es una constante de normalización.Ni se elige para que una puntuación de clasificación π π i s ndcg en la posición m sea 1. 3.2 algoritmo inspirado en el algoritmo AdaBoost para la clasificación, hemos ideado un algoritmo novedoso que puede optimizar una función de pérdida basada en las medidas de rendimiento IR. El algoritmo se conoce como Adarank y se muestra en la Figura 1. Adarank toma un conjunto de entrenamiento S = {(qi, di, yi)} m i = 1 como entrada y toma la función de medida de rendimiento e y el número de iteraciones t como parámetros. Adarank corre t rondas y en cada ronda crea un ranker ht débil (t = 1, · · ·, t). Finalmente, genera un modelo de clasificación F combinando linealmente a los rankers débiles. En cada ronda, Adarank mantiene una distribución de pesos sobre las consultas en los datos de entrenamiento. Denotamos la distribución de la entrada de pesos: s = {(qi, di, yi)} m i = 1, y los parámetros e y t inicializan p1 (i) = 1/m. Para t = 1, · · ·, t • Crear ranker ht débil con distribución ponderada PT en datos de entrenamiento s.• Elija αT αT = 1 2 · ln m i = 1 pt (i) {1 + e (π (qi, di, ht), yi)} m i = 1 pt (i) {1 - e (π (qi, di, di, ht), yi)}.• Crear ft ft (x) = t k = 1 αKHK (x).• Actualizar pt+1 pt+1 (i) = exp {−e (π (qi, di, ft), yi)} m j = 1 exp {−e (π (qj, dj, ft), yj)}. Fin para el modelo de clasificación de salida: F (x) = ft (x). Figura 1: El algoritmo Adarank.en la ronda T como PT y el peso en la I -ésima consulta de entrenamiento Qi en la ronda T como PT (i). Inicialmente, Adarank establece pesos igual a las consultas. En cada ronda, aumenta los pesos de esas consultas que no están bien clasificadas por FT, el modelo creado hasta ahora. Como resultado, el aprendizaje en la próxima ronda se centrará en la creación de un ranker débil que puede trabajar en la clasificación de esas consultas difíciles. En cada ronda, se construye un HT de Ranker débil en función de datos de entrenamiento con distribución de peso PT. La bondad de un ranker débil se mide por la medida de rendimiento e ponderada por Pt: M I = 1 Pt (I) E (π (Qi, Di, Ht), Yi). Se pueden considerar varios métodos para la construcción del rango débil. Por ejemplo, se puede crear un ranker débil utilizando un subconjunto de consultas (junto con su lista de documentos y lista de etiquetas) muestreadas de acuerdo con la distribución PT. En este artículo, utilizamos características individuales como rankers débiles, como se explicará en la Sección 3.6. Una vez que se construye un Ranker HT débil, Adarank elige un peso αT> 0 para el ranker débil. Intuitivamente, αT mide la importancia de HT. Se crea un modelo de clasificación FT en cada ronda combinando linealmente los rankers débiles construidos hasta ahora H1, · · ·, ht con pesos α1, · · ·, αT.FT se usa luego para actualizar la distribución PT+1.3.3 Análisis teórico Los algoritmos de aprendizaje existentes para el intento de clasificación de minimizar una función de pérdida basada en pares de instancias (pares de documentos). En contraste, Adarank intenta optimizar una función de pérdida basada en consultas. Además, la función de pérdida en Adarank se define sobre la base de medidas generales de rendimiento IR. Las medidas pueden ser MAP, NDCG, WTA, MRR o cualquier otra medida cuyo rango esté dentro de [−1, +1]. Luego explicamos por qué este es el caso. Idealmente, queremos maximizar la precisión de la clasificación en términos de una medida de rendimiento en los datos de entrenamiento: max f∈F m i = 1 e (π (qi, di, f), yi), (4) donde f es el conjunto de posiblesFunciones de clasificación. Esto es equivalente a minimizar la pérdida en los datos de entrenamiento min f∈F m i = 1 (1 - E (π (qi, di, f), yi)).(5) Es difícil optimizar directamente la pérdida, porque E es una función no continua y, por lo tanto, puede ser difícil de manejar. En su lugar, intentamos minimizar un límite superior de la pérdida en (5) min f∈F m i = 1 exp {−e (π (qi, di, f), yi)}, (6) porque e - x ≥ 1 −X se sostiene para cualquier x ∈. Consideramos el uso de una combinación lineal de rankers débiles como nuestro modelo de clasificación: F (x) = t t = 1 αTHT (x).(7) La minimización en (6) luego resulta ser min ht∈H, αT∈ + L (ht, αT) = m i = 1 exp {−e (π (qi, di, ft - 1 + αTHT),,yi)}, (8) donde h es el conjunto de posibles rankers débiles, αT es un peso positivo y (ft - 1 + αTHT) (x) = ft - 1 (x) + αTHT (x). Se pueden considerar varias formas de calcular los coeficientes αT y los rankers débiles HT. Siguiendo la idea de Adaboost, en Adarank adoptamos el enfoque del modelado aditivo en escena hacia adelante [12] y obtenemos el algoritmo en la Figura 1. Se puede demostrar que existe un límite inferior en la precisión de clasificación para Adarank en los datos de entrenamiento, como se presenta en el Teorema 1. T 1. El siguiente límite se mantiene en la precisión de clasificación del algoritmo Adarank en los datos de entrenamiento: 1 m m i = 1 e (π (qi, di, ft), yi) ≥ 1 - t t = 1 e - quin min 1 - ϕ (t)2, donde ϕ (t) = m i = 1 pt (i) e (π (qi, di, ht), yi), Δt min = mini = 1, ···, m Δt i y Δt i = e (π (qi, di, ft - 1 + αTht), yi) - e (π (qi, di, ft - 1) −αTE (π (qi, di, ht), yi), para todos i = =1, 2, · · ·, my t = 1, 2, · · ·, T. Una prueba del teorema se puede encontrar en el apéndice. El teorema implica que la precisión de la clasificación en términos de la medida de rendimiento puede mejorarse continuamente, siempre que E - Δt min 1 - ϕ (t) 2 <1 se mantenga.3.4 Ventajas Adarank es un método simple pero poderoso. Más importante aún, es un método que puede justificarse desde el punto de vista teórico, como se discutió anteriormente. Además, Adarank tiene varias otras ventajas en comparación con los métodos de aprendizaje existentes para clasificar, como clasificar SVM, RankBoost y RankNet. Primero, Adarank puede incorporar cualquier medida de rendimiento, siempre que la medida esté basada en la consulta y en el rango de [−1, +1]. Observe que las principales medidas IR cumplen con este requisito. Por el contrario, los métodos existentes solo minimizan las funciones de pérdida que están libremente relacionadas con las medidas IR [16]. En segundo lugar, el proceso de aprendizaje de Adarank es más eficiente que el de los algoritmos de aprendizaje existentes. La complejidad del tiempo de Adarank es de orden o ((k+t) · m · n log n), donde k denota el número de características, el número de rondas, el número de consultas en los datos de entrenamiento, y n es elNúmero máximo de documentos para consultas en datos de capacitación. La complejidad del tiempo de RankBoost, por ejemplo, es de orden O (t · m · n2) [8]. En tercer lugar, Adarank emplea un marco más razonable para realizar la tarea de clasificación que los métodos existentes. Específicamente en Adarank, las instancias corresponden a consultas, mientras que en los métodos existentes las instancias corresponden a pares de documentos. Como resultado, Adarank no tiene las siguientes deficiencias que afectan los métodos existentes.(a) Los métodos existentes deben hacer una suposición fuerte de que los pares de documentos de la misma consulta se distribuyen de forma independiente. En realidad, este claramente no es el caso y este problema no existe para Adarank.(b) La clasificación de los documentos más relevantes en la parte superior de las listas de documentos es crucial para la recuperación de documentos. Los métodos existentes no pueden centrarse en la capacitación en la parte superior, como se indica en [4]. Se han propuesto varios métodos para rectificar el problema (por ejemplo, [4]), sin embargo, no parecen resolver fundamentalmente el problema. Por el contrario, Adarank puede centrarse naturalmente en el entrenamiento en la parte superior de las listas de documentos, porque las medidas de rendimiento utilizadas a favor de las clasificaciones para los cuales los documentos relevantes están en la parte superior.(c) En los métodos existentes, los números de pares de documentos varían de consulta a consulta, lo que resulta en la creación de modelos sesgados hacia consultas con más pares de documentos, como se señala en [4]. Adarank no tiene este inconveniente, porque trata consultas en lugar de pares de documentos como unidades básicas en el aprendizaje.3.5 Las diferencias de Adaboost Adarank es un algoritmo de impulso. En ese sentido, es similar a Adaboost, pero también tiene varias diferencias sorprendentes con respecto a Adaboost. Primero, los tipos de instancias son diferentes. Adarank hace uso de consultas y sus listas de documentos correspondientes como instancias. Las etiquetas en los datos de capacitación son listas de rangos (niveles de relevancia). Adaboost utiliza los vectores de características como instancias. Las etiquetas en los datos de entrenamiento son simplemente +1 y −1. En segundo lugar, las medidas de rendimiento son diferentes. En Adarank, la medida de rendimiento es una medida genérica, definida en la lista de documentos y la lista de rango de una consulta. En ADABOOST, la medida de rendimiento correspondiente es una medida específica para la clasificación binaria, también conocida como margen [25]. Tercero, las formas de actualizar los pesos también son diferentes. En Adaboost, la distribución de pesos en las instancias de entrenamiento se calcula de acuerdo con la distribución actual y el rendimiento del alumno débil actual. En Adarank, en contraste, se calcula de acuerdo con el rendimiento del modelo de clasificación creado hasta ahora, como se muestra en la Figura 1. Tenga en cuenta que Adaboost también puede adoptar el método de actualización de peso utilizado en Adarank. Para Adaboost son equivalentes (cf., [12] Página 305). Sin embargo, esto no es cierto para Adarank.3.6 Construcción de Ranker débil Consideramos una implementación eficiente para la construcción de rango débil, que también se usa en nuestros experimentos. En la implementación, como Ranker débil, elegimos la característica que tiene el rendimiento ponderado óptimo entre todas las características: max k m i = 1 pt (i) e (π (qi, di, xk), yi). Creando clasificadores débiles de esta manera, el proceso de aprendizaje resulta ser el de seleccionar repetidamente características y combinar linealmente las características seleccionadas. Tenga en cuenta que las características que no se seleccionan en la fase de entrenamiento tendrán un peso de cero.4. Resultados experimentales realizamos experimentos para probar el rendimiento de Adarank utilizando cuatro conjuntos de datos de referencia: Ohsumed, WSJ, AP y .gov. Tabla 2: Características utilizadas en los experimentos en conjuntos de datos OHSUMED, WSJ y AP. C (w, d) representa la frecuencia de la palabra w en el documento d;C representa toda la colección;n denota el número de términos en la consulta;|· |denota la función de tamaño;e ID F (·) denota la frecuencia de documentos inversos.1 wi∈Q d ln (c (wi, d) + 1) 2 wi∈Q d ln (| c | c (wi, c) + 1) 3 wi∈Q d ln (id f (wi)) 4 wi∈Q d ln(c (wi, d) | d | + 1) 5 wi∈Q d ln (c (wi, d) | d | · id f (wi) + 1) 6 wi∈Q d ln (c (wi, d) · ·| C | | d | · c (wi, c) + 1) 7 ln (puntaje BM25) 0.2 0.3 0.4 0.4 0.5 0.6 mapa ndcg@1 ndcg@3 ndcg@5 ndcg@10 bm25 ranking svm rarnkboost adarank.map adarank.ndcgFigura 2: precisiones de clasificación en datos de ohsumed.4.1 Clasificación del experimento Ranking SVM [13, 16] y RankBoost [8] se seleccionaron como líneas de base en los experimentos, porque son los métodos de vanguardia para el aprendizaje de los arte. Además, BM25 [24] se utilizó como línea de base, que representa el método IR de última generación (en realidad utilizamos la herramienta Lemur1). Para Adarank, el parámetro T se determinó automáticamente durante cada experimento. Específicamente, cuando no hay mejora en la precisión de clasificación en términos de la medida de rendimiento, la iteración se detiene (y se determina T). Como se utilizaron la medida E, MAP y NDCG@5. Los resultados para Adarank usando MAP y NDCG@5 como medidas en el entrenamiento se representan como adarank.map y adarank.ndcg, respectivamente.4.2 Experimento con datos de ohsumed En este experimento, utilizamos el conjunto de datos de OHSUMED [14] para probar el rendimiento de Adarank. El conjunto de datos de OHSUMed consta de 348,566 documentos y 106 consultas. Hay en total 16.140 pares de documentos de consulta sobre los cuales se realizan juicios de relevancia. Los juicios de relevancia son D (definitivamente relevantes), P (posiblemente relevantes) o N (no relevantes). Los datos se han utilizado en muchos experimentos en IR, por ejemplo [4, 29]. Como características, adoptamos los utilizados en la recuperación de documentos [4]. La Tabla 2 muestra las características. Por ejemplo, TF (frecuencia de término), IDF (frecuencia de documento inversa), DL (longitud del documento) y combinaciones de ellos se definen como características. La puntuación BM25 en sí también es una característica. Se eliminaron las palabras de parada y se realizó la derecha en los datos. Dividimos al azar consultas en cuatro subconjuntos pares y realizamos experimentos de validación cruzada de 4 veces. Sintonizamos los parámetros para BM25 durante una de las pruebas y los aplicamos a los otros ensayos. Los resultados informados en la Figura 2 son los promediados en cuatro ensayos. En el cálculo del mapa, definimos el rango D como relevante y 1 http://www.lemurproject.com Tabla 3: Estadísticas en conjuntos de datos WSJ y AP. Conjunto de datos # consultas # # documentos recuperados # documentos por consulta ap 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 mapa ndcg@1 ndcg@3 ndcg@5 ndcg@10 bm25 clasificación svm rankoost adarank.map adarank.ndc figuraprecisiones en el conjunto de datos WSJ.Los otros dos rangos son irrelevantes. De la Figura 2, vemos que tanto adarank.map como adarank.ndcg superan a BM25, clasificando SVM y RankBoost en términos de todas las medidas. Realizamos pruebas significativas (prueba t) en las mejoras de Adarank.map sobre BM25, clasificación de SVM y RankBoost en términos de mapa. Los resultados indican que todas las mejoras son estadísticamente significativas (valor p <0.05). También realizamos la prueba t en las mejoras de adarank.ndcg sobre BM25, clasificación de SVM y RankBoost en términos de NDCG@5. Las mejoras también son estadísticamente significativas.4.3 Experimente con los datos de WSJ y AP En este experimento, utilizamos los conjuntos de datos WSJ y AP de la pista de recuperación AD-hoc trec, para probar las actuaciones de Adarank. WSJ contiene 74,520 artículos de Wall Street Journals de 1990 a 1992, y AP contiene 158,240 artículos de Associated Press en 1988 y 1990. 200 consultas se seleccionan de los temas de TREC (No.101 ∼ No.300). Cada consulta tiene una serie de documentos asociados y están etiquetados como relevantes o irrelevantes (para la consulta). Después de la práctica en [28], las consultas que tienen menos de 10 documentos relevantes se descartaron. La Tabla 3 muestra las estadísticas en los dos conjuntos de datos. De la misma manera que en la Sección 4.2, adoptamos las características enumeradas en la Tabla 2 para la clasificación. También realizamos experimentos de valoración cruzada de 4 veces. Los resultados informados en la Figura 3 y 4 son los promediaron en cuatro ensayos en conjuntos de datos WSJ y AP, respectivamente. De la Figura 3 y 4, podemos ver que Adarank.Map y Adarank.ndcg superan a BM25, clasificando SVM y RankBoost en términos de todas las medidas en WSJ y AP. Realizamos pruebas t sobre las mejoras de adarank.map y adarank.ndcg sobre BM25, clasificando SVM y RankBoost en WSJ y AP. Los resultados indican que todas las mejoras en términos de MAP son estadísticamente significativas (valor p <0.05). Sin embargo, solo algunas de las mejoras en términos de NDCG@5 son estadísticamente significativas, aunque en general las mejoras en las puntuaciones NDCG son bastante altas (1-2 puntos).4.4 Experimente con datos .gov En este experimento, utilizamos los datos de TREC .gov para probar el rendimiento de Adarank para la tarea de recuperación web. El corpus es un rastreo del dominio .gov a principios de 2002, y se ha utilizado en la pista web de TREC desde 2002. Hay un total de 0.40 0.45 0.50 0.55 mapa ndcg@1 ndcg@3 ndcg@5 ndcg@10 bm25 ranking svm rankboost adarank.map adarank.ndcg Figura 4: precisiones de clasificación en dataSet AP.0.1 0.2 0.3 0.4 0.5 0.6 0.7 mapa ndcg@1 ndcg@3 ndcg@5 ndcg@10 bm25 ranking svm rankboost adarank.map adarank.ndcg Figura 5: precisiones de clasificación en .gov dataSet. Tabla 4: Características utilizadas en los experimentos en el conjunto de datos .gov.1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 Hostrank [30] 5 Propagación de relevancia [23] (10 características) de 1,053,110 páginas web con 11,164,829 hipervínculos en los datos. Se utilizaron las 50 consultas en la tarea de destilación del tema en la pista web de TREC 2003 [6]. Las verdades de tierra para las consultas son proporcionadas por el Comité TREC con juicio binario: relevante o irrelevante. El número de páginas relevantes varía de consulta a consulta (de 1 a 86). Extrajimos 14 características de cada par de documentos de consulta. La Tabla 4 ofrece una lista de las características. Son las salidas de algunos algoritmos (sistemas) bien conocidos. Estas características son diferentes de las de la Tabla 2, porque la tarea es diferente. Nuevamente, realizamos experimentos de 4 veces de validación cruzada. Los resultados promediados en cuatro ensayos se informan en la Figura 5. A partir de los resultados, podemos ver que Adarank.Map y Adarank.ndcg superan a todas las líneas de base en términos de todas las medidas. Realizamos ttests en las mejoras de adarank.map y adarank.ndcg sobre BM25, clasificación de SVM y RankBoost. Algunas de las mejoras no son estadísticamente significativas. Esto se debe a que solo tenemos 50 consultas utilizadas en los experimentos, y el número de consultas es demasiado pequeña.4.5 Discusiones Investigamos las razones por las que Adarank supera los métodos de referencia, utilizando los resultados del conjunto de datos de ohsumido como ejemplos. Primero, examinamos la razón por la que Adarank tiene actuaciones más altas que en clasificar SVM y RankBoost. Específicamente, com0.58 0.60 0.62 0.64 0.66 0.68 D-N D-P P-N Tipo de pareja de precisión Ranking SVM RankBoost Adarank.map adarank.ndcg Figura 6: Precisión en los pares de documentos de clasificación con un conjunto de datos de suma.0 2 4 6 8 10 12 Número del número de pares de documentos por consulta Figura 7: Distribución de consultas con diferentes pares de pares de documentos en datos de entrenamiento de prueba 1. Apare las tasas de error entre los diferentes pares de rango realizados por la clasificación de SVM, RankBoost, Adarank.MAP, y adarank.ndcg en los datos de prueba. Los resultados promediados en cuatro ensayos en la validación cruzada de 4 veces se muestran en la Figura 6. Usamos D-N para defender los pares entre definitivamente relevantes y no relevantes, D-P los pares entre definitivamente relevantes y parcialmente relevantes, y P-N los pares entre parcialmente relevantes y no relevantes. De la Figura 6, podemos ver que Adarank.Map y Adarank.ndcg cometen menos errores para D-N y D-P, que están relacionados con la parte superior de las clasificaciones y son importantes. Esto se debe a que ADARANK.MAP y ADARANK.NDCG pueden centrarse naturalmente en el entrenamiento en las partes superiores optimizando MAP y NDCG@5, respectivamente. También hicimos estadísticas sobre el número de pares de documentos por consulta en los datos de capacitación (para el juicio 1). Las consultas se agrupan en diferentes grupos en función del número de sus pares de documentos asociados. La Figura 7 muestra la distribución de los grupos de consultas. En la figura, por ejemplo, 0-1K es el grupo de consultas cuyo número de pares de documentos se encuentran entre 0 y 999. Podemos ver que los números de pares de documentos realmente varían de consulta a consulta. A continuación, evaluamos las precisiones de Adarank.Map y RankBoost en términos de mapa para cada uno de los grupos de consultas. Los resultados se informan en la Figura 8. Descubrimos que el mapa promedio de Adarank.map sobre los grupos es dos puntos más alto que RankBoost. Además, es interesante ver que Adarank.Map funciona particularmente mejor que RankBoost para consultas con pequeños números de pares de documentos (por ejemplo, 0-1K, 1K-2K y 2K-3K). Los resultados indican que ADARANK.MAP puede evitar la creación de un modelo sesgado hacia consultas con más pares de documentos. Para adarank.ndcg, se pueden observar resultados similares.0.2 0.3 0.4 0.5 MAP GRUPO DE MAP RANCOAT ADARANK. MAP Figura 8: Diferencias en MAP para diferentes grupos de consultas.0.30 0.31 0.32 0.33 0.34 Prueba 1 Prueba 2 Prueba 3 Prueba 4 mapa adarank.map adarank.ndcg Figura 9: mapa en el conjunto de entrenamiento cuando el modelo está entrenado con map o ndcg@5. Además, realizamos un experimento para ver si Adarank tiene la capacidad de mejorar la precisión de la clasificación en términos de una medida utilizando la medida en el entrenamiento. Específicamente, entrenamos modelos de clasificación con adarank.map y adarank.ndcg y evaluamos sus precisiones en el conjunto de datos de entrenamiento en términos de map y ndcg@5. El experimento se realizó para cada prueba. La Figura 9 y la Figura 10 muestran los resultados en términos de MAP y NDCG@5, respectivamente. Podemos ver que, adarank.map entrenado con mapa funciona mejor en términos de mapa, mientras que adarank.ndcg entrenado con ndcg@5 funciona mejor en términos de ndcg@5. Los resultados indican que Adarank puede mejorar el rendimiento de la clasificación en términos de una medida utilizando la medida en el entrenamiento. Finalmente, tratamos de verificar la corrección del teorema 1. Es decir, la precisión de clasificación en términos de la medida de rendimiento puede mejorarse continuamente, siempre que E - Δt min 1 - ϕ (t) 2 <1 se mantenga. Como ejemplo, la Figura 11 muestra la curva de aprendizaje de Adarank.map en términos de mapa durante la fase de entrenamiento en una prueba de la validación cruzada. De la figura, podemos ver que la precisión de clasificación de Adarank.map mejora constantemente, a medida que avanza el entrenamiento, hasta que llega al pico. El resultado está de acuerdo bien con el teorema 1. 5. Conclusión y trabajo futuro En este documento hemos propuesto un algoritmo novedoso para los modelos de clasificación de aprendizaje en la recuperación de documentos, denominado Adarank. A diferencia de los métodos existentes, Adarank optimiza una función de pérdida que se define directamente en las medidas de rendimiento. Emplea una técnica de impulso en la clasificación del aprendizaje del modelo. Adarank ofrece varias ventajas: facilidad de implementación, solidez teórica, eficiencia en la capacitación y alta precisión en la clasificación. Los resultados experimentales basados en cuatro conjuntos de datos de referencia muestran que Adarank puede superar significativamente los métodos de referencia de BM25, Ranking SVM y RankBoost.0.49 0.50 0.51 0.52 0.53 Prueba 1 Prueba 2 Prueba 3 Prueba 4 NDCG@5 Adarank.map adarank.ndcg Figura 10: NDCG@5 en el conjunto de entrenamiento cuando el modelo está entrenado con MAP o NDCG@5.0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 Mapa Número de rondas Figura 11: Curva de aprendizaje de Adarank. El trabajo futuro incluye un análisis teórico sobre el error de generalización y otras propiedades del algoritmo Adarank, y más evaluaciones empíricas del algoritmo, incluidas las comparaciones con otros algoritmos que pueden optimizar directamente las medidas de rendimiento.6. Agradecimientos Agradecemos a Harry Shum, Wei-Ying MA, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire y Andrew Arnold por sus valiosos comentarios y sugerencias a este documento.7. Referencias [1] R. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison Wesley, mayo de 1999. [2] C. Burges, R. Ragno y Q. Le. Aprender a clasificarse con funciones de costo no suave. En Avances en Sistemas de Procesamiento de Información Neural 18, páginas 395-402. MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender. Aprendiendo a clasificarse usando descenso de gradiente. En ICML 22, páginas 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang y H.-W.Excmo Adaptación de ranking SVM para la recuperación de documentos. En Sigir 29, páginas 186-193, 2006. [5] D. Cossock y T. Zhang. Ranking de subconjunto usando regresión. En Colt, páginas 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson y M. Wu. Descripción general de la pista web TREC 2003. En Trec, páginas 78-92, 2003. [7] N. Duffy y D. Helmbold. Aumento de métodos para la regresión. Mach. Learn., 47 (2-3): 153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire e Y. Cantante. Un algoritmo de impulso eficiente para combinar las preferencias. Journal of Machine Learning Research, 4: 933-969, 2003. [9] Y. Freund y R. E. Schapire. Una generalización teórica de decisión del aprendizaje en línea y una aplicación para aumentar. J. Comput. Syst. Sci., 55 (1): 119-139, 1997. [10] J. Friedman, T. Hastie y R. Tibshirani. Regresión logística aditiva: una visión estadística del impulso. The Annals of Statistics, 28 (2): 337-374, 2000. [11] G. Fung, R. Rosales y B. Krishnapuram. Rankings de aprendizaje a través de la separación de casco convexo. En Avances en Sistemas de Procesamiento de Información Neural 18, páginas 395-402. MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani y J. H. Friedman. Los elementos del aprendizaje estadístico. Springer, agosto de 2001. [13] R. Herbrich, T. Graepel y K. Obermayer. Límites de rango de margen grande para la regresión ordinal. MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone y D. Hickam. OHSUMED: una evaluación de recuperación interactiva y una nueva recopilación de pruebas grandes para la investigación. En Sigir, páginas 192-201, 1994. [15] K. Jarvelin y J. Kekalainen. IR Métodos de evaluación para recuperar documentos altamente relevantes. En Sigir 23, páginas 41-48, 2000. [16] T. Joachims. Optimización de los motores de búsqueda utilizando datos de clics. En Sigkdd 8, páginas 133-142, 2002. [17] T. Joachims. Un método de vector de soporte para medidas de rendimiento multivariadas. En ICML 22, páginas 377-384, 2005. [18] J. Lafferty y C. Zhai. Documentar modelos de lenguaje, modelos de consulta y minimización de riesgos para la recuperación de información. En Sigir 24, páginas 111-119, 2001. [19] D. A. Metzler, W. B. Croft y A. McCallum. Maximización directa de métricas basadas en rango para la recuperación de información. Informe técnico, CIIR, 2005. [20] R. Nallapati. Modelos discriminativos para la recuperación de información. En Sigir 27, páginas 64-71, 2004. [21] L. Page, S. Brin, R. Motwani y T. Winograd. Ranking de citas de PageRank: traer orden a la web. Informe técnico, Proyecto de Tecnologías de la Biblioteca Digital de Stanford, 1998. [22] J. M. Ponte y W. B. Croft. Un enfoque de modelado de idiomas para la recuperación de información. En Sigir 21, páginas 275-281, 1998. [23] T. Qin, T.-Y. Liu, X.-D.Zhang, Z. Chen y W.-Y. Mamá. Un estudio de propagación de relevancia para la búsqueda web. En Sigir 28, páginas 408-415, 2005. [24] S. E. Robertson y D. A. Cáscara. El informe final de la pista de filtrado TREC-9. En Trec, páginas 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett y W. S. Lee. Aumentando el margen: una nueva explicación para la efectividad de los métodos de votación. En ICML 14, páginas 322-330, 1997. [26] R. E. Schapire e Y. Cantante. Algoritmos de impulso mejorados utilizando predicciones con clasificación de confianza. Mach. Learn., 37 (3): 297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. Yan Liu, T. Qin, X. Zheng, J. Zhang,G. Xue y W.-Y. Mamá. Microsoft Research Asia en Web Track y Terabyte Track de TREC 2004. En Trec, 2004. [28] A. Trotman. Aprendiendo a clasificar. Inf. Retr., 8 (3): 359-381, 2005. [29] J. Xu, Y. Cao, H. Li e Y. Huang. Aprendizaje sensible a los costos de SVM para la clasificación. En ECML, páginas 833-840, 2006. [30] G.-R.Xue, Q. Yang, H.-J. Zeng, Y. Yu y Z. Chen. Explotando la estructura jerárquica para el análisis de enlaces. En Sigir 28, páginas 186-193, 2005. [31] H. Yu. Muestreo selectivo SVM para clasificar con la aplicación a la recuperación de datos. En Sigkdd 11, páginas 354-363, 2005. Apéndice aquí damos la prueba del teorema 1. P. Establecer zt = m i = 1 exp {−e (π (qi, di, ft), yi)} y φ (t) = 1 2 (1 + ϕ (t)). Según la definición de αT, sabemos que eαt = φ (t) 1 - φ (t). Zt = m i = 1 exp {−e (π (qi, di, ft - 1 + αt ht), yi)} = m i = 1 exp −e (π (qi, di, ft - 1), yi) - αtE (π (qi, di, ht), yi) - Δt i ≤ m i = 1 exp {−e (π (qi, di, ft - 1), yi)} exp {−αt e (π (qi, di, ht), yi)} e - Δt min = e - quirado min zt - 1 m i = 1 exp {−e (π (qi, di, ft - 1), yi)} zt - 1 exp {−αt e (π (Qi, Di, Ht), yi)} = e - quirólo Zt - 1 m i = 1 pt (i) exp {−αt e (π (qi, di, ht), yi)}. Además, si e (π (qi, di, ht), yi) ∈ [−1, +1] entonces, zt ≤ e - quir, ht), yi) 2 e - αt + 1 - e (π (qi, di, ht), yi) 2 eαt = e - quirφ (t) φ (t) + (1 - φ (t)) φ (t) 1 - φ (t)   = zt - 1e - Δt min 4φ (t) (1 - φ(T)) ≤ zt - 2 t t = t - 1 e -quir)) = m m i = 1 1 m exp {−e (π (qi, di, α1h1), yi)} t t = 2 e - quirm exp {−α1e (π (qi, di, h1), yi) - Δ1 i} t t = 2 e - quirm exp {−α1e (π (qi, di, h1), yi)} t t = 2 e -quirφ (1)) t t = 2 e - Δt min 4φ (t) (1 - φ (t)) = m t t = 1 e - quirado min 1 - ϕ (t) 2.∴ 1 m m i = 1 e (π (qi, di, ft), yi) ≥ 1 m m i = 1 {1 - exp (−e (π (qi, di, ft), yi))} ≥ 1 - t = t =1 E - poro min 1 - ϕ (t) 2.",
    "original_sentences": [
        "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
        "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
        "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
        "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
        "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
        "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
        "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
        "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
        "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
        "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
        "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
        "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
        "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
        "When applied to document retrieval, learning to rank becomes a task as follows.",
        "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
        "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
        "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
        "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
        "Several methods for learning to rank have been developed and applied to document retrieval.",
        "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
        "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
        "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
        "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
        "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
        "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
        "AdaRank utilizes a linear combination of weak rankers as its model.",
        "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
        "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
        "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
        "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
        "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
        "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
        "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
        "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
        "Recently, direct optimization of performance measures in learning has become a hot research topic.",
        "Several methods for classification [17] and ranking [5, 19] have been proposed.",
        "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
        "The rest of the paper is organized as follows.",
        "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
        "Experimental results and discussions are given in Section 4.",
        "Section 5 concludes this paper and gives future work. 2.",
        "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
        "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
        "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
        "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
        "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
        "For example, Joachims [16] applies Ranking SVM to document retrieval.",
        "He utilizes click-through data to deduce training data for the model creation.",
        "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
        "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
        "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
        "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
        "They are learning to rank, boosting, and direct optimization of performance measures.",
        "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
        "Several approaches have been proposed to tackle the problem.",
        "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
        "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
        "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
        "For other approaches to learning to rank, refer to [2, 11, 31].",
        "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
        "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
        "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
        "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
        "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
        "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
        "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
        "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
        "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
        "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
        "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
        "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
        "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
        "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
        "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
        "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
        "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
        "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
        "The relevance scores are calculated with a ranking function (model).",
        "In learning (training), a number of queries and their corresponding retrieved documents are given.",
        "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
        "The relevance levels are represented as ranks (i.e., categories in a total order).",
        "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
        "Ideally the loss function is defined on the basis of the performance measure used in testing.",
        "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
        "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
        "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
        "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
        "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
        "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
        "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
        "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
        "We use π( j) to denote the position of item j (i.e., di j).",
        "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
        "Table 1: Notations and explanations.",
        "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
        "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
        "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
        "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
        "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
        "The first argument of E is the permutation π created using the ranking function f on di.",
        "The second argument is the list of ranks yi given by humans.",
        "E measures the agreement between π and yi.",
        "Table 1 gives a summary of notations described above.",
        "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
        "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
        "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
        "The algorithm is referred to as AdaRank and is shown in Figure 1.",
        "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
        "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
        "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
        "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
        "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
        "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
        "End For Output ranking model: f(x) = fT (x).",
        "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
        "Initially, AdaRank sets equal weights to the queries.",
        "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
        "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
        "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
        "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
        "Several methods for weak ranker construction can be considered.",
        "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
        "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
        "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
        "Intuitively, αt measures the importance of ht.",
        "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
        "In contrast, AdaRank tries to optimize a loss function based on queries.",
        "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
        "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
        "We next explain why this is the case.",
        "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
        "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
        "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
        "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
        "Several ways of computing coefficients αt and weak rankers ht may be considered.",
        "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
        "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
        "T 1.",
        "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
        "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
        "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
        "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
        "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
        "Notice that the major IR measures meet this requirement.",
        "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
        "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
        "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
        "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
        "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
        "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
        "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
        "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
        "The existing methods cannot focus on the training on the tops, as indicated in [4].",
        "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
        "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
        "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
        "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
        "First, the types of instances are different.",
        "AdaRank makes use of queries and their corresponding document lists as instances.",
        "The labels in training data are lists of ranks (relevance levels).",
        "AdaBoost makes use of feature vectors as instances.",
        "The labels in training data are simply +1 and −1.",
        "Second, the performance measures are different.",
        "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
        "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
        "Third, the ways of updating weights are also different.",
        "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
        "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
        "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
        "For AdaBoost they are equivalent (cf., [12] page 305).",
        "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
        "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
        "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
        "Note that features which are not selected in the training phase will have a weight of zero. 4.",
        "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
        "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
        "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
        "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
        "For AdaRank, the parameter T was determined automatically during each experiment.",
        "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
        "As the measure E, MAP and NDCG@5 were utilized.",
        "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
        "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
        "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
        "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
        "The data have been used in many experiments in IR, for example [4, 29].",
        "As features, we adopted those used in document retrieval [4].",
        "Table 2 shows the features.",
        "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
        "BM25 score itself is also a feature.",
        "Stop words were removed and stemming was conducted in the data.",
        "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
        "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
        "The results reported in Figure 2 are those averaged over four trials.",
        "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
        "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
        "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
        "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
        "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
        "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
        "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
        "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
        "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
        "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
        "Table 3 shows the statistics on the two datasets.",
        "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
        "We also conducted 4-fold cross-validation experiments.",
        "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
        "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
        "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
        "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
        "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
        "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
        "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
        "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
        "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
        "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
        "The number of relevant pages vary from query to query (from 1 to 86).",
        "We extracted 14 features from each query-document pair.",
        "Table 4 gives a list of the features.",
        "They are the outputs of some well-known algorithms (systems).",
        "These features are different from those in Table 2, because the task is different.",
        "Again, we conducted 4-fold cross-validation experiments.",
        "The results averaged over four trials are reported in Figure 5.",
        "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
        "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
        "Some of the improvements are not statistically significant.",
        "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
        "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
        "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
        "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
        "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
        "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
        "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
        "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
        "The queries are clustered into different groups based on the the number of their associated document pairs.",
        "Figure 7 shows the distribution of the query groups.",
        "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
        "We can see that the numbers of document pairs really vary from query to query.",
        "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
        "The results are reported in Figure 8.",
        "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
        "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
        "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
        "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
        "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
        "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
        "The experiment was conducted for each trial.",
        "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
        "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
        "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
        "Finally, we tried to verify the correctness of Theorem 1.",
        "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
        "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
        "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
        "The result agrees well with Theorem 1. 5.",
        "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
        "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
        "It employs a boosting technique in ranking model learning.",
        "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
        "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
        "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
        "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
        "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
        "Modern Information Retrieval.",
        "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
        "Le.",
        "Learning to rank with nonsmooth cost functions.",
        "In Advances in Neural Information Processing Systems 18, pages 395-402.",
        "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
        "Learning to rank using gradient descent.",
        "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
        "Liu, H. Li, Y. Huang, and H.-W. Hon.",
        "Adapting ranking SVM to document retrieval.",
        "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
        "Subset ranking using regression.",
        "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
        "Overview of the TREC 2003 web track.",
        "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
        "Boosting methods for regression.",
        "Mach.",
        "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
        "Singer.",
        "An efficient boosting algorithm for combining preferences.",
        "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
        "A decision-theoretic generalization of on-line learning and an application to boosting.",
        "J. Comput.",
        "Syst.",
        "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
        "Additive logistic regression: A statistical view of boosting.",
        "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
        "Learning rankings via convex hull separation.",
        "In Advances in Neural Information Processing Systems 18, pages 395-402.",
        "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
        "The Elements of Statistical Learning.",
        "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
        "Large Margin rank boundaries for ordinal regression.",
        "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
        "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
        "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
        "IR evaluation methods for retrieving highly relevant documents.",
        "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
        "Optimizing search engines using clickthrough data.",
        "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
        "A support vector method for multivariate performance measures.",
        "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
        "Document language models, query models, and risk minimization for information retrieval.",
        "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
        "Direct maximization of rank-based metrics for information retrieval.",
        "Technical report, CIIR, 2005. [20] R. Nallapati.",
        "Discriminative models for information retrieval.",
        "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
        "The pagerank citation ranking: Bringing order to the web.",
        "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
        "A language modeling approach to information retrieval.",
        "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
        "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
        "Ma.",
        "A study of relevance propagation for web search.",
        "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
        "Hull.",
        "The TREC-9 filtering track final report.",
        "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
        "Boosting the margin: A new explanation for the effectiveness of voting methods.",
        "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
        "Singer.",
        "Improved boosting algorithms using confidence-rated predictions.",
        "Mach.",
        "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
        "Ma.",
        "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
        "In TREC, 2004. [28] A. Trotman.",
        "Learning to rank.",
        "Inf.",
        "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
        "Cost-sensitive learning of SVM for ranking.",
        "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
        "Zeng, Y. Yu, and Z. Chen.",
        "Exploiting the hierarchical structure for link analysis.",
        "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
        "SVM selective sampling for ranking with application to data retrieval.",
        "In SIGKDD 11, pages 354-363, 2005.",
        "APPENDIX Here we give the proof of Theorem 1.",
        "P.",
        "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
        "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
        "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
        "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
    ],
    "error_count": 0,
    "keys": {
        "ranking model": {
            "translated_key": "modelo de clasificación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a <br>ranking model</br> that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a <br>ranking model</br> is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained <br>ranking model</br>.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the <br>ranking model</br> gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for <br>ranking model</br> tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the <br>ranking model</br> (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a <br>ranking model</br> using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to <br>ranking model</br> construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ <br>ranking model</br> π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a <br>ranking model</br> f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output <br>ranking model</br>: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A <br>ranking model</br> ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our <br>ranking model</br>: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the <br>ranking model</br> created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in <br>ranking model</br> learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Idealmente, un algoritmo de aprendizaje entrenaría un \"modelo de clasificación\" que podría optimizar directamente las medidas de rendimiento con respecto a los datos de capacitación.",
                "En la capacitación, se construye un \"modelo de clasificación\" con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia dados por los humanos.",
                "En la clasificación, dada una nueva consulta, los documentos recuperados correspondientes se clasifican utilizando el \"modelo de clasificación\" capacitado.",
                "A medida que el número de características en el \"modelo de clasificación\" se hace mayor y la cantidad de datos de entrenamiento se hace más grande, la afinación se vuelve más difícil.",
                "Desde el punto de vista de IR, Adarank se puede ver como un método de aprendizaje automático para el ajuste de \"modelo de clasificación\".",
                "Trabajo relacionado 2.1 Recuperación de información El problema clave para la recuperación de documentos es la clasificación, específicamente, cómo crear el \"modelo de clasificación\" (función) que puede ordenar documentos en función de su relevancia para la consulta dada.",
                "Es una práctica común en IR sintonizar los parámetros de un \"modelo de clasificación\" utilizando algunos datos etiquetados y una medida de rendimiento [1].",
                "Recientemente se han aplicado métodos de aprendizaje a clasificar a la construcción de \"clasificación del modelo\" y se han obtenido algunos resultados prometedores.",
                "Explicaciones de notaciones qi ∈ Q ith consulta di = {di1, di2, · · ·, di, n (qi)} Lista de documentos para qi yi j ∈ {r1, r2, · · ·, r} rango de di j w.r.t.qi yi = {yi1, yi2, · · ·, yi, n (qi)} Lista de rangos para qi s = {(qi, di, yi)} m i = 1 Conjunto de entrenamiento xij = ψ (qi, dij) ∈ XVector de características para (qi, di j) f (xij) ∈ \"modelo de clasificación\" π (qi, di, f) permutación para qi, di y f ht (xi j) ∈ Tth Ranker débil e (π (qi, di, di, di, di, f), yi) ∈ [−1, +1] Función de medida de rendimiento En el documento, definimos el modelo de ran(x) es un ranker débil, αT es su peso y t es el número de rankers débiles.",
                "Finalmente, genera un \"modelo de clasificación\" F combinando linealmente a los rankers débiles.",
                "Fin para la salida \"Modelo de clasificación\": f (x) = ft (x).",
                "Se crea un \"modelo de clasificación\" en cada ronda combinando linealmente los rankers débiles construidos hasta ahora H1, · · ·, ht con pesos α1, · · ·, αT.FT se usa luego para actualizar la distribución PT+1.3.3 Análisis teórico Los algoritmos de aprendizaje existentes para el intento de clasificación de minimizar una función de pérdida basada en pares de instancias (pares de documentos).",
                "Consideramos el uso de una combinación lineal de clasificadores débiles como nuestro \"modelo de clasificación\": F (x) = t t = 1 αTHT (x).(7) La minimización en (6) luego resulta ser min ht∈H, αT∈ + L (ht, αT) = m i = 1 exp {−e (π (qi, di, ft - 1 + αTHT),,yi)}, (8) donde h es el conjunto de posibles rankers débiles, αT es un peso positivo y (ft - 1 + αTHT) (x) = ft - 1 (x) + αTHT (x).",
                "En Adarank, en contraste, se calcula de acuerdo con el rendimiento del \"modelo de clasificación\" creado hasta ahora, como se muestra en la Figura 1.",
                "Emplea una técnica de impulso en el aprendizaje de \"modelo de clasificación\"."
            ],
            "translated_text": "",
            "candidates": [
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "clasificación del modelo",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "Modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "Modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación",
                "modelo de clasificación"
            ],
            "error": []
        },
        "novel learning algorithm": {
            "translated_key": "Algoritmo de aprendizaje novedoso",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a <br>novel learning algorithm</br> within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Para lidiar con el problema, proponemos un \"algoritmo de aprendizaje novedoso\" en el marco de impulso, que puede minimizar una función de pérdida directamente definida en las medidas de rendimiento."
            ],
            "translated_text": "",
            "candidates": [
                "Algoritmo de aprendizaje novedoso",
                "algoritmo de aprendizaje novedoso"
            ],
            "error": []
        },
        "re-weighted training datum": {
            "translated_key": "Dato de capacitación re-ponderado",
            "is_in_text": false,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "machine learning": {
            "translated_key": "aprendizaje automático",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and <br>machine learning</br>.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a <br>machine learning</br> method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a <br>machine learning</br> method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 <br>machine learning</br> There are three topics in <br>machine learning</br> which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of <br>machine learning</br> algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of <br>machine learning</br> Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La introducción recientemente el aprendizaje para clasificar ha ganado una atención cada vez mayor tanto en los campos de la recuperación de la información como en el \"aprendizaje automático\".",
                "Desde el punto de vista de IR, Adarank se puede ver como un método de \"aprendizaje automático\" para clasificar el ajuste del modelo.",
                "Adarank se puede ver como un método de \"aprendizaje automático\" para la optimización directa de las medidas de rendimiento, basado en un enfoque diferente.",
                "El método se conoce como RankNet.2.2 \"Aprendizaje automático\" Hay tres temas en \"Aprendizaje automático\" que están relacionados con nuestro trabajo actual.",
                "El impulso es una técnica general para mejorar las precisiones de los algoritmos de \"aprendizaje automático\".",
                "Journal of \"Machine Learning\" Research, 4: 933-969, 2003. [9] Y. Freund y R. E. Schapire."
            ],
            "translated_text": "",
            "candidates": [
                "aprendizaje automático",
                "aprendizaje automático",
                "aprendizaje automático",
                "aprendizaje automático",
                "aprendizaje automático",
                "aprendizaje automático",
                "aprendizaje automático",
                "Aprendizaje automático",
                "Aprendizaje automático",
                "aprendizaje automático",
                "aprendizaje automático",
                "aprendizaje automático",
                "Machine Learning"
            ],
            "error": []
        },
        "trained ranking model": {
            "translated_key": "modelo de clasificación entrenada",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the <br>trained ranking model</br>.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En la clasificación, dada una nueva consulta, los documentos recuperados correspondientes se clasifican utilizando el \"modelo de clasificación entrenado\"."
            ],
            "translated_text": "",
            "candidates": [
                "modelo de clasificación entrenada",
                "modelo de clasificación entrenado"
            ],
            "error": []
        },
        "support vector machine": {
            "translated_key": "máquinas de vectores soporte",
            "is_in_text": false,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "rankboost": {
            "translated_key": "RankBoost",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and <br>rankboost</br> train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and <br>rankboost</br>.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as <br>rankboost</br>.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and <br>rankboost</br> train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and <br>rankboost</br>, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], <br>rankboost</br> [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, <br>rankboost</br>, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, <br>rankboost</br>, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of <br>rankboost</br>, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and <br>rankboost</br> [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM <br>rankboost</br> AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and <br>rankboost</br> in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and <br>rankboost</br> in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and <br>rankboost</br> in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and <br>rankboost</br> in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and <br>rankboost</br> on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM <br>rankboost</br> AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM <br>rankboost</br> AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and <br>rankboost</br>.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and <br>rankboost</br>.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM <br>rankboost</br> AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, <br>rankboost</br>, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and <br>rankboost</br> in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than <br>rankboost</br>.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than <br>rankboost</br> for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group <br>rankboost</br> AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and <br>rankboost</br>. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Por ejemplo, la clasificación de modelos de clasificación SVM y \"RankBoost\" de trenes minimizando los errores de clasificación en los pares de instancias.",
                "Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que Adarank supera significativamente los métodos de referencia de BM25, clasificación de SVM y \"RankBoost\".",
                "Freund et al.[8] adopta un enfoque similar y realiza el aprendizaje utilizando el impulso, denominado \"RankBoost\".",
                "Por ejemplo, la clasificación de modelos de clasificación SVM y \"RankBoost\" de trenes minimizando los errores de clasificación en los pares de instancias.",
                "Los resultados experimentales indican que Adarank puede superar los métodos de referencia de BM25, clasificar SVM y \"RankBoost\", en cuatro conjuntos de datos de referencia, incluidos OHSUMED, WSJ, AP y .gov.",
                "Los métodos típicos del enfoque incluyen clasificar SVM [13], \"RankBoost\" [8] y RankNet [3].",
                "En ese sentido, los métodos existentes para clasificar SVM, \"RankBoost\" y RankNet solo pueden minimizar las funciones de pérdida que están libremente relacionadas con las medidas de rendimiento IR.",
                "Además, Adarank tiene varias otras ventajas en comparación con los métodos de aprendizaje existentes para clasificar, como clasificar SVM, \"RankBoost\" y RankNet.",
                "La complejidad del tiempo de \"rankboost\", por ejemplo, es de orden o (t · m · n2) [8].",
                "C (w, d) representa la frecuencia de la palabra w en el documento d;C representa toda la colección;n denota el número de términos en la consulta;|· |denota la función de tamaño;e ID F (·) denota la frecuencia de documentos inversos.1 wi∈Q d ln (c (wi, d) + 1) 2 wi∈Q d ln (| c | c (wi, c) + 1) 3 wi∈Q d ln (id f (wi)) 4 wi∈Q d ln(c (wi, d) | d | + 1) 5 wi∈Q d ln (c (wi, d) | d | · id f (wi) + 1) 6 wi∈Q d ln (c (wi, d) · ·| C | | d | · c (wi, c) + 1) 7 ln (puntaje BM25) 0.2 0.3 0.4 0.4 0.5 0.6 mapa ndcg@1 ndcg@3 ndcg@5 ndcg@10 bm25 ranking svm rarnkboost adarank.map adarank.ndcgFigura 2: precisiones de clasificación en datos de ohsumed.4.1 Ranking SVM [13, 16] y \"RankBoost\" [8] se seleccionaron como líneas de base en los experimentos, porque son el aprendizaje de vanguardia a los métodos de rango.",
                "Conjunto de datos # consultas # docs recuperados # documentos por consulta AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 mapa ndcg@1 ndcg@3 ndcg@5 ndcg@10 bm25 clasificación svm \": Precisiones de clasificación en el conjunto de datos WSJ.Los otros dos rangos son irrelevantes.",
                "De la Figura 2, vemos que adarank.map y adarank.ndcg superan a BM25, clasificando SVM y \"RankBoost\" en términos de todas las medidas.",
                "Realizamos pruebas significativas (prueba t) en las mejoras de adarank.map sobre BM25, clasificación de SVM y \"RankBoost\" en términos de mapa.",
                "También realizamos la prueba t en las mejoras de adarank.ndcg sobre BM25, clasificación de SVM y \"RankBoost\" en términos de NDCG@5.",
                "De la Figura 3 y 4, podemos ver que Adarank.Map y Adarank.ndcg superan a BM25, clasificando SVM y \"RankBoost\" en términos de todas las medidas en WSJ y AP.",
                "Realizamos pruebas t sobre las mejoras de adarank.map y adarank.ndcg sobre BM25, clasificando SVM y \"RankBoost\" en WSJ y AP.",
                "Hay un total de 0.40 0.45 0.50 0.55 mapa ndcg@1 ndcg@3 ndcg@5 ndcg@10 bm25 ranking svm \"rankboost\" adarank.map adarank.ndcg Figura 4: precisiones de clasificación en el datos de AP.0.1 0.2 0.3 0.4 0.5 0.6 0.7 mapa ndcg@1 ndcg@3 ndcg@5 ndcg@10 bm25 ranking svm \"rankboost\" adarank.map adarank.ndcg Figura 5: precisiones de clasificación en el datos de .gov.",
                "Realizamos ttests en las mejoras de adarank.map y adarank.ndcg sobre BM25, clasificación de SVM y \"RankBoost\".",
                "Primero, examinamos la razón por la que Adarank tiene actuaciones más altas que en clasificar SVM y \"RankBoost\".",
                "Específicamente, com0.58 0.60 0.62 0.64 0.66 0.68 D-N D-P P-N Tipo de pareja de precisión Ranking SVM \"RankBoost\" Adarank.map adarank.ndcg Figura 6: Precisión en pares de documentos de clasificación con un conjunto de datos de suma.0 2 4 6 8 10 12 Número de pares de pares de documentos por consulta Figura 7: Distribución de consultas con diferentes pares de pares de documentos en datos de entrenamiento de prueba 1. Apare las tasas de error entre los diferentes pares de rango realizados por clasificación SVM, \"RankBoost\",Adarank.map y adarank.ndcg en los datos de prueba.",
                "A continuación, evaluamos las precisiones de adarank.map y \"rankboost\" en términos de mapa para cada uno de los grupos de consultas.",
                "Encontramos que el mapa promedio de Adarank.map sobre los grupos es dos puntos más alto que \"RankBoost\".",
                "Además, es interesante ver que Adarank.Map funciona particularmente mejor que \"RankBoost\" para consultas con pequeños números de pares de documentos (por ejemplo, 0-1K, 1K-2K y 2K-3K).",
                "Para adarank.ndcg, se pueden observar resultados similares.0.2 0.3 0.4 0.5 Group de consultas de mapas \"RankBoost\" Adarank. MAP Figura 8: Diferencias en el MAP para diferentes grupos de consultas.0.30 0.31 0.32 0.33 0.34 Prueba 1 Prueba 2 Prueba 3 Prueba 4 mapa adarank.map adarank.ndcg Figura 9: mapa en el conjunto de entrenamiento cuando el modelo está entrenado con map o ndcg@5.",
                "Los resultados experimentales basados en cuatro conjuntos de datos de referencia muestran que Adarank puede superar significativamente los métodos de referencia de BM25, clasificar SVM y \"RankBoost\".0.49 0.50 0.51 0.52 0.53 Prueba 1 Prueba 2 Prueba 3 Prueba 4 NDCG@5 Adarank.map adarank.ndcg Figura 10: NDCG@5 en el conjunto de entrenamiento cuando el modelo está entrenado con MAP o NDCG@5.0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 Mapa Número de rondas Figura 11: Curva de aprendizaje de Adarank."
            ],
            "translated_text": "",
            "candidates": [
                "rango",
                "RankBoost",
                "rango",
                "RankBoost",
                "rango",
                "RankBoost",
                "rango",
                "RankBoost",
                "rango",
                "RankBoost",
                "rango",
                "RankBoost",
                "rango",
                "RankBoost",
                "rango",
                "RankBoost",
                "rango",
                "rankboost",
                "rango",
                "RankBoost",
                "rango",
                "rango",
                "RankBoost",
                "rango",
                "RankBoost",
                "rango",
                "RankBoost",
                "rango",
                "RankBoost",
                "rango",
                "RankBoost",
                "rango",
                "rankboost",
                "rankboost",
                "rango",
                "RankBoost",
                "rango",
                "RankBoost",
                "rango",
                "RankBoost",
                "RankBoost",
                "rango",
                "rankboost",
                "rango",
                "RankBoost",
                "rango",
                "RankBoost",
                "rango",
                "RankBoost",
                "rango",
                "RankBoost"
            ],
            "error": []
        },
        "new learning algorithm": {
            "translated_key": "Nuevo algoritmo de aprendizaje",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a <br>new learning algorithm</br> that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En este documento, nuestro objetivo es desarrollar un \"nuevo algoritmo de aprendizaje\" que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos."
            ],
            "translated_text": "",
            "candidates": [
                "Nuevo algoritmo de aprendizaje",
                "nuevo algoritmo de aprendizaje"
            ],
            "error": []
        },
        "document retrieval": {
            "translated_key": "recuperación de documentos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for <br>document retrieval</br>.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to <br>document retrieval</br>, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In <br>document retrieval</br>, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to <br>document retrieval</br>.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for <br>document retrieval</br> [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in <br>document retrieval</br>.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for <br>document retrieval</br> is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to <br>document retrieval</br>.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to <br>document retrieval</br> by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in <br>document retrieval</br>.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for <br>document retrieval</br>.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for <br>document retrieval</br>.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in <br>document retrieval</br> [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in <br>document retrieval</br>, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to <br>document retrieval</br>.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "49 Zhichun Road, Haidian Distints Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distint Distints Beijing, China 100080 Hangli@microsoft.com Resumen En este documento Nos dirigimos el problema de aprender a clasificar para clasificar\"Recuperación de documentos\".",
                "Cuando se aplica a la \"recuperación de documentos\", aprender a clasificar se convierte en una tarea de la siguiente manera.",
                "En la \"recuperación de documentos\", generalmente los resultados de clasificación se evalúan en términos de medidas de rendimiento, como MAP (precisión promedio media) [1] y NDCG (ganancia acumulativa con descuento normalizada) [15].",
                "Varios métodos para aprender a clasificar se han desarrollado y aplicado a \"recuperación de documentos\".",
                "Todos los métodos existentes utilizados para la \"recuperación de documentos\" [2, 3, 8, 13, 16, 20] están diseñados para optimizar las funciones de pérdidas relacionadas con las medidas de rendimiento IR, no las funciones de pérdida directamente en función de las medidas.",
                "En este documento, nuestro objetivo es desarrollar un nuevo algoritmo de aprendizaje que pueda optimizar directamente cualquier medida de rendimiento utilizada en la \"recuperación de documentos\".",
                "Trabajo relacionado 2.1 Recuperación de información El problema clave para la \"recuperación de documentos\" es la clasificación, específicamente, cómo crear el modelo de clasificación (función) que puede ordenar documentos en función de su relevancia para la consulta dada.",
                "Por ejemplo, Joachims [16] aplica la clasificación de SVM a \"Recuperación de documentos\".",
                "Cao et al.[4] Adapte la clasificación de SVM a \"recuperación de documentos\" modificando la función de pérdida de bisagra para cumplir mejor los requisitos de IR.",
                "Burges et al.[3] Emplea la entropía relativa como función de pérdida y descenso de gradiente como un algoritmo para capacitar a un modelo de red neuronal para clasificar en la \"recuperación de documentos\".",
                "Nuestro método: ADARANK 3.1 Marco general, primero describimos el marco general de aprender a clasificarse para la \"recuperación de documentos\".",
                "En realidad, este claramente no es el caso y este problema no existe para Adarank.(b) La clasificación de los documentos más relevantes en la parte superior de las listas de documentos es crucial para la \"recuperación de documentos\".",
                "Como características, adoptamos los utilizados en \"Recuperación de documentos\" [4].",
                "Conclusión y trabajo futuro En este documento hemos propuesto un algoritmo novedoso para los modelos de clasificación de aprendizaje en la \"recuperación de documentos\", denominado Adarank.",
                "Adaptación de clasificación SVM a \"Recuperación de documentos\"."
            ],
            "translated_text": "",
            "candidates": [
                "recuperación de documentos",
                "Recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "Recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "Recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "Recuperación de documentos"
            ],
            "error": []
        },
        "weak ranker": {
            "translated_key": "Rango débil",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a <br>weak ranker</br>, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth <br>weak ranker</br> E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a <br>weak ranker</br>, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a <br>weak ranker</br> ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create <br>weak ranker</br> ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a <br>weak ranker</br> that can work on the ranking of those hard queries.",
                "At each round, a <br>weak ranker</br> ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a <br>weak ranker</br> is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for <br>weak ranker</br> construction can be considered.",
                "For example, a <br>weak ranker</br> can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a <br>weak ranker</br> ht is built, AdaRank chooses a weight αt > 0 for the <br>weak ranker</br>.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of <br>weak ranker</br> We consider an efficient implementation for <br>weak ranker</br> construction, which is also used in our experiments.",
                "In the implementation, as <br>weak ranker</br> we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En el aprendizaje, repite el proceso de volver a alojar la muestra de entrenamiento, crear un \"ranker débil\" y calcular un peso para el ranker.",
                "Explicaciones de notaciones qi ∈ Q ith consulta di = {di1, di2, · · ·, di, n (qi)} Lista de documentos para qi yi j ∈ {r1, r2, · · ·, r} rango de di j w.r.t.qi yi = {yi1, yi2, · · ·, yi, n (qi)} Lista de rangos para qi s = {(qi, di, yi)} m i = 1 Conjunto de entrenamiento xij = ψ (qi, dij) ∈ XVector de características para (qi, di j) f (xij) ∈ Modelo de clasificación π (qi, di, f) permutación para qi, di y f ht (xi j) ∈ Tth \"ranker débil\" e (π (qi, di, di, di, f), yi) ∈ [−1, +1] Función de medida de rendimiento En el documento, definimos el modelo de ran(x) es un \"ranker débil\", αT es su peso, y t es el número de rankers débiles.",
                "Adarank corre t rondas y en cada ronda crea un \"ranker débil\" HT (t = 1, · · ·, t).",
                "Para t = 1, · · ·, t • Crear \"Ranker débil\" HT con PT de distribución ponderada en datos de entrenamiento s.• Elija αT αT = 1 2 · ln m i = 1 pt (i) {1 + e (π (qi, di, ht), yi)} m i = 1 pt (i) {1 - e (π (qi, di, di, ht), yi)}.• Crear ft ft (x) = t k = 1 αKHK (x).• Actualizar pt+1 pt+1 (i) = exp {−e (π (qi, di, ft), yi)} m j = 1 exp {−e (π (qj, dj, ft), yj)}.",
                "Como resultado, el aprendizaje en la próxima ronda se centrará en la creación de un \"ranker débil\" que puede funcionar en la clasificación de esas consultas difíciles.",
                "En cada ronda, se construye un HT \"Ranker débil\" en función de los datos de entrenamiento con distribución de peso PT.",
                "La bondad de un \"ranker débil\" se mide mediante la medida de rendimiento e ponderada por Pt: m i = 1 pt (i) e (π (qi, di, ht), yi).",
                "Se pueden considerar varios métodos para la construcción del \"ranker débil\".",
                "Por ejemplo, se puede crear un \"ranker débil\" utilizando un subconjunto de consultas (junto con su lista de documentos y lista de etiquetas) muestreadas de acuerdo con la distribución PT.",
                "Una vez que se construye un \"ranker débil\" HT, Adarank elige un peso αT> 0 para el \"Ranker débil\".",
                "Sin embargo, esto no es cierto para Adarank.3.6 Construcción de \"Ranker débil\" Consideramos una implementación eficiente para la construcción de \"rango débil\", que también se usa en nuestros experimentos.",
                "En la implementación, como \"Ranker débil\", elegimos la característica que tiene el rendimiento ponderado óptimo entre todas las características: max k m i = 1 pt (i) e (π (qi, di, xk), yi)."
            ],
            "translated_text": "",
            "candidates": [
                "Rango débil",
                "ranker débil",
                "Rango débil",
                "ranker débil",
                "ranker débil",
                "Rango débil",
                "ranker débil",
                "Rango débil",
                "Ranker débil",
                "Rango débil",
                "ranker débil",
                "Rango débil",
                "Ranker débil",
                "Rango débil",
                "ranker débil",
                "Rango débil",
                "ranker débil",
                "Rango débil",
                "ranker débil",
                "Rango débil",
                "ranker débil",
                "Ranker débil",
                "Rango débil",
                "Ranker débil",
                "rango débil",
                "Rango débil",
                "Ranker débil"
            ],
            "error": []
        },
        "training process": {
            "translated_key": "proceso de entrenamiento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the <br>training process</br> of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the <br>training process</br>.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Probamos que el \"proceso de entrenamiento\" de Adarank es exactamente el de mejorar la medida de rendimiento utilizada.",
                "Se proporciona un límite inferior del rendimiento en los datos de capacitación, lo que indica que la precisión de la clasificación en términos de la medida de rendimiento puede mejorarse continuamente durante el \"proceso de entrenamiento\"."
            ],
            "translated_text": "",
            "candidates": [
                "proceso de entrenamiento",
                "proceso de entrenamiento",
                "proceso de entrenamiento",
                "proceso de entrenamiento"
            ],
            "error": []
        },
        "ranking model tuning": {
            "translated_key": "ajuste del modelo de clasificación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for <br>ranking model tuning</br>.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Desde el punto de vista de IR, Adarank se puede ver como un método de aprendizaje automático para \"ajuste del modelo de clasificación\"."
            ],
            "translated_text": "",
            "candidates": [
                "ajuste del modelo de clasificación",
                "ajuste del modelo de clasificación"
            ],
            "error": []
        },
        "information retrieval": {
            "translated_key": "Recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for <br>information retrieval</br> Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of <br>information retrieval</br> and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for <br>information retrieval</br>, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 <br>information retrieval</br> The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for <br>information retrieval</br>) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with <br>information retrieval</br> and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In <br>information retrieval</br>, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern <br>information retrieval</br>.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for <br>information retrieval</br>.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for <br>information retrieval</br>.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for <br>information retrieval</br>.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to <br>information retrieval</br>.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Adarank: un algoritmo de impulso para la \"recuperación de información\" Jun Xu Microsoft Research Asia No.",
                "La introducción recientemente el aprendizaje para clasificar ha ganado una atención creciente tanto en los campos de la \"recuperación de la información\" como en el aprendizaje automático.",
                "Inspirados en el trabajo de Adaboost para la clasificación [9], proponemos desarrollar un algoritmo de impulso para la \"recuperación de información\", denominada Adarank.",
                "Trabajo relacionado 2.1 \"Recuperación de información\" El problema clave para la recuperación de documentos es la clasificación, específicamente, cómo crear el modelo de clasificación (función) que puede ordenar documentos en función de su relevancia para la consulta dada.",
                "Por ejemplo, los métodos de última generación de BM25 [24] y LMIR (modelos de lenguaje para \"recuperación de información\") [18, 22] tienen parámetros que sintonizar.",
                "Este enfoque pareal se ajusta bien a la \"recuperación de información\" y, por lo tanto, se usa ampliamente en IR.",
                "En \"Recuperación de información\", las medidas de rendimiento basadas en consultas se utilizan para evaluar la bondad de una función de clasificación.",
                "\"Recuperación de información\" moderna.",
                "Documentar modelos de lenguaje, modelos de consulta y minimización de riesgos para \"recuperación de información\".",
                "Maximización directa de métricas basadas en rango para \"recuperación de información\".",
                "Modelos discriminativos para la \"recuperación de información\".",
                "Un enfoque de modelado de idiomas para la \"recuperación de información\"."
            ],
            "translated_text": "",
            "candidates": [
                "Recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de la información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "Recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "Recuperación de información",
                "recuperación de información",
                "Recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información"
            ],
            "error": []
        },
        "learn to rank": {
            "translated_key": "aprender a clasificar",
            "is_in_text": false,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using boosting, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, boosting, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of boosting is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is boosted.",
                "Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a boosting technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient boosting algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to boosting.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of boosting.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved boosting algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "boost": {
            "translated_key": "aumentar",
            "is_in_text": true,
            "original_annotated_sentences": [
                "AdaRank: A Boosting Algorithm for Information Retrieval Jun Xu Microsoft Research Asia No.",
                "49 Zhichun Road, Haidian Distinct Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distinct Beijing, China 100080 hangli@microsoft.com ABSTRACT In this paper we address the issue of learning to rank for document retrieval.",
                "In the task, a model is automatically created with some training data and then is utilized for ranking of documents.",
                "The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain).",
                "Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data.",
                "Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "To deal with the problem, we propose a novel learning algorithm within the framework of <br>boost</br>ing, which can minimize a loss function directly defined on the performance measures.",
                "Our algorithm, referred to as AdaRank, repeatedly constructs weak rankers on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions.",
                "We prove that the training process of AdaRank is exactly that of enhancing the performance measure used.",
                "Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms, Experimentation, Theory 1.",
                "INTRODUCTION Recently learning to rank has gained increasing attention in both the fields of information retrieval and machine learning.",
                "When applied to document retrieval, learning to rank becomes a task as follows.",
                "In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans.",
                "In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.",
                "In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumulative Gain) [15].",
                "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.",
                "Several methods for learning to rank have been developed and applied to document retrieval.",
                "For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM.",
                "Freund et al. [8] take a similar approach and perform the learning by using <br>boost</br>ing, referred to as RankBoost.",
                "All the existing methods used for document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures.",
                "For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs.",
                "In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.",
                "Inspired by the work of AdaBoost for classification [9], we propose to develop a <br>boost</br>ing algorithm for information retrieval, referred to as AdaRank.",
                "AdaRank utilizes a linear combination of weak rankers as its model.",
                "In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.",
                "We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures.",
                "A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.",
                "AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.",
                "Tuning ranking models using certain training data and a performance measure is a common practice in IR [1].",
                "As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder.",
                "From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.",
                "Recently, direct optimization of performance measures in learning has become a hot research topic.",
                "Several methods for classification [17] and ranking [5, 19] have been proposed.",
                "AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.",
                "The rest of the paper is organized as follows.",
                "After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3.",
                "Experimental results and discussions are given in Section 4.",
                "Section 5 concludes this paper and gives future work. 2.",
                "RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query.",
                "It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1].",
                "For example, the state-ofthe-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune.",
                "As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.",
                "Recently methods of learning to rank have been applied to ranking model construction and some promising results have been obtained.",
                "For example, Joachims [16] applies Ranking SVM to document retrieval.",
                "He utilizes click-through data to deduce training data for the model creation.",
                "Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR.",
                "Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents.",
                "Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval.",
                "The method is referred to as RankNet. 2.2 Machine Learning There are three topics in machine learning which are related to our current work.",
                "They are learning to rank, <br>boost</br>ing, and direct optimization of performance measures.",
                "Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores.",
                "Several approaches have been proposed to tackle the problem.",
                "One major approach to learning to rank is that of transforming it into binary classification on instance pairs.",
                "This pair-wise approach fits well with information retrieval and thus is widely used in IR.",
                "Typical methods of the approach include Ranking SVM [13], RankBoost [8], and RankNet [3].",
                "For other approaches to learning to rank, refer to [2, 11, 31].",
                "In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked).",
                "Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16].",
                "In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.",
                "Boosting is a general technique for improving the accuracies of machine learning algorithms.",
                "The basic idea of <br>boost</br>ing is to repeatedly construct weak learners by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is <br>boost</br>ed.",
                "Freund and Schapire have proposed the first well-known <br>boost</br>ing algorithm called AdaBoost (Adaptive Boosting) [9], which is designed for binary classification (0-1 prediction).",
                "Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions [26].",
                "Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8].",
                "In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the exponential loss function with respect to the training data [26].",
                "Our work in this paper can be viewed as a <br>boost</br>ing method developed for ranking, particularly for ranking in IR.",
                "Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning.",
                "For instance, Joachims [17] presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification.",
                "Cossock & Zhang [5] find a way to approximately optimize the ranking performance measure DCG [15].",
                "Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning.",
                "AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach.",
                "AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a <br>boost</br>ing technique. 3.",
                "OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval.",
                "In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores.",
                "The relevance scores are calculated with a ranking function (model).",
                "In learning (training), a number of queries and their corresponding retrieved documents are given.",
                "Furthermore, the relevance levels of the documents with respect to the queries are also provided.",
                "The relevance levels are represented as ranks (i.e., categories in a total order).",
                "The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function.",
                "Ideally the loss function is defined on the basis of the performance measure used in testing.",
                "Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes the number of ranks.",
                "There exists a total order between the ranks r r −1 · · · r1, where  denotes a preference relationship.",
                "In training, a set of queries Q = {q1, q2, · · · , qm} is given.",
                "Each query qi is associated with a list of retrieved documents di = {di1, di2, · · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij ∈ Y denotes the rank of document di j.",
                "A feature vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair (qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi).",
                "Thus, the training set can be represented as S = {(qi, di, yi)}m i=1.",
                "The objective of learning is to create a ranking function f : X → , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores.",
                "Specifically, we create a permutation of integers π(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)}, then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · , n(qi)} to itself.",
                "We use π( j) to denote the position of item j (i.e., di j).",
                "The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation π(qi, di, f) and the list of ranks yi, for all of the queries.",
                "Table 1: Notations and explanations.",
                "Notations Explanations qi ∈ Q ith query di = {di1, di2, · · · , di,n(qi)} List of documents for qi yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j) f(xij) ∈ Ranking model π(qi, di, f) Permutation for qi, di, and f ht(xi j) ∈ tth weak ranker E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 αtht(x), where ht(x) is a weak ranker, αt is its weight, and T is the number of weak rankers.",
                "In information retrieval, query-based performance measures are used to evaluate the goodness of a ranking function.",
                "By query based measure, we mean a measure defined over a ranking list of documents with respect to a query.",
                "These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15].",
                "We utilize a general function E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance measures.",
                "The first argument of E is the permutation π created using the ranking function f on di.",
                "The second argument is the list of ranks yi given by humans.",
                "E measures the agreement between π and yi.",
                "Table 1 gives a summary of notations described above.",
                "Next, as examples of performance measures, we present the definitions of MAP and NDCG.",
                "Given a query qi, the corresponding list of ranks yi, and a permutation πi on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) · yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:πi(k)≤πi(j) yik πi(j) , (2) where πi( j) denotes the position of di j.",
                "Given a query qi, the list of ranks yi, and a permutation πi on di, NDCG at position m for qi is defined as: Ni = ni · j:πi(j)≤m 2yi j − 1 log(1 + πi( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking π∗ i s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures.",
                "The algorithm is referred to as AdaRank and is shown in Figure 1.",
                "AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters.",
                "AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, · · · , T).",
                "Finally, it outputs a ranking model f by linearly combining the weak rankers.",
                "At each round, AdaRank maintains a distribution of weights over the queries in the training data.",
                "We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m.",
                "For t = 1, · · · , T • Create weak ranker ht with weighted distribution Pt on training data S . • Choose αt αt = 1 2 · ln m i=1 Pt(i){1 + E(π(qi, di, ht), yi)} m i=1 Pt(i){1 − E(π(qi, di, ht), yi)} . • Create ft ft(x) = t k=1 αkhk(x). • Update Pt+1 Pt+1(i) = exp{−E(π(qi, di, ft), yi)} m j=1 exp{−E(π(qj, dj, ft), yj)} .",
                "End For Output ranking model: f(x) = fT (x).",
                "Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i).",
                "Initially, AdaRank sets equal weights to the queries.",
                "At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far.",
                "As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those hard queries.",
                "At each round, a weak ranker ht is constructed based on training data with weight distribution Pt.",
                "The goodness of a weak ranker is measured by the performance measure E weighted by Pt: m i=1 Pt(i)E(π(qi, di, ht), yi).",
                "Several methods for weak ranker construction can be considered.",
                "For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt.",
                "In this paper, we use single features as weak rankers, as will be explained in Section 3.6.",
                "Once a weak ranker ht is built, AdaRank chooses a weight αt > 0 for the weak ranker.",
                "Intuitively, αt measures the importance of ht.",
                "A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, · · · , ht with weights α1, · · · , αt. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs).",
                "In contrast, AdaRank tries to optimize a loss function based on queries.",
                "Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures.",
                "The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [−1, +1].",
                "We next explain why this is the case.",
                "Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f∈F m i=1 E(π(qi, di, f), yi), (4) where F is the set of possible ranking functions.",
                "This is equivalent to minimizing the loss on the training data min f∈F m i=1 (1 − E(π(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle.",
                "We instead attempt to minimize an upper bound of the loss in (5) min f∈F m i=1 exp{−E(π(qi, di, f), yi)}, (6) because e−x ≥ 1 − x holds for any x ∈ .",
                "We consider the use of a linear combination of weak rankers as our ranking model: f(x) = T t=1 αtht(x). (7) The minimization in (6) then turns out to be min ht∈H,αt∈ + L(ht, αt) = m i=1 exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8) where H is the set of possible weak rankers, αt is a positive weight, and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x).",
                "Several ways of computing coefficients αt and weak rankers ht may be considered.",
                "Following the idea of AdaBoost, in AdaRank we take the approach of forward stage-wise additive modeling [12] and get the algorithm in Figure 1.",
                "It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.",
                "T 1.",
                "The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2, where ϕ(t) = m i=1 Pt(i)E(π(qi, di, ht), yi), δt min = mini=1,··· ,m δt i, and δt i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi) −αtE(π(qi, di, ht), yi), for all i = 1, 2, · · · , m and t = 1, 2, · · · , T. A proof of the theorem can be found in appendix.",
                "The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method.",
                "More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above.",
                "In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.",
                "First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [−1, +1].",
                "Notice that the major IR measures meet this requirement.",
                "In contrast the existing methods only minimize loss functions that are loosely related to the IR measures [16].",
                "Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms.",
                "The time complexity of AdaRank is of order O((k+T)·m·n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data.",
                "The time complexity of RankBoost, for example, is of order O(T · m · n2 ) [8].",
                "Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods.",
                "Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs.",
                "As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed.",
                "In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval.",
                "The existing methods cannot focus on the training on the tops, as indicated in [4].",
                "Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fundamentally solve the problem.",
                "In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4].",
                "AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a <br>boost</br>ing algorithm.",
                "In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost.",
                "First, the types of instances are different.",
                "AdaRank makes use of queries and their corresponding document lists as instances.",
                "The labels in training data are lists of ranks (relevance levels).",
                "AdaBoost makes use of feature vectors as instances.",
                "The labels in training data are simply +1 and −1.",
                "Second, the performance measures are different.",
                "In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query.",
                "In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as margin [25].",
                "Third, the ways of updating weights are also different.",
                "In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner.",
                "In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1.",
                "Note that AdaBoost can also adopt the weight updating method used in AdaRank.",
                "For AdaBoost they are equivalent (cf., [12] page 305).",
                "However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments.",
                "In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max k m i=1 Pt(i)E(π(qi, di, xk), yi).",
                "Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features.",
                "Note that features which are not selected in the training phase will have a weight of zero. 4.",
                "EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.",
                "Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets.",
                "C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | · | denotes the size function; and id f(·) denotes inverse document frequency. 1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C| c(wi,C) + 1) 3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d) |d| + 1) 5 wi∈q d ln(c(wi,d) |d| · id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C| |d|·c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM [13, 16] and RankBoost [8] were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods.",
                "Furthermore, BM25 [24] was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ).",
                "For AdaRank, the parameter T was determined automatically during each experiment.",
                "Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined).",
                "As the measure E, MAP and NDCG@5 were utilized.",
                "The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank.",
                "The OHSUMED dataset consists of 348,566 documents and 106 queries.",
                "There are in total 16,140 query-document pairs upon which relevance judgments are made.",
                "The relevance judgments are either d (definitely relevant), p (possibly relevant), or n(not relevant).",
                "The data have been used in many experiments in IR, for example [4, 29].",
                "As features, we adopted those used in document retrieval [4].",
                "Table 2 shows the features.",
                "For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features.",
                "BM25 score itself is also a feature.",
                "Stop words were removed and stemming was conducted in the data.",
                "We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments.",
                "We tuned the parameters for BM25 during one of the trials and applied them to the other trials.",
                "The results reported in Figure 2 are those averaged over four trials.",
                "In MAP calculation, we define the rank d as relevant and 1 http://www.lemurproject.com Table 3: Statistics on WSJ and AP datasets.",
                "Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant.",
                "From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures.",
                "We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP.",
                "The results indicate that all the improvements are statistically significant (p-value < 0.05).",
                "We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5.",
                "The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank.",
                "WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101 ∼ No.300).",
                "Each query has a number of documents associated and they are labeled as relevant or irrelevant (to the query).",
                "Following the practice in [28], the queries that have less than 10 relevant documents were discarded.",
                "Table 3 shows the statistics on the two datasets.",
                "In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking.",
                "We also conducted 4-fold cross-validation experiments.",
                "The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively.",
                "From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP.",
                "We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP.",
                "The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05).",
                "However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval.",
                "The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002.",
                "There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset.",
                "Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data.",
                "The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used.",
                "The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant.",
                "The number of relevant pages vary from query to query (from 1 to 86).",
                "We extracted 14 features from each query-document pair.",
                "Table 4 gives a list of the features.",
                "They are the outputs of some well-known algorithms (systems).",
                "These features are different from those in Table 2, because the task is different.",
                "Again, we conducted 4-fold cross-validation experiments.",
                "The results averaged over four trials are reported in Figure 5.",
                "From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures.",
                "We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost.",
                "Some of the improvements are not statistically significant.",
                "This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples.",
                "First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost.",
                "Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 0 2 4 6 8 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data.",
                "The results averaged over four trials in the 4-fold cross validation are shown in Figure 6.",
                "We use d-n to stand for the pairs between definitely relevant and not relevant, d-p the pairs between definitely relevant and partially relevant, and p-n the pairs between partially relevant and not relevant.",
                "From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for d-n and d-p, which are related to the tops of rankings and are important.",
                "This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.",
                "We also made statistics on the number of document pairs per query in the training data (for trial 1).",
                "The queries are clustered into different groups based on the the number of their associated document pairs.",
                "Figure 7 shows the distribution of the query groups.",
                "In the figure, for example, 0-1k is the group of queries whose number of document pairs are between 0 and 999.",
                "We can see that the numbers of document pairs really vary from query to query.",
                "Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group.",
                "The results are reported in Figure 8.",
                "We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost.",
                "Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., 0-1k, 1k-2k, and 2k-3k).",
                "The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs.",
                "For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5.",
                "We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training.",
                "Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5.",
                "The experiment was conducted for each trial.",
                "Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively.",
                "We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5.",
                "The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.",
                "Finally, we tried to verify the correctness of Theorem 1.",
                "That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e−δt min 1 − ϕ(t)2 < 1 holds.",
                "As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation.",
                "From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak.",
                "The result agrees well with Theorem 1. 5.",
                "CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.",
                "In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.",
                "It employs a <br>boost</br>ing technique in ranking model learning.",
                "AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.",
                "Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.",
                "Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6.",
                "ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper. 7.",
                "REFERENCES [1] R. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison Wesley, May 1999. [2] C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with nonsmooth cost functions.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In ICML 22, pages 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In SIGIR 29, pages 186-193, 2006. [5] D. Cossock and T. Zhang.",
                "Subset ranking using regression.",
                "In COLT, pages 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.",
                "Overview of the TREC 2003 web track.",
                "In TREC, pages 78-92, 2003. [7] N. Duffy and D. Helmbold.",
                "Boosting methods for regression.",
                "Mach.",
                "Learn., 47(2-3):153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y.",
                "Singer.",
                "An efficient <br>boost</br>ing algorithm for combining preferences.",
                "Journal of Machine Learning Research, 4:933-969, 2003. [9] Y. Freund and R. E. Schapire.",
                "A decision-theoretic generalization of on-line learning and an application to <br>boost</br>ing.",
                "J. Comput.",
                "Syst.",
                "Sci., 55(1):119-139, 1997. [10] J. Friedman, T. Hastie, and R. Tibshirani.",
                "Additive logistic regression: A statistical view of <br>boost</br>ing.",
                "The Annals of Statistics, 28(2):337-374, 2000. [11] G. Fung, R. Rosales, and B. Krishnapuram.",
                "Learning rankings via convex hull separation.",
                "In Advances in Neural Information Processing Systems 18, pages 395-402.",
                "MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani, and J. H. Friedman.",
                "The Elements of Statistical Learning.",
                "Springer, August 2001. [13] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large Margin rank boundaries for ordinal regression.",
                "MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.",
                "Ohsumed: an interactive retrieval evaluation and new large test collection for research.",
                "In SIGIR, pages 192-201, 1994. [15] K. Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In SIGIR 23, pages 41-48, 2000. [16] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In SIGKDD 8, pages 133-142, 2002. [17] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In ICML 22, pages 377-384, 2005. [18] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In SIGIR 24, pages 111-119, 2001. [19] D. A. Metzler, W. B. Croft, and A. McCallum.",
                "Direct maximization of rank-based metrics for information retrieval.",
                "Technical report, CIIR, 2005. [20] R. Nallapati.",
                "Discriminative models for information retrieval.",
                "In SIGIR 27, pages 64-71, 2004. [21] L. Page, S. Brin, R. Motwani, and T. Winograd.",
                "The pagerank citation ranking: Bringing order to the web.",
                "Technical report, Stanford Digital Library Technologies Project, 1998. [22] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In SIGIR 21, pages 275-281, 1998. [23] T. Qin, T.-Y.",
                "Liu, X.-D. Zhang, Z. Chen, and W.-Y.",
                "Ma.",
                "A study of relevance propagation for web search.",
                "In SIGIR 28, pages 408-415, 2005. [24] S. E. Robertson and D. A.",
                "Hull.",
                "The TREC-9 filtering track final report.",
                "In TREC, pages 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee.",
                "Boosting the margin: A new explanation for the effectiveness of voting methods.",
                "In ICML 14, pages 322-330, 1997. [26] R. E. Schapire and Y.",
                "Singer.",
                "Improved <br>boost</br>ing algorithms using confidence-rated predictions.",
                "Mach.",
                "Learn., 37(3):297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, J. Zhang, G. Xue, and W.-Y.",
                "Ma.",
                "Microsoft Research Asia at web track and terabyte track of TREC 2004.",
                "In TREC, 2004. [28] A. Trotman.",
                "Learning to rank.",
                "Inf.",
                "Retr., 8(3):359-381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang.",
                "Cost-sensitive learning of SVM for ranking.",
                "In ECML, pages 833-840, 2006. [30] G.-R. Xue, Q. Yang, H.-J.",
                "Zeng, Y. Yu, and Z. Chen.",
                "Exploiting the hierarchical structure for link analysis.",
                "In SIGIR 28, pages 186-193, 2005. [31] H. Yu.",
                "SVM selective sampling for ranking with application to data retrieval.",
                "In SIGKDD 11, pages 354-363, 2005.",
                "APPENDIX Here we give the proof of Theorem 1.",
                "P.",
                "Set ZT = m i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1 2 (1 + ϕ(t)).",
                "According to the definition of αt, we know that eαt = φ(t) 1−φ(t) .",
                "ZT = m i=1 exp {−E(π(qi, di, fT−1 + αT hT ), yi)} = m i=1 exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT i ≤ m i=1 exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT min = e−δT min ZT−1 m i=1 exp {−E(π(qi, di, fT−1), yi)} ZT−1 exp{−αT E(π(qi, di, hT ), yi)} = e−δT min ZT−1 m i=1 PT (i) exp{−αT E(π(qi, di, hT ), yi)}.",
                "Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then, ZT ≤ e−δT minZT−1 m i=1 PT (i) 1+E(π(qi, di, hT ), yi) 2 e−αT + 1−E(π(qi, di, hT ), yi) 2 eαT = e−δT min ZT−1  φ(T) 1 − φ(T) φ(T) + (1 − φ(T)) φ(T) 1 − φ(T)   = ZT−1e−δT min 4φ(T)(1 − φ(T)) ≤ ZT−2 T t=T−1 e−δt min 4φ(t)(1 − φ(t)) ≤ Z1 T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−E(π(qi, di, α1h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) = m m i=1 1 m exp{−α1E(π(qi, di, h1), yi) − δ1 i } T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ me−δ1 min m i=1 1 m exp{−α1E(π(qi, di, h1), yi)} T t=2 e−δt min 4φ(t)(1 − φ(t)) ≤ m e−δ1 min 4φ(1)(1 − φ(1)) T t=2 e−δt min 4φ(t)(1 − φ(t)) = m T t=1 e−δt min 1 − ϕ(t)2. ∴ 1 m m i=1 E(π(qi, di, fT ), yi) ≥ 1 m m i=1 {1 − exp(−E(π(qi, di, fT ), yi))} ≥ 1 − T t=1 e−δt min 1 − ϕ(t)2."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Para lidiar con el problema, proponemos un algoritmo de aprendizaje novedoso dentro del marco de \"aumentar\", que puede minimizar una función de pérdida directamente definida en las medidas de rendimiento.",
                "Freund et al.[8] adopta un enfoque similar y realiza el aprendizaje mediante el uso de \"Boost\" ING, denominado RankBoost.",
                "Inspirados en el trabajo de Adaboost para la clasificación [9], proponemos desarrollar un algoritmo de \"impulso\" para la recuperación de información, denominado Adarank.",
                "Están aprendiendo a clasificar, \"aumentar\" y la optimización directa de las medidas de rendimiento.",
                "La idea básica de \"aumentar\" es construir repetidamente a los alumnos débiles al volver a alojar los datos de capacitación y formar un conjunto de estudiantes débiles, de modo que el rendimiento total del conjunto se \"impulse\" ed.",
                "Freund y Schapire han propuesto el primer algoritmo de \"impulso\" bien conocido llamado Adaboost (impulso adaptativo) [9], que está diseñado para la clasificación binaria (predicción 0-1).",
                "Nuestro trabajo en este documento puede verse como un método de \"impulso\" desarrollado para la clasificación, particularmente para la clasificación en IR.",
                "Adarank es único porque emplea una función de pérdida exponencial basada en medidas de rendimiento de IR y una técnica de \"impulso\".3.",
                "Adarank no tiene este inconveniente, porque trata consultas en lugar de pares de documentos como unidades básicas en el aprendizaje.3.5 Las diferencias de ADABOOST Adarank es un algoritmo de \"impulso\".",
                "Emplea una técnica de \"impulso\" en la clasificación del aprendizaje del modelo.",
                "Un algoritmo eficiente de \"impulso\" para combinar las preferencias.",
                "Una generalización teórica de decisión del aprendizaje en línea y una aplicación para \"aumentar\".",
                "Regresión logística aditiva: una visión estadística de \"impulso\".",
                "Algoritmos mejorados de \"impulso\" utilizando predicciones con clasificación de confianza."
            ],
            "translated_text": "",
            "candidates": [
                "aumentar",
                "aumentar",
                "aumentar",
                "Boost",
                "aumentar",
                "impulso",
                "aumentar",
                "aumentar",
                "aumentar",
                "aumentar",
                "impulse",
                "aumentar",
                "impulso",
                "aumentar",
                "impulso",
                "Boost",
                "impulso",
                "aumentar",
                "impulso",
                "aumentar",
                "impulso",
                "aumentar",
                "impulso",
                "aumentar",
                "aumentar",
                "aumentar",
                "impulso",
                "aumentar",
                "impulso"
            ],
            "error": []
        }
    }
}