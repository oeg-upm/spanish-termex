{
    "id": "H-9",
    "original_text": "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine. Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly. However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster. In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users. We evaluate our proposed method on a commercial search engine log data. Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels. Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1. INTRODUCTION The utility of a search engine is affected by multiple factors. While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly. Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization. The most common strategy of presenting search results is a simple ranked list. Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results. However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group. For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team. Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD. In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list. Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document. As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28]. The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic. A label will be generated to indicate what each cluster is about. A user can then view the labels to decide which cluster to look into. Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26]. However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective. For example, users are often interested in finding either phone codes or zip codes when entering the query area codes. But the clusters discovered by the current methods may partition the results into local codes and international codes. Such clusters would not be very useful for users; even the best cluster would still have a low precision. Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster. There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms. For example, the ambiguous query jaguar may mean an animal or a car. A cluster may be labeled as panthera onca. Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful. In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results. That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly. Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects. For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query. In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar. More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data). Such aspects can be very useful for organizing future search results about car. Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways. Second, we will generate more meaningful cluster labels using past query words entered by users. Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects. Thus they can be better labels than those extracted from the ordinary contents of search results. To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs. Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs. We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster. We evaluate our method for result organization using logs of a commercial search engine. We compare our method with the default search engine ranking and the traditional clustering of search results. The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches. The rest of the paper is organized as follows. We first review the related work in Section 2. In Section 3, we describe search engine log data and our procedure of building a history collection. In Section 4, we present our approach in details. We describe the data set in Section 5 and the experimental results are discussed in Section 6. Finally, we conclude our paper and discuss future work in Section 7. 2. RELATED WORK Our work is closely related to the study of clustering search results. In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system. Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters. The system Grouper was described in [26, 27]. In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents. Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one. They also showed that using snippets is as effective as using whole documents. However, an important challenge of document clustering is to generate meaningful labels for clusters. To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results. In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster. Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22]. However, in all these works, the clusters are generated solely based on the search results. Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint. Methods of organizing search results based on text categorization are studied in [6, 8]. In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories. The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces. However predefined categories are often too general to reflect the finer granularity aspects of a query. Search logs have been exploited for several different purposes in the past. For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4]. Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1]. In our work, we explore past query history in order to better organize the search results for future queries. We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query. Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3. SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs. Different IDs mean different sessions. Web search. They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked. Search engine logs are separated by sessions. A session includes a single query and all the URLs that a user clicked after issuing the query [24]. A small sample of search log data is shown in Table 1. Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries. For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car. Different users are probably interested in different aspects of car. Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio. By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective. As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ... In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection. As shown above, search engine logs consist of sessions. Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks. However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately. To gather rich information, we enrich each URL with additional text content. Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session. All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session. Different sessions may contain the same queries. Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant. In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together. That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated. The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents. All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4. OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs. Given an input query, the general procedure of our approach is: 1. Get its related information from search engine logs. All the information forms a working set. 2. Learn aspects from the information in the working set. These aspects correspond to users interests given the input query. Each aspect is labeled with a representative query. 3. Categorize and organize the search results of the input query according to the aspects learned above. We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages. To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection. Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }. Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3. To find qs related queries in H, a natural way is to use a text retrieval algorithm. Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods. Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection. Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in. Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in. In this subsection, we propose to use a clustering method to discover these aspects. Any clustering algorithm could be applied here. In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2]. A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally. We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18]. Then the clusters are formed by dense subgraphs that are star-shaped. These clusters form a cover of the similarity graph. Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector. Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | . A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ. Each document di is a vertex of Gσ. If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices. After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1. Associate every vertex in Gσ with a flag, initialized as unmarked. 2. From those unmarked vertices, find the one which has the highest degree and let it be u. 3. Mark the flag of u as center. 4. Form a cluster C containing u and all its neighbors that are not marked as center. Mark all the selected neighbors as satellites. 5. Repeat from step 2 until all the vertices in Gσ are marked. Each cluster is star-shaped, which consists a single center and several satellites. There is only one parameter σ in the star clustering algorithm. A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small. On the other hand, a small σ will make the clusters big and less coherent. We will study the impact of this parameter in our experiments. A good feature of the star clustering algorithm is that it outputs a center for each cluster. In the past query collection Hq, each document corresponds to a query. This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally. All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results. Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm. In principle, any categorization algorithm can be used here. Here we use a simple centroid-based method for categorization. Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance. Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl. All these pis are used to categorize the search results. Specifically, for any search result sj, we build a TF-IDF vector. The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi. We then assign sj to the aspect with which it has the highest cosine similarity score. All the aspects are finally ranked according to the number of search results they have. Within each aspect, the search results are ranked according to their original search engine ranking. 5. DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14]. In total, this log data spans 31 days from 05/01/2006 to 05/31/2006. There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data. To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries. In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times). After cleaning, we get 169,057 unique queries in our history data collection totally. On average, each query has 3.5 distinct clicks. We build the pseudo-documents for all these queries as described in Section 3. The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB. We construct our test data from the last 1/3 data. According to the time, we separate this data into two test sets equally for cross-validation to set parameters. For each test set, we use every session as a test case. Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases. Different test cases may have the same queries but possibly different clicks.) Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents. Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs. Organizing search results into different aspects is expected to help informational queries. It thus makes sense to focus on the informational queries in our evaluation. For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query. Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection. Finally, we obtain 172 and 177 test cases in the first and second test sets respectively. On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6. EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results. For each test case, the first method is the default ranked list from a search engine (baseline). The second method is to organize the search results by clustering them (cluster-based). For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering). That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters. We compare our method (log-based) with the two baseline methods in the following experiments. For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine. To compare different result organization methods, we adopt a similar method as in the paper [9]. That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents. Organizing search results into clusters is to help users navigate into relevant documents quickly. The above metric is to simulate a scenario when users always choose the right cluster and look into it. Specifically, we download and organize the top 100 search results into aspects for each test case. We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods. P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents. We also use Mean Reciprocal Rank (MRR) as another metric. MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q. To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect. The number of aspects is fixed at 10 in all the following experiments. The star clustering algorithm can output different number of clusters for different input. To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates. We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid. In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results. In the following, we test our hypothesis from two perspectives - organization and labeling. Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5. We also show the percentage of relative improvement in the lower part. Comparison Test set 1 Test set 2 Impr./Decr. Impr./Decr. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5. We optimize the parameter σs for each collection individually based on P@5 values. This shows the best performance that each method can achieve. In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods. For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783. We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement). The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method. Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy. This is because cluster-based method organizes the search results only based on the contents. Thus it could organize the results differently from users preferences. This confirms our hypothesis of the bias of the cluster-based method. Comparing our method with the cluster-based method, we achieve significant improvement on both test collections. The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively. This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users. We showed the optimal results above. To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set. We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1. We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance. However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection. We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity. In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased. We can see that our method improves more test cases compared with the other two methods. In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty. All the analysis below is based on test set 1. Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters. In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity. If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse. In this case, we would expect our method to help more. The results are shown in Figure 2. In this figure, we partition the ratios into 4 bins. The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.) In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure. We can observe that when the ratio is smaller, the log-based method can improve more test cases. But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline. For example, in bin 1, 48 test cases are improved and 34 are decreased. But in bin 4, all the 4 test cases are decreased. This confirms our hypothesis that our method can help more if the query has more diverse results. This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio). Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5]. Here we analyze the effectiveness of our method in helping difficult queries. We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case. We then order the 172 test cases in test set 1 in an increasing order of MAP values. We partition the test cases into 4 bins with each having a roughly equal number of test cases. A small MAP means that the utility of the original ranking is low. Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs. For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased. Figure 3 shows the results. Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20). This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries. This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section. For the star clustering algorithm, we study the similarity threshold parameter σ. For the OKAPI retrieval function, we study the parameters k1 and b. We also study the impact of the number of past queries retrieved in our log-based method. Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets. We vary σ from 0.05 to 0.3 with step 0.05. Figure 4 shows that the performance is not very sensitive to the parameter σ. We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25. In Table 4, we show the impact of OKAPI parameters. We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2. From this table, it is clear that P@5 is also not very sensitive to the parameter setting. Most of the values are larger than 0.35. The default values k1 = 1.2 and b = 0.8 give approximately optimal results. We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods. We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects. The results on both test collections are shown in Figure 5. We can see that the performance gradually increases as we enlarge the number of past queries retrieved. Thus our method could potentially learn more as we accumulate more history. More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method. This query may mean phone codes or zip codes. Table 5 shows the representative keywords extracted from the three biggest clusters of both methods. In the clusterbased method, the results are partitioned based on locations: local or international. In the log-based method, the results are disambiguated into two senses: phone codes or zip codes. While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively. Therefore our log-based method is more effective in helping users to navigate into their desired results. Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method. The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster. Our log-based method can avoid this difficulty by taking advantage of queries. Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label. For log-based method, we use the center of each star cluster as the label for the corresponding cluster. In general, it is not easy to quantify the readability of a cluster label automatically. We use examples to show the difference between the cluster-based and the log-based methods. In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple. For the cluster-based method, we separate keywords by commas since they do not form a phrase. From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries. This is another advantage of our way of organizing search results over the clustering approach. Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7. CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner. To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective. Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned. We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking. The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse. Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results. There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple. It would be interesting to explore other potentially more effective methods. In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously. Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view). It would thus be interesting to study how to further improve the organization of the results based on such feedback information. Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8. ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments. This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9. REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais. Improving web search ranking by incorporating user behavior information. In SIGIR, pages 19-26, 2006. [2] J. A. Aslam, E. Pelekov, and D. Rus. The star clustering algorithm for static and dynamic information organization. Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Applications of web query mining. In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger. Agglomerative clustering of a search engine query log. In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. What makes a query difficult? In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais. Bringing order to the web: automatically categorizing search results. In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen. Optimizing search by showing results in context. In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen. Reexamining the cluster hypothesis: Scatter/gather on retrieval results. In SIGIR, pages 76-84, 1996. [10] T. Joachims. Optimizing search engines using clickthrough data. In KDD, pages 133-142, 2002. [11] T. Joachims. Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96. Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner. Generating query substitutions. In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram. A hierarchical monothetic document clustering algorithm for summarization and browsing search results. In WWW, pages 658-665, 2004. [14] Microsoft Live Labs. Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl. Scatter/gather browsing communicates the topic structure of a very large text collection. In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims. Query chains: learning to rank from implicit feedback. In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Commun. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai. Context-sensitive information retrieval using implicit feedback. In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen. Information Retrieval, second edition. Butterworths, London, 1979. [21] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai. Latent semantic analysis for multiple-type interrelated data objects. In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, and H. Zhang. Clustering user queries of a search engine. In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow. Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval. In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni. Web document clustering: A feasibility demonstration. In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni. Grouper: A dynamic clustering interface to web search results. Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. He, Z. Chen, W.-Y. Ma, and J. Ma. Learning to cluster web search results. In SIGIR, pages 210-217, 2004.",
    "original_translation": "Aprenda de los registros de búsqueda web para organizar los resultados de búsqueda Xuanhui Wang Departamento de Ciencias de la Computación de la Universidad de Illinois en Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu Chengxiang Zhai Departamento de Ciencias de la Computación de la Universidad de Illinois en Urbana Champaign Urbana, il 61801czhai@cs.uiuc.edu Resumen La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Los resultados de búsqueda de agrupación son una forma efectiva de organizar los resultados de búsqueda, lo que permite a un usuario navegar por documentos relevantes rápidamente. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no se corresponden necesariamente a los aspectos interesantes de un tema desde la perspectiva de los usuarios;y (2) las etiquetas de clúster generadas no son lo suficientemente informativas como para permitir que un usuario identifique el clúster correcto. En este documento, proponemos abordar estas dos deficiencias al aprender aspectos interesantes de un tema de los registros de búsqueda web y organizar los resultados de búsqueda en consecuencia;y (2) generar etiquetas de clúster más significativas utilizando palabras de consulta pasadas ingresadas por los usuarios. Evaluamos nuestro método propuesto en los datos de registro de motor de búsqueda comercial. En comparación con los métodos tradicionales para agrupar los resultados de búsqueda, nuestro método puede dar una mejor organización de resultados y etiquetas más significativas. Categorías y descriptores de sujetos: H.3.3 [Búsqueda y recuperación de información]: agrupación, proceso de búsqueda Términos generales: algoritmo, experimentación 1. Introducción La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, cómo organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de resultados de búsqueda. La estrategia más común de presentar los resultados de búsqueda es una lista simple de clasificación. Intuitivamente, dicha estrategia de presentación es razonable para resultados de búsqueda homogéneos y no ambiguos;En general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados más clasificados. Sin embargo, cuando los resultados de búsqueda son diversos (por ejemplo, debido a la ambigüedad o múltiples aspectos de un tema), como suele ser el caso en la búsqueda web, la presentación de la lista clasificada no sería efectiva;En tal caso, sería mejor agrupar los resultados de búsqueda en grupos para que un usuario pueda navegar fácilmente hacia un grupo interesante particular. Por ejemplo, los resultados en la primera página regresaron de Google para la consulta ambigua Jaguar (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de Jaguar (es decir, automóvil, animal, software y un equipo deportivo);Incluso para una consulta más refinada, como la imagen del equipo de Jaguar, los resultados siguen siendo bastante ambiguos, incluidos al menos cuatro equipos de Jaguar diferentes: un equipo de lucha libre, un equipo de autos de Jaguar, el equipo de softbol Jaguar de Jacksonville Southwestern y el equipo de fútbol Jaguar Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software Jaguar, una consulta como la descarga de Jaguar tampoco es muy efectivo ya que los resultados dominantes se tratan de descargar el folleto de Jaguar, el fondo de pantalla Jaguar y el DVD de Jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una lista simple de clasificación. La agrupación también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, un usuario tendría que pasar por una larga lista secuencialmente para llegar al primer documento relevante. Como una estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda se ha estudiado de manera relativamente amplia [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar la agrupación en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en grupos naturales, que a menudo corresponden a diferentes subtópicos del tema de consulta general. Se generará una etiqueta para indicar de qué se trata cada clúster. Un usuario puede ver las etiquetas para decidir qué clúster buscar. Se ha demostrado que dicha estrategia es más útil que la presentación de la lista clasificada simple en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que no siempre funcionan bien: primero, los grupos descubiertos de esta manera no se corresponden necesariamente a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios a menudo están interesados en encontrar códigos telefónicos o códigos postales al ingresar los códigos de área de consulta. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales grupos no serían muy útiles para los usuarios;Incluso el mejor grupo todavía tendría una precisión baja. En segundo lugar, las etiquetas de clúster generadas no son lo suficientemente informativas como para permitir que un usuario identifique el clúster correcto. Hay dos razones para este problema: (1) los grupos no se corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles.(2) Incluso si un clúster realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un clúster, y es posible que el usuario no esté muy familiarizado con algunos delos términos. Por ejemplo, la consulta ambigua Jaguar puede significar un animal o un automóvil. Un grupo puede ser etiquetado como Panthera Onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de Jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este documento, proponemos una estrategia diferente para dividir los resultados de búsqueda, que aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, tratamos de descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos ver qué tipos de páginas vistas por los usuarios en los resultados y qué tipo de palabras se usan junto con dicha consulta. En caso de que la consulta sea ambigua, como Jaguar, podemos esperar ver algunos grupos claros que corresponden diferentes sentidos de Jaguar. Más importante aún, incluso si una palabra no es ambigua (por ejemplo, automóvil), aún podemos descubrir aspectos interesantes como el alquiler de automóviles y los precios del automóvil (que resultan ser los dos aspectos principales descubiertos en nuestros datos del registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre el automóvil. Tenga en cuenta que en el caso del automóvil, los grupos generados utilizando la agrupación regular pueden no reflejar necesariamente aspectos tan interesantes sobre el automóvil desde la perspectiva de un usuario, a pesar de que los grupos generados son coherentes y significativos de otras maneras. En segundo lugar, generaremos etiquetas de clúster más significativas utilizando palabras de consulta pasadas ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dado el tema de consulta actual, también podríamos esperar que esas palabras de consulta ingresadas por usuarios en el pasado que están asociados con la consulta actual pueden proporcionar descripciones significativas de laaspectos distintos. Por lo tanto, pueden ser mejores etiquetas que las extraídas del contenido ordinario de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en registros de motores de búsqueda y creamos una colección de historial que contiene las consultas anteriores y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas de la colección de historial y aprendemos aspectos a través de la aplicación del algoritmo de agrupación Star [2] a estas consultas y clics pasados. Luego podemos organizar los resultados de búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto por la consulta pasada más representativa en el clúster de consulta. Evaluamos nuestro método para la organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y la agrupación tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consulta pasadas son más legibles que las generadas utilizando enfoques de agrupación tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos del registro de motores de búsqueda y nuestro procedimiento para construir una recopilación de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro documento y discutimos el trabajo futuro en la Sección 7. 2. Trabajo relacionado Nuestro trabajo está estrechamente relacionado con el estudio de los resultados de búsqueda de agrupación. En [9, 15], los autores usaron algoritmo de dispersión/recopilación para agrupar los documentos superiores devueltos de un sistema de recuperación de información tradicional. Sus resultados validan la hipótesis del grupo [20] que los documentos relevantes tienden a formar grupos. El mero de sistema se describió en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basado en los fragmentos o el contenido de los documentos devueltos. Se comparan varios algoritmos de agrupación y se demostró que el algoritmo de agrupación de árboles sufijo (STC) es el más efectivo. También demostraron que usar fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante de la agrupación de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron los algoritmos de aprendizaje supervisados para extraer frases significativas de los fragmentos de resultados de búsqueda y estas frases se usaron para agrupar los resultados de búsqueda. En [13], los autores propusieron usar un algoritmo de agrupación monotética, en el que se asigna un documento a un clúster basado en una sola característica, organizar los resultados de búsqueda, y la característica única se utiliza para etiquetar el clúster correspondiente. Los resultados de búsqueda de agrupación también han atraído mucha atención en los servicios web de la industria y los comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de búsqueda. Por lo tanto, los grupos obtenidos no reflejan necesariamente las preferencias de los usuarios y las etiquetas generadas pueden no ser informativas desde el punto de vista de los usuarios. Los métodos para organizar los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, un clasificador de texto está capacitado utilizando un directorio web y los resultados de búsqueda se clasifican luego en las categorías predefinidas. Los autores diseñaron y estudiaron interfaces de categoría diferentes y descubrieron que las interfaces de categoría son más efectivas que las interfaces de la lista. Sin embargo, las categorías predefinidas a menudo son demasiado generales para reflejar los aspectos de granularidad más finos de una consulta. Los registros de búsqueda han sido explotados para varios propósitos diferentes en el pasado. Por ejemplo, las consultas de búsqueda de agrupación para encontrar esas preguntas frecuentes (preguntas frecuentes) se estudian en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño del sitio web [3], análisis semántico latente [23] y funciones de clasificación de recuperación de aprendizaje [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasada para organizar mejor los resultados de búsqueda para futuras consultas. Utilizamos el algoritmo de clúster Star [2], que es un enfoque basado en la partición gráfica, para aprender aspectos interesantes de los registros de búsqueda dada una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de manera específica de consulta y esta es otra diferencia de los trabajos anteriores, como [24, 4] en el que todas las consultas en los registros se agrupan de manera lota.3. Registros de motores de búsqueda Los registros de los motores de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses de los usuarios reales al realizar la consulta de identificación URL TIME 1 Gane zip http://www.winzip.com XXXX 1 Win Zip http: //www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Entradas de muestra de registros de motores de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda Web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL en las que hicieron clic después de enviar las consultas y el momento en que hicieron clic. Los registros de los motores de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL en las que un usuario hizo clic después de emitir la consulta [24]. En la Tabla 1 se muestra una pequeña muestra de datos de registro de búsqueda. Nuestra idea de usar registros de motores de búsqueda es tratar estos registros como historial pasado, aprender los intereses de los usuarios utilizando estos datos del historial automáticamente y representar sus intereses por consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con el automóvil y esto refleja que una gran cantidad de usuarios están interesados en la información sobre el automóvil. Probablemente diferentes usuarios estén interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un automóvil, por lo que pueden enviar una consulta como el alquiler de automóviles;Algunos están más interesados en comprar un automóvil usado, y pueden enviar una consulta como un automóvil usado;Y a otros pueden preocuparse más por comprar un accesorio para automóvil, por lo que pueden usar una consulta como Audio de automóviles. Al extraer todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente sean interesantes desde la perspectiva de los usuarios. Como ejemplo, los siguientes son algunos aspectos sobre el automóvil aprendido de nuestros datos del registro de búsqueda (ver Sección 5).1. Alquiler de automóviles, alquiler de automóviles Hertz, alquiler de automóviles empresariales, ... 2. Precios de automóviles, automóvil usado, valores de automóvil, ... 3. accidentes automovilísticos, accidentes automovilísticos, naufragios de automóviles, ... 4. Audio de automóvil, automóvilestéreo, altavoz de coche, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros sin procesar para crear una recopilación de datos del historial. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de la página web que se hace clic, junto con el momento en que el usuario hizo los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas como para decir el significado previsto de una consulta enviada con precisión. Para recopilar información rica, enriquecemos cada URL con contenido de texto adicional. Específicamente, dada la consulta en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del que obtuvimos nuestros datos de registro y extraemos los fragmentos de las URL que se hacen clic de acuerdo con la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web que hacen clic de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la escasez de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas en sí mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra recopilación de datos de historia, que se utiliza para aprender aspectos interesantes en la siguiente sección.4. Nuestro enfoque Nuestro enfoque es organizar los resultados de búsqueda por aspectos aprendidos de los registros de motores de búsqueda. Dada una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtenga su información relacionada de los registros de motores de búsqueda. Toda la información forma un conjunto de trabajo.2. Aprenda aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dada la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa.3. Clasifique y organice los resultados de búsqueda de la consulta de entrada de acuerdo con los aspectos aprendidos anteriormente. Ahora damos una presentación detallada de cada paso.4.1 Encontrar consultas anteriores relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber qué están realmente interesados a los usuarios dada esta consulta, primero recuperamos sus consultas similares en nuestra recopilación de datos del historial preprocesado. Formalmente, supongamos que tenemos n pseudo-documentos en nuestro conjunto de datos del historial: h = {q1, q2, ..., qn}. Cada Qi corresponde a una consulta única y se enriquece con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con QS en H, una forma natural es usar un algoritmo de recuperación de texto. Aquí usamos el método Okapi [17], uno de los métodos de recuperación de última generación. Específicamente, usamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi: w∈Q ¡qi c (w, q) × idf (w) × (k1 + 1) × c (w, qi) k1((1 - b) + b | qi | avdl) + c (w, qi) donde k1 y b son parámetros de okapi establecidos empíricamente, c (w, qi) y c (w, q) son el recuento de palabras w enQi y Q respectivamente, IDF (W) es la frecuencia de documento inversa de la palabra W, y AVDL es la longitud promedio del documento en nuestra colección de historia. Según los puntajes de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos que los usuarios generalmente están interesados. Cada documento en H corresponde a una consulta pasada y, por lo tanto, los documentos de alto nivel corresponden a consultas pasadas relacionadas con QS.4.2 Aspectos de aprendizaje por agrupación Dada una consulta Q, usamos HQ = {D1, ..., DN} para representar los pseudo-documentos clasificados de la colección del historial H. Estos pseudo-documentos contienen los aspectos que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupación para descubrir estos aspectos. Cualquier algoritmo de agrupación podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en la partición gráfica: el algoritmo de agrupación de estrellas [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir una buena etiqueta para cada clúster de forma natural. Describimos el algoritmo de agrupación de estrellas a continuación.4.2.1 Agrupación de estrellas Dada HQ, Star Clustering comienza con la construcción de un gráfico de similitud de pareja en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Luego, los grupos están formados por subgrafías densas que tienen forma de estrella. Estos grupos forman una cubierta del gráfico de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} En la colección HQ, calculamos un vector TF-IDF. Luego, para cada par de documentos DI y DJ (i = j), su similitud se calcula como la puntuación coseno de sus vectores correspondientes VI y VJ, es decir, sim (di, dj) = cos (vi, vj) = vi ·VJ | VI |· | VJ |. Un gráfico de similitud Gσ se puede construir de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento DI es un vértice de Gσ. Si SIM (DI, DJ)> σ, habría un borde que conecta los dos vértices correspondientes. Después de construir el gráfico de similitud Gσ, el algoritmo de agrupación de estrellas agrupa los documentos utilizando un algoritmo codicioso de la siguiente manera: 1. Asociar cada vértice en Gσ con una bandera, inicializado como sin marcar.2. De esos vértices sin marcar, encuentre el que tiene el más alto grado y deje que sea u.3. Marque la bandera de u como centro.4. Forma un grupo C que contiene U y todos sus vecinos que no están marcados como centro. Marque todos los vecinos seleccionados como satélites.5. Repita desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo tiene forma de estrella, que consiste en un solo centro y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupación de estrellas. Un gran σ aplica que los documentos conectados tienen altas similitudes y, por lo tanto, los grupos tienden a ser pequeños. Por otro lado, un pequeño σ hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de clúster Star es que genera un centro para cada clúster. En el pasado de la colección de consultas, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el clúster y, por lo tanto, proporciona una etiqueta para el clúster de forma natural. Todos los grupos obtenidos están relacionados con la consulta de entrada Q desde diferentes perspectivas, y representan los posibles aspectos de los intereses sobre la consulta Q de los usuarios.4.3 Clasificación de resultados de búsqueda Para organizar los resultados de búsqueda de acuerdo con los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas anteriores relacionadas para clasificar los resultados de búsqueda. Dadas las páginas web TOP M devueltas por un motor de búsqueda para Q: {S1, ..., SM}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización se puede usar aquí. Aquí utilizamos un método simple basado en centroides para la categorización. Naturalmente, se puede esperar que los métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basado en los pseudo-documentos en cada aspecto descubierto CI, construimos un prototipo de Centroide Pi tomando el promedio de todos los vectores de los documentos en CI: PI = 1 | CI |l∈Ci vl. Todos estos PI se utilizan para clasificar los resultados de búsqueda. Específicamente, para cualquier resultado de búsqueda SJ, construimos un vector TF-IDF. El método basado en centroides calcula la similitud cosena entre la representación vectorial de SJ y cada prototipo Centroide PI. Luego asignamos SJ al aspecto con el que tiene el puntaje de similitud de coseno más alto. Todos los aspectos finalmente se clasifican de acuerdo con el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican de acuerdo con su clasificación original del motor de búsqueda.5. Recopilación de datos Construimos nuestro conjunto de datos en función del conjunto de datos de registro de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, este registro de datos abarca 31 días desde el 01/05/2006 al 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, separamos todo el conjunto de datos en dos partes de acuerdo con la hora: los primeros 2/3 de datos se utilizan para simular los datos históricos que un motor de búsqueda acumuló, y usamos los últimos 1/3 para simular consultas futuras. En la recopilación de la historia, limpiamos los datos solo manteniendo esas consultas en inglés frecuentes, bien formadas y bien formadas (consultas que solo contienen los caracteres A, B, ..., Z y el espacio, y aparecen más de 5 veces). Después de la limpieza, obtenemos 169,057 consultas únicas en nuestra recopilación de datos de historia totalmente. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra recopilación de historial es de 129 MB. Construimos nuestros datos de prueba a partir de los últimos 1/3 de datos. Según el tiempo, separamos estos datos en dos conjuntos de pruebas por igual para la validación cruzada para establecer parámetros. Para cada conjunto de pruebas, usamos cada sesión como caso de prueba. Cada sesión contiene una sola consulta y varios clics.(Tenga en cuenta que no agregamos sesiones para casos de prueba. Diferentes casos de prueba pueden tener las mismas consultas pero posiblemente diferentes clics). Dado que es inviable pedirle al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por usar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda para ver qué tan bien estos algoritmos pueden ayudar a los usuarios a alcanzar las URL en los que se hacen clic. Se espera que la organización de los resultados de búsqueda en diferentes aspectos ayude a consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos esos casos de prueba con menos de 4 clics bajo el supuesto de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no pueden recuperar al menos 100 pseudocdocumentos de nuestra colección de historial. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente.6. Experimentos En la sección, describimos nuestros experimentos en la organización de resultados de búsqueda basados en registros de motores de búsqueda anteriores.6.1 Diseño experimental Utilizamos dos métodos de referencia para evaluar el método propuesto para organizar los resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (línea de base). El segundo método es organizar los resultados de búsqueda agrupándolos (basados en clúster). Para una comparación justa, utilizamos el mismo algoritmo de agrupación que nuestro método de base log (es decir, agrupación estelar). Es decir, tratamos cada resultado de la búsqueda como un documento, construimos el gráfico de similitud y encontramos los grupos en forma de estrella. Comparamos nuestro método (basado en log) con los dos métodos de referencia en los siguientes experimentos. Para los métodos basados en clúster y basados en log, los resultados de búsqueda dentro de cada clúster se clasifican en función de su clasificación original dada por el motor de búsqueda. Para comparar los diferentes métodos de organización de resultados, adoptamos un método similar al del documento [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como el que tiene el mayor número de documentos relevantes. Organizar los resultados de búsqueda en clústeres es ayudar a los usuarios a navegar en documentos relevantes rápidamente. La métrica anterior es simular un escenario cuando los usuarios siempre eligen el clúster correcto y lo analicen. Específicamente, descargamos y organizamos los 100 mejores resultados de búsqueda en aspectos para cada caso de prueba. Usamos precisión en 5 documentos (p@5) en el mejor grupo como medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos dice la precisión percibida cuando el usuario abre un clúster y mira los primeros 5 documentos. También utilizamos el rango recíproco medio (MRR) como otra métrica. MRR se calcula como MRR = 1 | T |q∈T 1 rq donde t es un conjunto de consultas de prueba, RQ es el rango del primer documento relevante para q. Para dar una comparación justa entre diferentes algoritmos de organización, forzamos métodos basados en clúster y basados en log para producir el mismo número de aspectos y obligar a cada resultado de la búsqueda a estar en un solo aspecto. El número de aspectos se fija en 10 en todos los siguientes experimentos. El algoritmo de agrupación STAR puede generar diferentes grupos de grupos para diferentes entradas. Para restringir el número de grupos a 10, ordenamos todos los grupos por sus tamaños, seleccione los 10 principales como candidatos de aspecto. Luego reasignamos cada resultado de la búsqueda a uno de estos 10 aspectos seleccionados que tiene la puntuación de mayor similitud con el centroide de aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores que 5, y esto asegura que P@5 sea una métrica significativa.6.2 Resultados experimentales Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una lista simple o resultados de búsqueda de clúster. A continuación, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Método de prueba Conjunto de prueba 1 Conjunto de prueba 2 mmr p@5 mmr p@5 línea de base 0.7347 0.3325 0.7393 0.3288 0.7735 0.3162 0.7666 0.2994 0.7833 0.3534 0.7697 0.33389 clúster/basal 5.28% -4.87% 3.69% --8.9.93.93% en línea6.62% 6.31% 4.10% 3.09% log/clúster 1.27% 11.76% 0.40% 13.20% Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de prueba de comparación 1 Conjunto de prueba 2 impr./DECR. Impr./Decr. Clúster/Baseleta 53/55 50/64 Log/Basora 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación por pares W.R.T El número de casos de prueba cuyos P@5s mejoran versus disminución de W.R.T la línea de base.6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica del motor de búsqueda (línea de base), método basado en la agrupación tradicional (basado en clúster) y nuestro método basado en log (basado en registros), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección individualmente en función de los valores P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que la línea de base y los métodos basados en clúster. Por ejemplo, en la primera colección de pruebas, el método de referencia de MMR es 0.734, el método basado en clúster es 0.773 y nuestro método es 0.783. Logramos una mayor precisión que el método basado en clúster (1,27% de mejora) y el método de referencia (mejora del 6,62%). Los valores P@5 son 0.332 para la línea de base, 0.316 para el método basado en clúster, pero 0.353 para nuestro método. Nuestro método mejora sobre la línea de base en un 6,31%, mientras que el método basado en clúster incluso disminuye la precisión. Esto se debe a que el método basado en clúster organiza los resultados de búsqueda solo en función del contenido. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis del sesgo del método basado en clúster. Comparando nuestro método con el método basado en clúster, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto muestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios del historial de consultas anteriores y, por lo tanto, puede organizar los resultados de búsqueda de una manera más útil a los usuarios. Mostramos los resultados óptimos anteriores. Para probar la sensibilidad del parámetro σ de nuestro método basado en log, usamos uno de los conjuntos de pruebas para ajustar el parámetro para que sea óptimo y luego usamos el parámetro sintonizado en el otro conjunto. Comparamos este resultado (registro afuera afuera) con los resultados óptimos de los métodos basados en clúster (optimizados en clúster) y basados en log (optimizado de registro) en la Figura 1. Podemos ver que, como se esperaba, el rendimiento que usa el parámetro sintonizado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método aún funciona mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas.0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de prueba 1 Conjunto de prueba 2 P@5 Registro optimizado de registro optimizado ajustado fuera de la Figura 1: Resultados usando parámetros sintonizados de la otra colección de pruebas. Lo comparamos con el rendimiento óptimo de los métodos basados en clúster y nuestros métodos logbased.0 10 20 30 30 40 50 60 1 2 3 4 Número de contenedor #Queries Mejoró la Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos comparaciones por pares de los tres métodos en términos de los números de casos de prueba para los cuales P@5 aumenta en lugar de disminuir. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden mejorarse con nuestro método.6.2.2 Análisis detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultades de consulta. Todo el análisis a continuación se basa en el conjunto de pruebas 1. Análisis de diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para esas consultas cuyos resultados son más diversos, como para tales consultas, los resultados tienden a formar dos o más grupos grandes. Para probar la hipótesis de que el método basado en log ayuda más esas consultas con diversos resultados, calculamos las relaciones de tamaño de los grupos más grandes y más grandes en nuestros resultados basados en log y utilizamos esta relación como un indicador de diversidad. Si la relación es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo que los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de relación [1, 2), [2, 3), [3, 4) y [4, +∞) respectivamente.([i, j) significa que yo ≤ relación <j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5s se mejoran en lugar de disminuir con respecto a la línea de base de clasificación y trazar los números en esta figura. Podemos observar que cuando la relación es más pequeña, el método basado en log puede mejorar más casos de prueba. Pero cuando 0 5 10 15 20 25 30 1 2 3 4 Número de contenedor #Quiteries mejoró la Figura 3: La correlación entre el cambio de rendimiento y la dificultad de la consulta.La relación es grande, el método basado en log no puede mejorar sobre la línea de base. Por ejemplo, en Bin 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en Bin 4, todos los 4 casos de prueba disminuyen. Esto confirma nuestra hipótesis de que nuestro método puede ayudar más si la consulta tiene resultados más diversos. Esto también sugiere que debemos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la relación de tamaño del clúster). Análisis de dificultad: se han estudiado consultas difíciles en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método para ayudar a consultas difíciles. Cuantificamos la dificultad de consulta por la precisión promedio media (MAP) de la clasificación del motor de búsqueda original para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de prueba 1 en un orden creciente de los valores de MAP. Participamos los casos de prueba en 4 contenedores con cada uno con un número aproximadamente igual de casos de prueba. Un pequeño mapa significa que la utilidad de la clasificación original es baja. Bin 1 contiene esos casos de prueba con los mapas más bajos y el contenedor 4 contiene esos casos de prueba con los mapas más altos. Para cada contenedor, calculamos el número de casos de prueba cuyos P@5s mejoran en lugar de disminuir. La Figura 3 muestra los resultados. Claramente, en Bin 1, la mayoría de los casos de prueba mejoran (24 frente a 3), mientras que en Bin 4, el método basado en log puede disminuir el rendimiento (3 vs 20). Esto muestra que nuestro método es más beneficioso para las consultas difíciles, lo cual es el esperado ya que agrupar los resultados de búsqueda está destinado a ayudar a consultas difíciles. Esto también muestra que nuestro método realmente no ayuda a consultas fáciles, por lo que debemos desactivar nuestra opción de organización para consultas fáciles.6.2.3 Configuración de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupación de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación de Okapi, estudiamos los parámetros K1 y B. También estudiamos el impacto del número de consultas anteriores recuperadas en nuestro método basado en log. La Figura 4 muestra el impacto del parámetro σ para los métodos basados en clúster y basados en log en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con el paso 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros OKAPI. Variamos K1 de 1.0 a 2.0 con el paso 0.2 y B de 0 a 1 con el paso 0.2. A partir de esta tabla, está claro que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores que 0.35. Los valores predeterminados K1 = 1.2 y B = 0.8 dan resultados aproximadamente óptimos. Además, estudiamos el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 p@5 Similitud TRAVEDIumbral de similitud σ en métodos basados en clúster y basados en log. Mostramos el resultado en ambas colecciones de pruebas.B 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.34104 0.3546 0.3546 1.6 0.6 0.6 0.34476 0.3448 0.3448 3 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546Tabla 4: Impacto de los parámetros de Okapi K1 y B.Información para aprender variando el número de consultas pasadas que se recuperarán para los aspectos de aprendizaje. Los resultados en ambas colecciones de prueba se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que ampliamos el número de consultas anteriores recuperadas. Por lo tanto, nuestro método podría aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, cada vez más consultas tendrán suficiente historia, por lo que podemos mejorar más y más consultas.6.2.4 Un ejemplo ilustrativo Usamos los códigos de área de consulta para mostrar la diferencia en los resultados del método basado en log y el método basado en clúster. Esta consulta puede significar códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres grupos más grandes de ambos métodos. En el método basado en clúster, los resultados se dividen en función de ubicaciones: local o internacional. En el método basado en registros, los resultados se desambigan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que usan dicha consulta a menudo están interesados en códigos telefónicos o códigos postales.Dado que los valores P@5 de los métodos basados en clúster y basados en log son 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia sus resultados deseados. Método basado en clúster Método basado en el registro Ciudad, teléfono estatal, ciudad, local internacional, teléfono de área, marcación de zip internacional, Tabla postal 5: un ejemplo que muestra la diferencia entre el método basado en clúster y nuestro método basado en log 0.16 0.18 0.2 0.220.24 0.26 0.28 0.3 1501201008050403020 P@5 #Queries Recuperado Conjunto de prueba 1 Test Set 2 Figura 5: El impacto del número de consultas anteriores recuperadas.6.2.5 Comparación de etiquetado Ahora comparamos las etiquetas entre el método basado en clúster y el método basado en log. El método basado en clúster debe confiar en las palabras clave extraídas de los fragmentos para construir la etiqueta para cada clúster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clúster, contamos la frecuencia de una palabra clave que aparece en un clúster y usamos las palabras clave más frecuentes como la etiqueta del clúster. Para el método basado en log, utilizamos el centro de cada clúster Star como la etiqueta para el clúster correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de clúster automáticamente. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clúster y los métodos basados en log. En la Tabla 6, enumeramos las etiquetas de los 5 mejores grupos para dos ejemplos Jaguar y Apple. Para el método basado en clúster, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en consultas de usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupación. Comparación de etiquetas para consulta Jaguar Método basado en log Método basado en clúster 1. Jaguar Animal 1. Jaguar, Auto, Accesorios 2. Jaguar Accesorios automáticos 2. Jaguar, Tipo, Precios 3. Jaguar Cats 3. Jaguar, Pantera, Cats 4. JaguarReparación 4. Jaguar, Servicios, Boston 5. Jaguar Animal Pictures 5. Jaguar, Colección, comparación de etiquetas de ropa para consulta Método basado en el clúster basado en el registro de Apple 1. Computadora de Apple 1. Apple, soporte, producto 2. Apple iPod 2.Apple, sitio, computadora 3. Receta de Apple Crisp 3. Apple, Mundo, Visita 4. Pastel de Apple Fresh 4. Apple, iPod, Amazon 5. Laptop de Apple 5. Apple, Productos, Noticias Tabla 6: Comparación de etiquetas de clúster.7. Conclusiones y trabajo futuro En este documento, estudiamos el problema de organizar los resultados de búsqueda de manera orientada al usuario. Para alcanzar este objetivo, confiamos en registros de motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas del historial de consultas pasado, aprendemos los aspectos agrupando las consultas pasadas y la información de clics asociada y clasificamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clúster y la línea de base de la clasificación del motor de búsqueda. Los experimentos muestran que nuestro método basado en log puede superar constantemente el método basado en clúster y mejorar la línea de base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspecto más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda cuando clúster los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: primero, aunque los resultados de nuestros experimentos han mostrado claramente la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. En segundo lugar, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información informativa de comentarios de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Por lo tanto, sería interesante estudiar cómo mejorar aún más la organización de los resultados en función de dicha información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual.8. Agradecimientos Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo está en parte respaldado por una beca de investigación de Microsoft Live Labs, una beca de investigación en Google y una subvención de carrera NSF IIS-0347933.9. Referencias [1] E. Agichtein, E. Brill y S. T. Dumais. Mejora de la clasificación de búsqueda web incorporando información de comportamiento del usuario. En Sigir, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupación de estrellas para la organización de información estática y dinámica. Journal of Graph Algorithms and Applications, 8 (1): 95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Clustering aglomerativo de un registro de consulta de motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En Sigir, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Llevar el orden a la web: categorizar automáticamente los resultados de búsqueda. En Chi, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predecir el rendimiento de la consulta. En Actas de ACM Sigir 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimización de la búsqueda mostrando resultados en contexto. En Chi, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del grupo: dispersión/recolección en los resultados de recuperación. En Sigir, páginas 76-84, 1996. [10] T. Joachims. Optimización de los motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de la recuperación utilizando datos de clics., Páginas 79-96. Physica/Springer Verlag, 2003. En J. Franke y G. Nakhaeizadeh e I. Renz, minería de texto.[12] R. Jones, B. Rey, O. Madani y W. Greiner. Generación de sustituciones de consulta. En www, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo de agrupación de documentos monotéticos jerárquicos para resumir y navegar por los resultados de búsqueda. En www, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerar la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/rfps/ Buscar 2006 rfp.aspx.[15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recopilación comunica la estructura del tema de una colección de texto muy grande. En Chi, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: Aprender a clasificarse a partir de comentarios implícitos. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples efectivas al modelo de 2 poisson para la recuperación ponderada probabilística. En Sigir, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para la indexación automática. Comun. ACM, 18 (11): 613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando comentarios implícitos. En Sigir, páginas 43-50, 2005. [20] C. J. Van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo.http://vivisimo.com/.[23] X. Wang, J.-T.Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de tipo múltiple. En Sigir, páginas 236-243, 2006. [24] J.-R.Wen, J.-Y. Nie y H. Zhang. Agrupación de consultas de usuario de un motor de búsqueda. En www, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprender a estimar la dificultad de consulta: incluidas las aplicaciones para faltar detección de contenido y recuperación de información distribuida. En Sigir, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Clustering de documentos web: una demostración de factibilidad. En Sigir, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: una interfaz de agrupación dinámica para los resultados de búsqueda web. Computer Networks, 31 (11-16): 1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma y J. Ma. Aprender a agrupar los resultados de búsqueda web. En Sigir, páginas 210-217, 2004.",
    "original_sentences": [
        "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
        "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
        "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
        "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
        "We evaluate our proposed method on a commercial search engine log data.",
        "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
        "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
        "INTRODUCTION The utility of a search engine is affected by multiple factors.",
        "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
        "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
        "The most common strategy of presenting search results is a simple ranked list.",
        "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
        "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
        "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
        "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
        "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
        "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
        "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
        "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
        "A label will be generated to indicate what each cluster is about.",
        "A user can then view the labels to decide which cluster to look into.",
        "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
        "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
        "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
        "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
        "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
        "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
        "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
        "For example, the ambiguous query jaguar may mean an animal or a car.",
        "A cluster may be labeled as panthera onca.",
        "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
        "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
        "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
        "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
        "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
        "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
        "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
        "Such aspects can be very useful for organizing future search results about car.",
        "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
        "Second, we will generate more meaningful cluster labels using past query words entered by users.",
        "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
        "Thus they can be better labels than those extracted from the ordinary contents of search results.",
        "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
        "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
        "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
        "We evaluate our method for result organization using logs of a commercial search engine.",
        "We compare our method with the default search engine ranking and the traditional clustering of search results.",
        "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
        "The rest of the paper is organized as follows.",
        "We first review the related work in Section 2.",
        "In Section 3, we describe search engine log data and our procedure of building a history collection.",
        "In Section 4, we present our approach in details.",
        "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
        "Finally, we conclude our paper and discuss future work in Section 7. 2.",
        "RELATED WORK Our work is closely related to the study of clustering search results.",
        "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
        "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
        "The system Grouper was described in [26, 27].",
        "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
        "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
        "They also showed that using snippets is as effective as using whole documents.",
        "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
        "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
        "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
        "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
        "However, in all these works, the clusters are generated solely based on the search results.",
        "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
        "Methods of organizing search results based on text categorization are studied in [6, 8].",
        "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
        "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
        "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
        "Search logs have been exploited for several different purposes in the past.",
        "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
        "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
        "In our work, we explore past query history in order to better organize the search results for future queries.",
        "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
        "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
        "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
        "Different IDs mean different sessions.",
        "Web search.",
        "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
        "Search engine logs are separated by sessions.",
        "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
        "A small sample of search log data is shown in Table 1.",
        "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
        "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
        "Different users are probably interested in different aspects of car.",
        "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
        "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
        "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
        "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
        "As shown above, search engine logs consist of sessions.",
        "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
        "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
        "To gather rich information, we enrich each URL with additional text content.",
        "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
        "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
        "Different sessions may contain the same queries.",
        "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
        "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
        "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
        "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
        "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
        "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
        "Given an input query, the general procedure of our approach is: 1.",
        "Get its related information from search engine logs.",
        "All the information forms a working set. 2.",
        "Learn aspects from the information in the working set.",
        "These aspects correspond to users interests given the input query.",
        "Each aspect is labeled with a representative query. 3.",
        "Categorize and organize the search results of the input query according to the aspects learned above.",
        "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
        "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
        "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
        "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
        "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
        "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
        "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
        "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
        "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
        "In this subsection, we propose to use a clustering method to discover these aspects.",
        "Any clustering algorithm could be applied here.",
        "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
        "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
        "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
        "Then the clusters are formed by dense subgraphs that are star-shaped.",
        "These clusters form a cover of the similarity graph.",
        "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
        "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
        "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
        "Each document di is a vertex of Gσ.",
        "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
        "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
        "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
        "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
        "Mark the flag of u as center. 4.",
        "Form a cluster C containing u and all its neighbors that are not marked as center.",
        "Mark all the selected neighbors as satellites. 5.",
        "Repeat from step 2 until all the vertices in Gσ are marked.",
        "Each cluster is star-shaped, which consists a single center and several satellites.",
        "There is only one parameter σ in the star clustering algorithm.",
        "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
        "On the other hand, a small σ will make the clusters big and less coherent.",
        "We will study the impact of this parameter in our experiments.",
        "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
        "In the past query collection Hq, each document corresponds to a query.",
        "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
        "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
        "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
        "In principle, any categorization algorithm can be used here.",
        "Here we use a simple centroid-based method for categorization.",
        "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
        "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
        "All these pis are used to categorize the search results.",
        "Specifically, for any search result sj, we build a TF-IDF vector.",
        "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
        "We then assign sj to the aspect with which it has the highest cosine similarity score.",
        "All the aspects are finally ranked according to the number of search results they have.",
        "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
        "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
        "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
        "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
        "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
        "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
        "After cleaning, we get 169,057 unique queries in our history data collection totally.",
        "On average, each query has 3.5 distinct clicks.",
        "We build the pseudo-documents for all these queries as described in Section 3.",
        "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
        "We construct our test data from the last 1/3 data.",
        "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
        "For each test set, we use every session as a test case.",
        "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
        "Different test cases may have the same queries but possibly different clicks.)",
        "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
        "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
        "Organizing search results into different aspects is expected to help informational queries.",
        "It thus makes sense to focus on the informational queries in our evaluation.",
        "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
        "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
        "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
        "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
        "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
        "For each test case, the first method is the default ranked list from a search engine (baseline).",
        "The second method is to organize the search results by clustering them (cluster-based).",
        "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
        "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
        "We compare our method (log-based) with the two baseline methods in the following experiments.",
        "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
        "To compare different result organization methods, we adopt a similar method as in the paper [9].",
        "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
        "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
        "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
        "Specifically, we download and organize the top 100 search results into aspects for each test case.",
        "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
        "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
        "We also use Mean Reciprocal Rank (MRR) as another metric.",
        "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
        "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
        "The number of aspects is fixed at 10 in all the following experiments.",
        "The star clustering algorithm can output different number of clusters for different input.",
        "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
        "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
        "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
        "In the following, we test our hypothesis from two perspectives - organization and labeling.",
        "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
        "We also show the percentage of relative improvement in the lower part.",
        "Comparison Test set 1 Test set 2 Impr./Decr.",
        "Impr./Decr.",
        "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
        "We optimize the parameter σs for each collection individually based on P@5 values.",
        "This shows the best performance that each method can achieve.",
        "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
        "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
        "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
        "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
        "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
        "This is because cluster-based method organizes the search results only based on the contents.",
        "Thus it could organize the results differently from users preferences.",
        "This confirms our hypothesis of the bias of the cluster-based method.",
        "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
        "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
        "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
        "We showed the optimal results above.",
        "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
        "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
        "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
        "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
        "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
        "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
        "We can see that our method improves more test cases compared with the other two methods.",
        "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
        "All the analysis below is based on test set 1.",
        "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
        "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
        "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
        "In this case, we would expect our method to help more.",
        "The results are shown in Figure 2.",
        "In this figure, we partition the ratios into 4 bins.",
        "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
        "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
        "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
        "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
        "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
        "But in bin 4, all the 4 test cases are decreased.",
        "This confirms our hypothesis that our method can help more if the query has more diverse results.",
        "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
        "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
        "Here we analyze the effectiveness of our method in helping difficult queries.",
        "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
        "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
        "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
        "A small MAP means that the utility of the original ranking is low.",
        "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
        "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
        "Figure 3 shows the results.",
        "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
        "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
        "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
        "For the star clustering algorithm, we study the similarity threshold parameter σ.",
        "For the OKAPI retrieval function, we study the parameters k1 and b.",
        "We also study the impact of the number of past queries retrieved in our log-based method.",
        "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
        "We vary σ from 0.05 to 0.3 with step 0.05.",
        "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
        "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
        "In Table 4, we show the impact of OKAPI parameters.",
        "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
        "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
        "Most of the values are larger than 0.35.",
        "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
        "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
        "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
        "The results on both test collections are shown in Figure 5.",
        "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
        "Thus our method could potentially learn more as we accumulate more history.",
        "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
        "This query may mean phone codes or zip codes.",
        "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
        "In the clusterbased method, the results are partitioned based on locations: local or international.",
        "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
        "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
        "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
        "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
        "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
        "Our log-based method can avoid this difficulty by taking advantage of queries.",
        "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
        "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
        "In general, it is not easy to quantify the readability of a cluster label automatically.",
        "We use examples to show the difference between the cluster-based and the log-based methods.",
        "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
        "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
        "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
        "This is another advantage of our way of organizing search results over the clustering approach.",
        "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
        "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
        "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
        "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
        "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
        "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
        "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
        "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
        "It would be interesting to explore other potentially more effective methods.",
        "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
        "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
        "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
        "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
        "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
        "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
        "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
        "Improving web search ranking by incorporating user behavior information.",
        "In SIGIR, pages 19-26, 2006. [2] J.",
        "A. Aslam, E. Pelekov, and D. Rus.",
        "The star clustering algorithm for static and dynamic information organization.",
        "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
        "Applications of web query mining.",
        "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
        "Agglomerative clustering of a search engine query log.",
        "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
        "What makes a query difficult?",
        "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
        "Bringing order to the web: automatically categorizing search results.",
        "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
        "Predicting query performance.",
        "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
        "Optimizing search by showing results in context.",
        "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
        "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
        "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
        "Optimizing search engines using clickthrough data.",
        "In KDD, pages 133-142, 2002. [11] T. Joachims.",
        "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
        "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
        "Generating query substitutions.",
        "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
        "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
        "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
        "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
        "Scatter/gather browsing communicates the topic structure of a very large text collection.",
        "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
        "Query chains: learning to rank from implicit feedback.",
        "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
        "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
        "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
        "A vector space model for automatic indexing.",
        "Commun.",
        "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
        "Context-sensitive information retrieval using implicit feedback.",
        "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
        "Information Retrieval, second edition.",
        "Butterworths, London, 1979. [21] V. N. Vapnik.",
        "The Nature of Statistical Learning Theory.",
        "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
        "Latent semantic analysis for multiple-type interrelated data objects.",
        "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
        "Nie, and H. Zhang.",
        "Clustering user queries of a search engine.",
        "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
        "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
        "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
        "Web document clustering: A feasibility demonstration.",
        "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
        "Grouper: A dynamic clustering interface to web search results.",
        "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
        "Zeng, Q.-C.",
        "He, Z. Chen, W.-Y.",
        "Ma, and J. Ma.",
        "Learning to cluster web search results.",
        "In SIGIR, pages 210-217, 2004."
    ],
    "error_count": 0,
    "keys": {
        "retrieval model": {
            "translated_key": "modelo de recuperación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying <br>retrieval model</br> and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Si bien el factor principal es la solidez del \"modelo de recuperación\" subyacente y la función de clasificación, cómo organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda."
            ],
            "translated_text": "",
            "candidates": [
                "modelo de recuperación",
                "modelo de recuperación"
            ],
            "error": []
        },
        "ranking function": {
            "translated_key": "función de clasificación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and <br>ranking function</br>, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Si bien el factor principal es la solidez del modelo de recuperación subyacente y la \"función de clasificación\", cómo organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda."
            ],
            "translated_text": "",
            "candidates": [
                "función de clasificación",
                "función de clasificación"
            ],
            "error": []
        },
        "ambiguity": {
            "translated_key": "ambigüedad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to <br>ambiguity</br> or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Sin embargo, cuando los resultados de búsqueda son diversos (por ejemplo, debido a la \"ambigüedad\" o múltiples aspectos de un tema), como suele ser el caso en la búsqueda web, la presentación de la lista clasificada no sería efectiva;En tal caso, sería mejor agrupar los resultados de búsqueda en grupos para que un usuario pueda navegar fácilmente hacia un grupo interesante particular."
            ],
            "translated_text": "",
            "candidates": [
                "ambigüedad",
                "ambigüedad"
            ],
            "error": []
        },
        "clustering view": {
            "translated_key": "vista de agrupación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a <br>clustering view</br> of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En estos ejemplos, una \"vista de agrupación\" de los resultados de búsqueda sería mucho más útil para un usuario que una lista simple de clasificación."
            ],
            "translated_text": "",
            "candidates": [
                "vista de agrupación",
                "vista de agrupación"
            ],
            "error": []
        },
        "meaningful cluster label": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more <br>meaningful cluster label</br>s using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more <br>meaningful cluster label</br>s using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En este documento, proponemos abordar estas dos deficiencias al aprender aspectos interesantes de un tema de los registros de búsqueda web y organizar los resultados de búsqueda en consecuencia;y (2) generar más \"etiqueta de clúster significativa\" utilizando palabras de consulta pasadas ingresadas por los usuarios.",
                "En segundo lugar, generaremos más \"etiquetas de clúster significativas\" utilizando palabras de consulta pasadas ingresadas por los usuarios."
            ],
            "translated_text": "",
            "candidates": [
                "etiqueta de clúster significativa",
                "etiqueta de clúster significativa",
                "etiqueta de clúster significativa",
                "etiquetas de clúster significativas"
            ],
            "error": []
        },
        "history collection": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a <br>history collection</br> containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the <br>history collection</br> and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a <br>history collection</br>.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our <br>history collection</br>.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the <br>history collection</br> H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the <br>history collection</br>, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our <br>history collection</br> is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our <br>history collection</br>.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Para implementar las ideas presentadas anteriormente, confiamos en registros de motores de búsqueda y creamos una \"colección de historia\" que contiene las consultas anteriores y los clics asociados.",
                "Dada una nueva consulta, encontramos sus consultas anteriores relacionadas de la \"Colección de Historia\" y aprendemos aspectos a través de la aplicación del algoritmo de agrupación de estrellas [2] a estas consultas y clics pasados.",
                "En la Sección 3, describimos los datos del registro del motor de búsqueda y nuestro procedimiento para construir una \"recopilación de historial\".",
                "Específicamente, usamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi: w∈Q ¡qi c (w, q) × idf (w) × (k1 + 1) × c (w, qi) k1((1 - b) + b | qi | avdl) + c (w, qi) donde k1 y b son parámetros de okapi establecidos empíricamente, c (w, qi) y c (w, q) son el recuento de palabras w enQi y Q respectivamente, IDF (W) es la frecuencia de documento inversa de la palabra W, y AVDL es la longitud promedio del documento en nuestra \"colección de historia\".",
                "Cada documento en H corresponde a una consulta pasada y, por lo tanto, los documentos de alto nivel corresponden a consultas pasadas relacionadas con QS.4.2 Aspectos de aprendizaje por agrupación Dada una consulta q, usamos hq = {d1, ..., dn} para representar los pseudo-documentos clasificados de la \"colección de historial\" H. Estos pseudo-documentos contienen los aspectos que los usuarios están interesadosen.",
                "En la \"Colección de Historia\", limpiamos los datos solo manteniendo esas consultas en inglés frecuentes, bien formatadas (consultas que solo contienen los caracteres A, B, ..., Z y el espacio, y aparecen más de 5 veces).",
                "La longitud promedio de estos pseudocumentos es de 68 palabras y el tamaño total de datos de nuestra \"recopilación de historial\" es de 129 MB.",
                "Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no pueden recuperar al menos 100 pseudocdocumentos de nuestra \"colección de historia\"."
            ],
            "translated_text": "",
            "candidates": [
                "colección de historia",
                "colección de historia",
                "colección de historia",
                "Colección de Historia",
                "colección de historia",
                "recopilación de historial",
                "colección de historia",
                "colección de historia",
                "Colección de historia",
                "colección de historial",
                "colección de historia",
                "Colección de Historia",
                "colección de historia",
                "recopilación de historial",
                "colección de historia",
                "colección de historia"
            ],
            "error": []
        },
        "past query": {
            "translated_key": "consulta pasada",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using <br>past query</br> words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using <br>past query</br> words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative <br>past query</br> in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using <br>past query</br> words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore <br>past query</br> history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a <br>past query</br>, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the <br>past query</br> collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the <br>past query</br> history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from <br>past query</br> history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En este documento, proponemos abordar estas dos deficiencias al aprender aspectos interesantes de un tema de los registros de búsqueda web y organizar los resultados de búsqueda en consecuencia;y (2) generar etiquetas de clúster más significativas utilizando palabras de \"consulta pasada\" ingresadas por los usuarios.",
                "En segundo lugar, generaremos etiquetas de clúster más significativas utilizando palabras de \"consulta pasada\" ingresadas por los usuarios.",
                "Luego podemos organizar los resultados de búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto por la \"consulta pasada\" más representativa en el clúster de consulta.",
                "Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de \"consulta pasada\" son más legibles que las generadas utilizando enfoques de agrupación tradicionales.",
                "En nuestro trabajo, exploramos el historial de \"consulta pasada\" para organizar mejor los resultados de búsqueda de futuras consultas.",
                "Cada documento en H corresponde a una \"consulta pasada\" y, por lo tanto, los documentos de alto nivel corresponden a consultas pasadas relacionadas con QS.4.2 Aspectos de aprendizaje por agrupación Dada una consulta Q, usamos HQ = {D1, ..., DN} para representar los pseudo-documentos clasificados de la colección del historial H. Estos pseudo-documentos contienen los aspectos que los usuarios están interesados.",
                "En el cuartel general de la colección de \"consulta pasada\", cada documento corresponde a una consulta.",
                "Esto muestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios del historial de \"consulta pasada\" y, por lo tanto, puede organizar los resultados de búsqueda de una manera más útil a los usuarios.",
                "Dada una consulta, recuperamos sus consultas relacionadas del historial de \"consulta pasada\", aprendemos los aspectos agrupando las consultas pasadas y la información de clics asociada, y clasificamos los resultados de búsqueda en los aspectos aprendidos."
            ],
            "translated_text": "",
            "candidates": [
                "consulta pasada",
                "consulta pasada",
                "consulta pasada",
                "consulta pasada",
                "consulta pasada",
                "consulta pasada",
                "consulta pasada",
                "consulta pasada",
                "consulta pasada",
                "consulta pasada",
                "consulta pasada",
                "consulta pasada",
                "consulta pasada",
                "consulta pasada",
                "consulta pasada",
                "consulta pasada",
                "consulta pasada",
                "consulta pasada"
            ],
            "error": []
        },
        "clickthrough": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with <br>clickthrough</br> information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated <br>clickthrough</br> information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using <br>clickthrough</br> data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using <br>clickthrough</br> Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Cada Qi corresponde a una consulta única y se enriquece con información de \"clics\" como se discute en la Sección 3.",
                "Dada una consulta, recuperamos sus consultas relacionadas del historial de consultas pasados, aprendemos los aspectos agrupando las consultas pasadas y la información de \"clic\" asociada y clasificamos los resultados de búsqueda en los aspectos aprendidos.",
                "Optimización de los motores de búsqueda utilizando datos de \"clics\".",
                "Evaluación del rendimiento de la recuperación utilizando datos de \"clics\", páginas 79-96."
            ],
            "translated_text": "",
            "candidates": [
                "Clickthrough",
                "clics",
                "hacer clic a través",
                "clic",
                "hacer clic a través",
                "clics",
                "hacer clic a través",
                "clics"
            ],
            "error": []
        },
        "star clustering algorithm": {
            "translated_key": "algoritmo de agrupación de estrellas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the <br>star clustering algorithm</br> [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the <br>star clustering algorithm</br> [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the <br>star clustering algorithm</br> [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the <br>star clustering algorithm</br> below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the <br>star clustering algorithm</br> clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the <br>star clustering algorithm</br>.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the <br>star clustering algorithm</br> is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The <br>star clustering algorithm</br> can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the <br>star clustering algorithm</br>, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The <br>star clustering algorithm</br> for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Dada una nueva consulta, encontramos sus consultas anteriores relacionadas de la colección de historial y aprendemos aspectos a través de la aplicación del \"Algoritmo de agrupación de estrellas\" [2] a estas consultas y clics pasados.",
                "Utilizamos el \"Algoritmo de agrupación de estrellas\" [2], que es un enfoque basado en la partición gráfica, para aprender aspectos interesantes de los registros de búsqueda dada una nueva consulta.",
                "En este artículo, utilizamos un algoritmo basado en la partición gráfica: el \"Algoritmo de agrupación de estrellas\" [2].",
                "Describimos el \"Algoritmo de agrupación de estrellas\" a continuación.4.2.1 Agrupación de estrellas Dada HQ, Star Clustering comienza con la construcción de un gráfico de similitud de pareja en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18].",
                "Después de que se construye el gráfico de similitud Gσ, el \"algoritmo de agrupación de estrellas\" agrupa los documentos utilizando un algoritmo codicioso de la siguiente manera: 1.",
                "Solo hay un parámetro σ en el \"Algoritmo de agrupación de estrellas\".",
                "Una buena característica del \"Algoritmo de agrupación de estrellas\" es que genera un centro para cada clúster.",
                "El \"Algoritmo de agrupación de estrellas\" puede generar diferentes grupos de clústeres para diferentes entradas.",
                "Para el \"Algoritmo de agrupación de estrellas\", estudiamos el parámetro de umbral de similitud σ.",
                "El \"Algoritmo de agrupación de estrellas\" para la organización de información estática y dinámica."
            ],
            "translated_text": "",
            "candidates": [
                "algoritmo de agrupación de estrellas",
                "Algoritmo de agrupación de estrellas",
                "algoritmo de agrupación de estrellas",
                "Algoritmo de agrupación de estrellas",
                "algoritmo de agrupación de estrellas",
                "Algoritmo de agrupación de estrellas",
                "algoritmo de agrupación de estrellas",
                "Algoritmo de agrupación de estrellas",
                "Algoritmo de agrupación de estrellas",
                "algoritmo de agrupación de estrellas",
                "algoritmo de agrupación de estrellas",
                "Algoritmo de agrupación de estrellas",
                "algoritmo de agrupación de estrellas",
                "Algoritmo de agrupación de estrellas",
                "algoritmo de agrupación de estrellas",
                "Algoritmo de agrupación de estrellas",
                "algoritmo de agrupación de estrellas",
                "Algoritmo de agrupación de estrellas",
                "algoritmo de agrupación de estrellas",
                "Algoritmo de agrupación de estrellas"
            ],
            "error": []
        },
        "suffix tree clustering algorithm": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the <br>suffix tree clustering algorithm</br> (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Se comparan varios algoritmos de agrupación y se demostró que el \"algoritmo de agrupación de árboles sufijos\" (STC) es el más efectivo."
            ],
            "translated_text": "",
            "candidates": [
                "Algoritmo de agrupación de árboles de sufijo",
                "algoritmo de agrupación de árboles sufijos"
            ],
            "error": []
        },
        "search result snippet": {
            "translated_key": "Fragmento de resultados de búsqueda",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the <br>search result snippet</br>s and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Para superar esta dificultad, en [28], se estudiaron los algoritmos de aprendizaje supervisados para extraer frases significativas del \"fragmento de resultados de búsqueda\" y estas frases se usaron para agrupar los resultados de búsqueda."
            ],
            "translated_text": "",
            "candidates": [
                "Fragmento de resultados de búsqueda",
                "fragmento de resultados de búsqueda"
            ],
            "error": []
        },
        "monothetic clustering algorithm": {
            "translated_key": "algoritmo de agrupación monotética",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a <br>monothetic clustering algorithm</br>, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En [13], los autores propusieron usar un \"algoritmo de agrupación monotética\", en el que se asigna un documento a un clúster basado en una sola característica, organizar los resultados de búsqueda, y la característica única se utiliza para etiquetar el clúster correspondiente."
            ],
            "translated_text": "",
            "candidates": [
                "algoritmo de agrupación monotética",
                "algoritmo de agrupación monotética"
            ],
            "error": []
        },
        "pseudo-document": {
            "translated_key": "pseudo-documento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a <br>pseudo-document</br> which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and <br>pseudo-document</br> Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Es decir, para cada consulta única, construimos un \"pseudo-documento\" que consiste en todas las descripciones de sus clics en todas las sesiones agregadas.",
                "Específicamente, usamos la siguiente fórmula para calcular la similitud entre la consulta Q y el \"pseudo-documento\" Qi: W∈Q ¡Qi C (W, Q) × IDF (W) × (K1 + 1) × C (W, Qi) k1 ((1 - b) + b | qi | avdl) + c (w, qi) donde k1 y b son parámetros okapi establecidos empíricamente, c (w, qi) y c (w, q) son el conteo de palabrasW en Qi y Q respectivamente, IDF (W) es la frecuencia de documento inversa de la palabra W, y AVDL es la longitud promedio del documento en nuestra colección de historia."
            ],
            "translated_text": "",
            "candidates": [
                "pseudo-documento",
                "pseudo-documento",
                "pseudo-documento",
                "pseudo-documento"
            ],
            "error": []
        },
        "pairwise similarity graph": {
            "translated_key": "Gráfico de similitud por pares",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a <br>pairwise similarity graph</br> on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Describimos el algoritmo de agrupación de estrellas a continuación.4.2.1 La agrupación de estrellas Dada HQ, la agrupación de estrellas comienza con la construcción de un \"gráfico de similitud por pares\" en esta colección basada en el modelo de espacio vectorial en la recuperación de información [18]."
            ],
            "translated_text": "",
            "candidates": [
                "Gráfico de similitud por pares",
                "gráfico de similitud por pares"
            ],
            "error": []
        },
        "similarity threshold parameter": {
            "translated_key": "Parámetro de umbral de similitud",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a <br>similarity threshold parameter</br> σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the <br>similarity threshold parameter</br> σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un gráfico de similitud Gσ se puede construir de la siguiente manera utilizando un \"parámetro de umbral de similitud\" σ.",
                "Para el algoritmo de agrupación de estrellas, estudiamos el \"parámetro de umbral de similitud\" σ."
            ],
            "translated_text": "",
            "candidates": [
                "Parámetro de umbral de similitud",
                "parámetro de umbral de similitud",
                "Parámetro de umbral de similitud",
                "parámetro de umbral de similitud"
            ],
            "error": []
        },
        "centroid-based method": {
            "translated_key": "método basado en centroides",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple <br>centroid-based method</br> for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The <br>centroid-based method</br> computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Aquí utilizamos un simple \"método basado en centroides\" para la categorización.",
                "El \"método basado en centroides\" calcula la similitud cosena entre la representación vectorial de SJ y cada prototipo Centroide PI."
            ],
            "translated_text": "",
            "candidates": [
                "método basado en centroides",
                "método basado en centroides",
                "método basado en centroides",
                "método basado en centroides"
            ],
            "error": []
        },
        "cosine similarity": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the <br>cosine similarity</br> between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest <br>cosine similarity</br> score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El método basado en el centroide calcula la \"similitud coseno\" entre la representación vectorial de SJ y cada prototipo Centroide PI.",
                "Luego asignamos SJ al aspecto con el que tiene la puntuación más alta de \"similitud de coseno\"."
            ],
            "translated_text": "",
            "candidates": [
                "similitud de coseno",
                "similitud coseno",
                "similitud de coseno",
                "similitud de coseno"
            ],
            "error": []
        },
        "centroid prototype": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a <br>centroid prototype</br> pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each <br>centroid prototype</br> pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Basado en los pseudo-documentos en cada aspecto de aspecto descubierto, construimos un \"prototipo centroide\" PI tomando el promedio de todos los vectores de los documentos en CI: PI = 1 | CI |l∈Ci vl.",
                "El método basado en el centroide calcula la similitud cosena entre la representación vectorial de SJ y cada PI de \"prototipo centroide\"."
            ],
            "translated_text": "",
            "candidates": [
                "prototipo de centroides",
                "prototipo centroide",
                "prototipo de centroides",
                "prototipo centroide"
            ],
            "error": []
        },
        "reciprocal rank": {
            "translated_key": "rango recíproco",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean <br>reciprocal rank</br> (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "También utilizamos el \"rango recíproco\" (MRR) medio como otra métrica."
            ],
            "translated_text": "",
            "candidates": [
                "rango recíproco",
                "rango recíproco"
            ],
            "error": []
        },
        "log-based method": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our <br>log-based method</br> is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our <br>log-based method</br>, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our <br>log-based method</br> can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that <br>log-based method</br> help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the <br>log-based method</br> can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the <br>log-based method</br> can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, <br>log-based method</br> may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our <br>log-based method</br>.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the <br>log-based method</br> and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the <br>log-based method</br>, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our <br>log-based method</br> is more effective in helping users to navigate into their desired results.",
                "Cluster-based method <br>log-based method</br> city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our <br>log-based method</br> 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our <br>log-based method</br> can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For <br>log-based method</br>, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our <br>log-based method</br> gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar <br>log-based method</br> Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple <br>log-based method</br> Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our <br>log-based method</br> with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our <br>log-based method</br> can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our <br>log-based method</br> can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Esto muestra que nuestro \"método basado en registros\" es efectivo para aprender las preferencias de los usuarios del historial de consultas anteriores y, por lo tanto, puede organizar los resultados de búsqueda de una manera más útil a los usuarios.",
                "Para probar la sensibilidad del parámetro σ de nuestro \"método basado en registros\", usamos uno de los conjuntos de pruebas para ajustar el parámetro para que sea óptimo y luego usamos el parámetro sintonizado en el otro conjunto.",
                "En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden mejorarse con nuestro método.6.2.2 Análisis detallado Para comprender mejor los casos en los que nuestro \"método basado en registros\" puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultades de consulta.",
                "Para probar la hipótesis de que el \"método basado en registros\" ayuda más esas consultas con diversos resultados, calculamos las relaciones de tamaño de los grupos más grandes y más grandes en nuestros resultados basados en log y utilizamos esta relación como un indicador de diversidad.",
                "Podemos observar que cuando la relación es más pequeña, el \"método basado en registros\" puede mejorar más casos de prueba.",
                "Pero cuando 0 5 10 15 20 25 30 1 2 3 4 Número de contenedor #Quiteries mejoró la Figura 3: La correlación entre el cambio de rendimiento y la dificultad de la consulta.La relación es grande, el \"método basado en log\" no puede mejorar sobre la línea de base.",
                "Claramente, en Bin 1, la mayoría de los casos de prueba se mejoran (24 frente a 3), mientras que en Bin 4, el \"método basado en log\" puede disminuir el rendimiento (3 vs 20).",
                "También estudiamos el impacto del número de consultas anteriores recuperadas en nuestro \"método basado en registros\".",
                "Más importante aún, a medida que pasa el tiempo, cada vez más consultas tendrán suficiente historia, por lo que podemos mejorar más y más consultas.6.2.4 Un ejemplo ilustrativo Usamos los códigos de área de consulta para mostrar la diferencia en los resultados del \"Método basado en log\" y el método basado en clúster.",
                "En el \"método basado en registros\", los resultados se desambigan en dos sentidos: códigos telefónicos o códigos postales.",
                "Por lo tanto, nuestro \"método basado en registros\" es más efectivo para ayudar a los usuarios a navegar hacia sus resultados deseados.",
                "Método basado en clúster \"Método basado en registro\" Ciudad, teléfono estatal, ciudad, local internacional, teléfono de área, marcación de zip internacional, Tabla postal 5: un ejemplo que muestra la diferencia entre el método basado en clúster y nuestro \"método basado en registros\"0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #Queries Conjunto de prueba recuperado 1 Test Set 2 Figura 5: El impacto del número de consultas anteriores recuperadas.6.2.5 Comparación de etiquetado Ahora comparamos las etiquetas entre el método basado en clúster y el método basado en log.",
                "Nuestro \"método basado en registros\" puede evitar esta dificultad aprovechando las consultas.",
                "Para el \"método basado en log\", utilizamos el centro de cada clúster Star como la etiqueta para el clúster correspondiente.",
                "Desde esta tabla, podemos ver que nuestro \"método basado en registros\" proporciona etiquetas más legibles porque genera etiquetas basadas en consultas de usuarios.",
                "Comparación de etiquetas para la consulta Jaguar \"Método basado en log\" Método basado en clúster 1. Jaguar Animal 1. Jaguar, Auto, Accesorios 2. Jaguar Accesorios automáticos 2. Jaguar, Tipo, Precios 3. Jaguar Cats 3. Jaguar, Pantera, Cats 4. Jaguar Repair 4. Jaguar, Servicios, Boston 5. Jaguar Animal Pictures 5. Jaguar, Colección, comparación de etiquetas de ropa para la consulta Apple \"Método basado en log\" Método basado en clúster 1. Computadora de Apple 1. Apple, soporte, producto 2.Apple iPod 2. Apple, sitio, computadora 3. Receta de Apple Crisp 3. Apple, Mundo, Visita 4. Cake de Apple Fresh 4. Apple, iPod, Amazon 5. la computadora portátil de Apple 5. Apple, Productos, Noticias Tabla 6: Comparación de etiquetas de clúster.7.",
                "Comparamos nuestro \"método basado en registros\" con el método tradicional basado en clúster y la línea de base de la clasificación del motor de búsqueda.",
                "Los experimentos muestran que nuestro \"método basado en registros\" puede superar consistentemente al método basado en clúster y mejorar sobre la línea de base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos.",
                "Además, nuestro \"método basado en registros\" puede generar etiquetas de aspecto más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda cuando clúster los resultados de búsqueda."
            ],
            "translated_text": "",
            "candidates": [
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "método basado en log",
                "método basado en registros",
                "método basado en log",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "Método basado en log",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "Método basado en registro",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "método basado en log",
                "método basado en registros",
                "método basado en registros",
                "Método basado en registros",
                "Método basado en log",
                "Método basado en log",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros",
                "método basado en registros"
            ],
            "error": []
        },
        "mean average precision": {
            "translated_key": "Precisión promedio media",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the <br>mean average precision</br> (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Cuantificamos la dificultad de consulta por la \"precisión promedio media\" (mapa) de la clasificación del motor de búsqueda original para cada caso de prueba."
            ],
            "translated_text": "",
            "candidates": [
                "Precisión promedio media",
                "precisión promedio media"
            ],
            "error": []
        },
        "search result organization": {
            "translated_key": "Organización de resultados de búsqueda",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of <br>search result organization</br>.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the <br>search result organization</br> based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la \"organización de resultados de búsqueda\".",
                "Experimentos En la sección, describimos nuestros experimentos en los registros de los motores de búsqueda anteriores de la \"organización de resultados de búsqueda\".6.1 Diseño experimental Utilizamos dos métodos de referencia para evaluar el método propuesto para organizar los resultados de búsqueda."
            ],
            "translated_text": "",
            "candidates": [
                "Organización de resultados de búsqueda",
                "organización de resultados de búsqueda",
                "Organización de resultados de búsqueda",
                "organización de resultados de búsqueda"
            ],
            "error": []
        },
        "search engine log": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial <br>search engine log</br> data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe <br>search engine log</br> data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Evaluamos nuestro método propuesto en datos comerciales de \"registro de motores de búsqueda\".",
                "En la Sección 3, describimos los datos de \"registro de motores de búsqueda\" y nuestro procedimiento para construir una recopilación de historial."
            ],
            "translated_text": "",
            "candidates": [
                "registro de motor de búsqueda",
                "registro de motores de búsqueda",
                "registro de motor de búsqueda",
                "registro de motores de búsqueda"
            ],
            "error": []
        },
        "interest aspect": {
            "translated_key": "aspecto de interés",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}